<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>python | psadil</title><link>https://psadil.github.io/psadil/tag/python/</link><atom:link href="https://psadil.github.io/psadil/tag/python/index.xml" rel="self" type="application/rss+xml"/><description>python</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© 2021 Patrick Sadil</copyright><lastBuildDate>Sat, 23 Apr 2022 00:00:00 +0000</lastBuildDate><image><url>https://psadil.github.io/psadil/media/icon_hu3896c2ce465988ba1fc8077f9a6388c6_268630_512x512_fill_lanczos_center_2.png</url><title>python</title><link>https://psadil.github.io/psadil/tag/python/</link></image><item><title>How to store a NifTi as a TFRecord</title><link>https://psadil.github.io/psadil/post/tf-dataset-from-3d-nifti/</link><pubDate>Sat, 23 Apr 2022 00:00:00 +0000</pubDate><guid>https://psadil.github.io/psadil/post/tf-dataset-from-3d-nifti/</guid><description>&lt;h1 id="how-to-store-a-nifti-as-a-tfrecord">How to store a NifTi as a TFRecord&lt;/h1>
&lt;p>Patrick Sadil
2022-04-23&lt;/p>
&lt;p>A recent project required sending brain images to TensorFlow.
Unfortunately, the data exceeded memory and so during training would
need to be read from the disk. Poking around the TF documentation, it
seems like a recommended way to do this is to store the images as a
&lt;a href="https://www.tensorflow.org/tutorials/load_data/tfrecord" target="_blank" rel="noopener">TFRecord&lt;/a>. The
steps for doing that are collected in this gist&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>There are five main steps&lt;/p>
&lt;ol>
&lt;li>load the images as numpy array,&lt;/li>
&lt;li>perform an preprocessing (e.g., resizing, masking),&lt;/li>
&lt;li>serialize the preprocessed images and store with their labels (all
in memory) as a
&lt;a href="https://www.tensorflow.org/api_docs/python/tf/train/Example" target="_blank" rel="noopener">&lt;code>tf.train.Example&lt;/code>&lt;/a>,&lt;/li>
&lt;li>store (on disk) the examples as
&lt;a href="https://www.tensorflow.org/tutorials/load_data/tfrecord" target="_blank" rel="noopener">TFRecord&lt;/a>,
and finally&lt;/li>
&lt;li>create a
&lt;a href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset" target="_blank" rel="noopener">&lt;code>tf.Data.Dataset&lt;/code>&lt;/a>
pipeline that serves the examples to the model.&lt;/li>
&lt;/ol>
&lt;pre>&lt;code class="language-python">import math
from numbers import Number
import numpy as np
import tensorflow as tf
import nibabel as nb
from nilearn import plotting
from typing import List
import numpy.typing as npt
&lt;/code>&lt;/pre>
&lt;h2 id="preprocess">Preprocess&lt;/h2>
&lt;p>A full implementation might involve several preprocessing steps (e.g.,
registration, masking, cropping), but in this gist we&amp;rsquo;ll just rescale
the images to have intensity values that range from 0-1.&lt;/p>
&lt;pre>&lt;code class="language-python">def to_numpy(img: nb.Nifti1Image) -&amp;gt; np.ndarray:
return np.asanyarray(img.dataobj)
def load_and_preprocess(img: str) -&amp;gt; np.ndarray:
# convert to numpy array and preprocess
nii = to_numpy(nb.load(img))
# rescale to 0-1
preprocessed = (nii - nii.min())/(nii.max() - nii.min()).astype(np.float32)
return preprocessed
&lt;/code>&lt;/pre>
&lt;p>Since this method of storing data was new for me, I wanted to ensure
that I didn&amp;rsquo;t mess up the data. For that, I mainly relied on a basic
plot of the images.&lt;/p>
&lt;pre>&lt;code class="language-python">def plot_array(img: np.ndarray) -&amp;gt; None:
nii = nb.Nifti1Image(img, affine=np.eye(4)*2)
plotting.plot_anat(nii)
&lt;/code>&lt;/pre>
&lt;p>This first step is standard neuroimaging, but just to check that the
functions are working preprocess an example brain and see how it looks.&lt;/p>
&lt;pre>&lt;code class="language-python"># example images packaged with fsl, found at $FSLDIR/data/standard
img = ['MNI152lin_T1_2mm.nii.gz']
preprocesed = load_and_preprocess(img[0])
plot_array(preprocesed)
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="index.en_files/figure-gfm/cell-5-output-1.png" width="653"
height="269" />&lt;/p>
&lt;p>Great! That looks like a brain. I&amp;rsquo;ll use it as a reference to ensure
that the roundtrip processing, serializing and unserializing, returns
the arrays we need.&lt;/p>
&lt;h2 id="serialize">Serialize&lt;/h2>
&lt;p>Here&amp;rsquo;s where TensorFlow starts. As those that came before have always
done, &lt;a href="https://www.tensorflow.org/tutorials/load_data/tfrecord" target="_blank" rel="noopener">we&amp;rsquo;ll rely on these
incantations&lt;/a>.&lt;/p>
&lt;pre>&lt;code class="language-python">def _bytes_feature(value: bytes):
&amp;quot;&amp;quot;&amp;quot;
Returns a bytes_list from a string / byte.
Example:
_bytes_feature(b'\x00')
_bytes_feature(b'a')
&amp;quot;&amp;quot;&amp;quot;
return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))
def _float_feature(value: Number):
&amp;quot;&amp;quot;&amp;quot;
Returns a float_list from a float / double.
Example:
_float_feature(2)
_float_feature(2.)
&amp;quot;&amp;quot;&amp;quot;
return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))
&lt;/code>&lt;/pre>
&lt;p>Those functions seem to help ensure that the serialized data ends up
with appropriate types. But the full steps involve serializing both the
image and the label together.&lt;/p>
&lt;pre>&lt;code class="language-python">def serialize_example(img: str, label: Number):
# convert to numpy array and preprocess
preprocessed = load_and_preprocess(img)
# Notice how the numpy array is converted into a byte_string. That's serialization in memory!
feature = {
'label': _float_feature(label),
'image_raw': _bytes_feature(tf.io.serialize_tensor(preprocessed).numpy())
}
return tf.train.Example(features=tf.train.Features(feature=feature))
&lt;/code>&lt;/pre>
&lt;p>To get the image back out, we need two more functions: one to
unpack/parse the example (&lt;code>decode_example&lt;/code>) and another to
unserialize/parse (&lt;code>parse_1_example&lt;/code>) the unpacked example.&lt;/p>
&lt;pre>&lt;code class="language-python">def decode_example(record_bytes) -&amp;gt; dict:
example = tf.io.parse_example(
record_bytes,
features = {
&amp;quot;label&amp;quot;: tf.io.FixedLenFeature([], dtype=tf.float32),
'image_raw': tf.io.FixedLenFeature([], dtype=tf.string)
}
)
return example
def parse_1_example(example):
&amp;quot;&amp;quot;&amp;quot;
Note that the network I was using worked with 3D images, and so the batches of data were of shape `(batch_size, x_dim, y_dim, z_dim, 1)`, rather than what is typical for 2d images: `(batch_size, x_dim, y_dim, n_channels)`.
&amp;quot;&amp;quot;&amp;quot;
X = tf.io.parse_tensor(example['image_raw'], out_type=tf.float32)
# the images output by tf.io.parse_tensor will have shape (x_dim, y_dim, z_dim), which is to say that they're missing the channels dimension. expand_dims is used to indicate channel (i.e., be explicit about grayscale)
return tf.expand_dims(X, 3), example['label']
&lt;/code>&lt;/pre>
&lt;p>At this point, we have functions for preprocessing the images,
serializing them&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>, and packing each of them along with their labels
into a
&lt;a href="https://www.tensorflow.org/api_docs/python/tf/train/Example" target="_blank" rel="noopener">&lt;code>tf.train.Example&lt;/code>&lt;/a>.
This is all for converting the data into a format that can then be more
easily written to and read from the disk.&lt;/p>
&lt;h2 id="write-serialized-examples-as-tfrecords">Write serialized examples as TFRecords&lt;/h2>
&lt;p>So far, everything has been in memory. Next comes a function that
performs the above steps sequentially on several examples, and along the
way writes the examples as a
&lt;a href="https://www.tensorflow.org/tutorials/load_data/tfrecord" target="_blank" rel="noopener">&lt;code>TFRecord&lt;/code>&lt;/a>.&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>&lt;/p>
&lt;pre>&lt;code class="language-python">def write_records(niis: List[str], labels: npt.ArrayLike, n_per_record: int, outfile: str) -&amp;gt; None:
&amp;quot;&amp;quot;&amp;quot;
store list of niftis (and associated label) into tfrecords for use as dataset
Args:
niis: files that will be preprocessed and stored in record
labels: true label associated with each element in niis. these are the &amp;quot;y&amp;quot;
n_per_record: number of examples to store in each record. TF documentation advises that the files are +100Mb. Around 400 images cropped images at 2mm resolution seems to work.
outfile: base prefix to use when writing the records
Returns:
Nothing, but the records will be written to disk.
&amp;quot;&amp;quot;&amp;quot;
n_niis = len(niis)
n_records = math.ceil(len(niis) / n_per_record)
for i, shard in enumerate(range(0, n_niis, n_per_record)):
print(f&amp;quot;writing record {i} of {n_records-1}&amp;quot;)
with tf.io.TFRecordWriter(
f&amp;quot;{outfile}_{i:0&amp;gt;3}-of-{n_records-1:0&amp;gt;3}.tfrecords&amp;quot;,
options= tf.io.TFRecordOptions(compression_type=&amp;quot;GZIP&amp;quot;)
) as writer:
for nii, label in zip(niis[shard:shard+n_per_record], labels[shard:shard+n_per_record]):
example = serialize_example(img=nii, label=label)
writer.write(example.SerializeToString())
&lt;/code>&lt;/pre>
&lt;h2 id="create-dataset">Create Dataset&lt;/h2>
&lt;p>For this gist, let&amp;rsquo;s store several copies of the MNI 2mm brain.&lt;/p>
&lt;pre>&lt;code class="language-python"># (e.g., put nifti of label MNI152_T1_1mm_brain.nii.gz in the working directory)
n_imgs = 3
mni_nii = ['MNI152lin_T1_2mm.nii.gz'] * n_imgs
# store examples in each tfrecord. number of examples per record is configurable.
# aim for as many examples as produces files of size &amp;gt; 100M
prefix = &amp;quot;tmp&amp;quot;
write_records(mni_nii, np.arange(n_imgs), n_imgs, prefix)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>writing record 0 of 0
2022-05-13 15:00:56.962382: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
&lt;/code>&lt;/pre>
&lt;p>Calling the above will write a TFRecord file to disk. To read that
record, define a pipeline that will create a &lt;code>tf.Data.dataset&lt;/code>.&lt;/p>
&lt;pre>&lt;code class="language-python">def get_batched_dataset(files, batch_size: int = 32) -&amp;gt; tf.data.Dataset:
# Note: an actual pipeline would probably include at least the shuffle and prefetch methods
dataset = (
tf.data.Dataset.list_files(files) # note shuffling is on by default, changes order when there are multiple records
.flat_map(lambda x: tf.data.TFRecordDataset(x, compression_type=&amp;quot;GZIP&amp;quot;))
.map(decode_example)
.map(parse_1_example)
.batch(batch_size)
)
return dataset
&lt;/code>&lt;/pre>
&lt;p>Now, use that function to read the records back.&lt;/p>
&lt;pre>&lt;code class="language-python"># a full dataset will have a list with many records
list_of_records=[f'{prefix}*.tfrecords']
ds = get_batched_dataset(list_of_records, batch_size=2)
&lt;/code>&lt;/pre>
&lt;p>That dataset, &lt;code>ds&lt;/code>, was our goal in the gist. It can be passed to
methods of the &lt;code>tf.keras.Model&lt;/code>, including
[&lt;code>tf.keras.Model.fit()&lt;/code>(&lt;a href="https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit">https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit&lt;/a>)],
serving up more data than can fit in memory.&lt;/p>
&lt;p>But first, the serialization is a lot, so it is a good idea to verify
that the images look okay when loaded. To do so, pluck a single example
from the dataset.&lt;/p>
&lt;pre>&lt;code class="language-python">(Xs, Ys) = next(ds.as_numpy_iterator())
&lt;/code>&lt;/pre>
&lt;p>The dimensions of the labels are relatively easy. It&amp;rsquo;s a 1d array with
as many elements as are in the batch.&lt;/p>
&lt;pre>&lt;code class="language-python"># (batch_size, )
Ys.shape
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>(2,)
&lt;/code>&lt;/pre>
&lt;p>The order will depend on what shuffling is embedded in the dataset
pipeline. In this case, there was no shuffling and so we should expect
that the order is preserved.&lt;/p>
&lt;pre>&lt;code class="language-python">Ys
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>array([0., 1.], dtype=float32)
&lt;/code>&lt;/pre>
&lt;p>The data, &lt;code>Xs&lt;/code>, also has a predictable shape.&lt;/p>
&lt;pre>&lt;code class="language-python"># (batch_size, x_dim, y_dim, z_dim, 1)
Xs.shape
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>(2, 91, 109, 91, 1)
&lt;/code>&lt;/pre>
&lt;p>Take that first element in the batch and plot.&lt;/p>
&lt;pre>&lt;code class="language-python">parsed_img = np.squeeze(Xs[0,])
plot_array(parsed_img)
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="index.en_files/figure-gfm/cell-17-output-1.png" width="653"
height="269" />&lt;/p>
&lt;p>That looks great! Just in case, let&amp;rsquo;s check more explicitly&lt;/p>
&lt;pre>&lt;code class="language-python">np.array_equal(preprocesed, parsed_img)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>True
&lt;/code>&lt;/pre>
&lt;p>Yay! done&lt;/p>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Inspiration for writing came from responding to &lt;a href="https://neurostars.org/t/tensorflow-issue-when-trying-to-use-nibabel-in-dataset/22410/2" target="_blank" rel="noopener">this question on
NeuroStars&lt;/a>,
and also from an urge to try a python-based post.&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>This serializing stuff is spooky black magic. I&amp;rsquo;m going to skip
over those details and instead leave this reference, a journey
through serializing in &lt;code>R&lt;/code>
&lt;a href="https://blog.djnavarro.net/posts/2021-11-15_serialisation-with-rds/">https://blog.djnavarro.net/posts/2021-11-15_serialisation-with-rds/&lt;/a>.&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>It seems to me that this storage on disk involves a &lt;em>second&lt;/em>
serialization step (where the first was done by
&lt;code>tf.io.serialize_tensor&lt;/code> in &lt;code>serialize_example&lt;/code>). Even so, I assume
the roundtrip isn&amp;rsquo;t so much of a big deal, considering that the data
can be served to the model in parallel.&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>New England GAN</title><link>https://psadil.github.io/psadil/post/2021-01-02-gan-mass/</link><pubDate>Sun, 17 Jan 2021 00:00:00 +0000</pubDate><guid>https://psadil.github.io/psadil/post/2021-01-02-gan-mass/</guid><description>
&lt;script src="https://psadil.github.io/psadil/post/2021-01-02-gan-mass/index.en_files/header-attrs/header-attrs.js">&lt;/script>
&lt;p>A friend recently moved away from Western Mass, so I wanted to send them a gift to help remind them of the area. I also happened to want to learn a bit more about Generative Adversarial Networks (GANs).&lt;/p>
&lt;div id="goals" class="section level1">
&lt;h1>Goals&lt;/h1>
&lt;ul>
&lt;li>Briefly overview GANs&lt;/li>
&lt;li>Link to fun code for acquiring data from Google Street View&lt;/li>
&lt;li>Share a neat set of photographs&lt;/li>
&lt;/ul>
&lt;p>Note that this isn’t an ‘intro to GANs’. If that’s what you’re after, keep browsing.&lt;/p>
&lt;/div>
&lt;div id="overview" class="section level1">
&lt;h1>Overview&lt;/h1>
&lt;p>Computers have gotten very good at extracting information from images, particularly at identifying images’ contents. Such categorization is powerful, but it often requires access to labeled data – hundreds of thousands of pictures for which we can tell the computer: this is a cat, that is a dog, no that’s also a dog, yes that’s a dog but it’s also a husky. However, many applications remain where computer-aided categorization would be invaluable, but for which there isn’t sufficient labeled data. If an algorithm can learn to recognize the subtle features distinguishing &lt;a href="https://en.wikipedia.org/wiki/ImageNet">120 dog breeds&lt;/a>, it could probably learn visual features that help radiologists locate potential anomalies. But the guess-and-check strategy, despite being sufficient for many advanced computer vision algorithms, flounders when it has access to only a few hundred training examples. Computers have the potential to do some very clever things, but there is not always enough data to supervise their training.&lt;/p>
&lt;p>To mitigate a lack of data, one developing solution is a GAN. A common analogy for these networks envisions art forgery (&lt;a href="https://www.tensorflow.org/tutorials/generative/dcgan">e.g.&lt;/a>), a forger and a critic collaborating to learn about an artist. The forger paints fake works in the style of van Gough, while the critic distinguishes the fake from the real van Goughs. For the forger to succeed, it must paint the essences of van Gough: the reductionist features like the strokes and the yellows, and the holistic feelings of urgency and presence. For the critic to succeed, it must identify those essences, learning the sharp boundaries between longing and yearning. As the forgeries improve, the critic becomes more discerning, further inspiring the forger. Although the networks are taught the essences – the labels – explicitly, the two together learn about van Gough. And they’ll learn without supervision.&lt;/p>
&lt;p>After learning, the critic can be deployed for standard categorization tasks (e.g., aiding medical diagnoses). But the training also produces another useful machine, a machine that is capable of generating images. Predictably, there are challenges to training a generator that is capable of producing good quality, large, and diverse images. But I didn’t need the images to be stellar, so long as their content was clear (to a human). A lack of photorealism – imperfect training – could make the pictures more interesting&lt;a href="#fn1" class="footnote-ref" id="fnref1">&lt;sup>1&lt;/sup>&lt;/a>. To make a gift, I wanted a forger that could paint New England.&lt;/p>
&lt;/div>
&lt;div id="setup" class="section level1">
&lt;h1>Setup&lt;/h1>
&lt;p>I wanted the forger to generate images of New England, so I first needed a bunch of pictures of New England. I have photographed a few hundred pictures, but this wouldn’t be nearly enough&lt;a href="#fn2" class="footnote-ref" id="fnref2">&lt;sup>2&lt;/sup>&lt;/a>. Instead, I relied on a combination of Google’s &lt;a href="https://developers.google.com/maps/documentation/streetview/overview">Street View Static&lt;/a> and &lt;a href="https://developers.google.com/maps/documentation/directions/overview">Directions&lt;/a> APIs. The Street View API gives a picture associated with a location, and those locations were provided by the Directions API. &lt;a href="https://github.com/psadil/gan-mass">The repository&lt;/a> for the network has the details, but the result was that I could input an origin and a destination – meandering through a few waypoints – and download whatever the Street View Car recorded when it traveled along those directions&lt;a href="#fn3" class="footnote-ref" id="fnref3">&lt;sup>3&lt;/sup>&lt;/a>. In the end, I collected ~25,000 images.&lt;/p>
&lt;p>25k may feel like a lot of images. But skimming online suggested that &lt;a href="https://blogs.nvidia.com/blog/2020/12/07/neurips-research-limited-data-gan/">even 25k would not have been enough to adequately constrain the networks&lt;/a>. GANs may not require labeled examples, but they are still data-hungry. Given my relatively small dataset, I picked an adversarial architecture that incorporates a few extra tricks to glean information from smaller datasets: &lt;a href="https://github.com/NVlabs/stylegan2-ada">stylegan2-ada&lt;/a>&lt;a href="#fn4" class="footnote-ref" id="fnref4">&lt;sup>4&lt;/sup>&lt;/a>. To train the network, I used free credits on the Google’s cloud console.&lt;/p>
&lt;/div>
&lt;div id="curated-samples" class="section level1">
&lt;h1>Curated Samples&lt;/h1>
&lt;p>After one day of training&lt;a href="#fn5" class="footnote-ref" id="fnref5">&lt;sup>5&lt;/sup>&lt;/a>, the network started producing some useful images.&lt;/p>
&lt;div class="figure">
&lt;img src="print_5x7_256.png" alt="" />
&lt;p class="caption">These six images are fake, produced from a collaboration to learn about New England.&lt;/p>
&lt;/div>
&lt;p>I chose these six – and the seventh at the top – because they illustrate a few fun features of what the GAN learned. For example, the GAN learned, very early, that pictures of New England always have, in the bottom corners, the word “Google”&lt;a href="#fn6" class="footnote-ref" id="fnref6">&lt;sup>6&lt;/sup>&lt;/a>. That machine learning can produce realistic text surprises me (e.g., &lt;a href="https://www.facebook.com/botsofnewyork/photos/a.2028566864113743/2490502274586864/?type=3&amp;amp;theater">if the face is weird, how are all of the pixels in place to spell out a word&lt;/a>?!). I assume that text comes out clean because most lettering is tightly constrained. That is, when the forger paints something that could be categorized as lettering, the critic severely constrains those pixels; fuzzy letters betray forgery, and real photographs don’t have nonsense like UNS;QD*LKJ. So if the training images contain enough text that the generator starts producing letters, there is also enough text for the critic to learn what text is realistic.&lt;/p>
&lt;p>The forger had difficulty with buildings. I downloaded mostly images of the highways connecting cities. This means that there were enough cityscapes for the GAN to generate buildings, but relative to a road, it was much slower at learning the intricacies of a building. Of course, the roads are imperfect, too (the telephone pole in the upper middle ripples, the upper left has too many roads, the colors of the painted lines mismatch, etc). But unlike, say, a bad photoshop, these errors have a kind of global coherence that, subjectively, allows the images to seem not fake but instead surreal.&lt;/p>
&lt;/div>
&lt;div class="footnotes">
&lt;hr />
&lt;ol>
&lt;li id="fn1">&lt;p>If I wanted perfect pictures, I could have just used a camera.&lt;a href="#fnref1" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn2">&lt;p>The van Gough example is slightly misleading; in practice, van Gough didn’t paint enough pictures to train a GAN. Training a GAN from scratch doesn’t require labeled data, but it still requires many images. There are tricks that could help a GAN, but simply training his images would likely be insufficient.&lt;a href="#fnref2" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn3">&lt;p>Having not owned a car during graduate school, I found it funny that these networks learned about New England through its highways&lt;a href="#fnref3" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn4">&lt;p>But also, they provided a helpful docker image, functions to prep the data, and decent documentation. This is a good reminder about the benefits of polishing a repository.&lt;a href="#fnref4" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn5">&lt;p>After one day, the training error was still decreasing. But I was using a &lt;a href="https://cloud.google.com/compute/docs/instances/preemptible">preemptible virtual machine&lt;/a>, and so after 24 hours it was automatically shutdown.&lt;a href="#fnref5" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn6">&lt;p>I removed the text from the curated examples, but it can be seen in the preview image at the top.&lt;a href="#fnref6" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;/ol>
&lt;/div></description></item></channel></rss>