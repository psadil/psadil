<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>python | psadil</title><link>https://psadil.github.io/psadil/tag/python/</link><atom:link href="https://psadil.github.io/psadil/tag/python/index.xml" rel="self" type="application/rss+xml"/><description>python</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© 2021 Patrick Sadil</copyright><lastBuildDate>Sun, 17 Jan 2021 00:00:00 +0000</lastBuildDate><image><url>https://psadil.github.io/psadil/media/icon_hu3896c2ce465988ba1fc8077f9a6388c6_268630_512x512_fill_lanczos_center_2.png</url><title>python</title><link>https://psadil.github.io/psadil/tag/python/</link></image><item><title>New England GAN</title><link>https://psadil.github.io/psadil/post/2021-01-02-gan-mass/</link><pubDate>Sun, 17 Jan 2021 00:00:00 +0000</pubDate><guid>https://psadil.github.io/psadil/post/2021-01-02-gan-mass/</guid><description>
&lt;script src="https://psadil.github.io/psadil/post/2021-01-02-gan-mass/index.en_files/header-attrs/header-attrs.js">&lt;/script>
&lt;p>A friend recently moved away from Western Mass, so I wanted to send them a gift to help remind them of the area. I also happened to want to learn a bit more about Generative Adversarial Networks (GANs).&lt;/p>
&lt;div id="goals" class="section level1">
&lt;h1>Goals&lt;/h1>
&lt;ul>
&lt;li>Briefly overview GANs&lt;/li>
&lt;li>Link to fun code for acquiring data from Google Street View&lt;/li>
&lt;li>Share a neat set of photographs&lt;/li>
&lt;/ul>
&lt;p>Note that this isn’t an ‘intro to GANs’. If that’s what you’re after, keep browsing.&lt;/p>
&lt;/div>
&lt;div id="overview" class="section level1">
&lt;h1>Overview&lt;/h1>
&lt;p>Computers have gotten very good at extracting information from images, particularly at identifying images’ contents. Such categorization is powerful, but it often requires access to labeled data – hundreds of thousands of pictures for which we can tell the computer: this is a cat, that is a dog, no that’s also a dog, yes that’s a dog but it’s also a husky. However, many applications remain where computer-aided categorization would be invaluable, but for which there isn’t sufficient labeled data. If an algorithm can learn to recognize the subtle features distinguishing &lt;a href="https://en.wikipedia.org/wiki/ImageNet">120 dog breeds&lt;/a>, it could probably learn visual features that help radiologists locate potential anomalies. But the guess-and-check strategy, despite being sufficient for many advanced computer vision algorithms, flounders when it has access to only a few hundred training examples. Computers have the potential to do some very clever things, but there is not always enough data to supervise their training.&lt;/p>
&lt;p>To mitigate a lack of data, one developing solution is a GAN. A common analogy for these networks envisions art forgery (&lt;a href="https://www.tensorflow.org/tutorials/generative/dcgan">e.g.&lt;/a>), a forger and a critic collaborating to learn about an artist. The forger paints fake works in the style of van Gough, while the critic distinguishes the fake from the real van Goughs. For the forger to succeed, it must paint the essences of van Gough: the reductionist features like the strokes and the yellows, and the holistic feelings of urgency and presence. For the critic to succeed, it must identify those essences, learning the sharp boundaries between longing and yearning. As the forgeries improve, the critic becomes more discerning, further inspiring the forger. Although the networks are taught the essences – the labels – explicitly, the two together learn about van Gough. And they’ll learn without supervision.&lt;/p>
&lt;p>After learning, the critic can be deployed for standard categorization tasks (e.g., aiding medical diagnoses). But the training also produces another useful machine, a machine that is capable of generating images. Predictably, there are challenges to training a generator that is capable of producing good quality, large, and diverse images. But I didn’t need the images to be stellar, so long as their content was clear (to a human). A lack of photorealism – imperfect training – could make the pictures more interesting&lt;a href="#fn1" class="footnote-ref" id="fnref1">&lt;sup>1&lt;/sup>&lt;/a>. To make a gift, I wanted a forger that could paint New England.&lt;/p>
&lt;/div>
&lt;div id="setup" class="section level1">
&lt;h1>Setup&lt;/h1>
&lt;p>I wanted the forger to generate images of New England, so I first needed a bunch of pictures of New England. I have photographed a few hundred pictures, but this wouldn’t be nearly enough&lt;a href="#fn2" class="footnote-ref" id="fnref2">&lt;sup>2&lt;/sup>&lt;/a>. Instead, I relied on a combination of Google’s &lt;a href="https://developers.google.com/maps/documentation/streetview/overview">Street View Static&lt;/a> and &lt;a href="https://developers.google.com/maps/documentation/directions/overview">Directions&lt;/a> APIs. The Street View API gives a picture associated with a location, and those locations were provided by the Directions API. &lt;a href="https://github.com/psadil/gan-mass">The repository&lt;/a> for the network has the details, but the result was that I could input an origin and a destination – meandering through a few waypoints – and download whatever the Street View Car recorded when it traveled along those directions&lt;a href="#fn3" class="footnote-ref" id="fnref3">&lt;sup>3&lt;/sup>&lt;/a>. In the end, I collected ~25,000 images.&lt;/p>
&lt;p>25k may feel like a lot of images. But skimming online suggested that &lt;a href="https://blogs.nvidia.com/blog/2020/12/07/neurips-research-limited-data-gan/">even 25k would not have been enough to adequately constrain the networks&lt;/a>. GANs may not require labeled examples, but they are still data-hungry. Given my relatively small dataset, I picked an adversarial architecture that incorporates a few extra tricks to glean information from smaller datasets: &lt;a href="https://github.com/NVlabs/stylegan2-ada">stylegan2-ada&lt;/a>&lt;a href="#fn4" class="footnote-ref" id="fnref4">&lt;sup>4&lt;/sup>&lt;/a>. To train the network, I used free credits on the Google’s cloud console.&lt;/p>
&lt;/div>
&lt;div id="curated-samples" class="section level1">
&lt;h1>Curated Samples&lt;/h1>
&lt;p>After one day of training&lt;a href="#fn5" class="footnote-ref" id="fnref5">&lt;sup>5&lt;/sup>&lt;/a>, the network started producing some useful images.&lt;/p>
&lt;div class="figure">
&lt;img src="print_5x7_256.png" alt="" />
&lt;p class="caption">These six images are fake, produced from a collaboration to learn about New England.&lt;/p>
&lt;/div>
&lt;p>I chose these six – and the seventh at the top – because they illustrate a few fun features of what the GAN learned. For example, the GAN learned, very early, that pictures of New England always have, in the bottom corners, the word “Google”&lt;a href="#fn6" class="footnote-ref" id="fnref6">&lt;sup>6&lt;/sup>&lt;/a>. That machine learning can produce realistic text surprises me (e.g., &lt;a href="https://www.facebook.com/botsofnewyork/photos/a.2028566864113743/2490502274586864/?type=3&amp;amp;theater">if the face is weird, how are all of the pixels in place to spell out a word&lt;/a>?!). I assume that text comes out clean because most lettering is tightly constrained. That is, when the forger paints something that could be categorized as lettering, the critic severely constrains those pixels; fuzzy letters betray forgery, and real photographs don’t have nonsense like UNS;QD*LKJ. So if the training images contain enough text that the generator starts producing letters, there is also enough text for the critic to learn what text is realistic.&lt;/p>
&lt;p>The forger had difficulty with buildings. I downloaded mostly images of the highways connecting cities. This means that there were enough cityscapes for the GAN to generate buildings, but relative to a road, it was much slower at learning the intricacies of a building. Of course, the roads are imperfect, too (the telephone pole in the upper middle ripples, the upper left has too many roads, the colors of the painted lines mismatch, etc). But unlike, say, a bad photoshop, these errors have a kind of global coherence that, subjectively, allows the images to seem not fake but instead surreal.&lt;/p>
&lt;/div>
&lt;div class="footnotes">
&lt;hr />
&lt;ol>
&lt;li id="fn1">&lt;p>If I wanted perfect pictures, I could have just used a camera.&lt;a href="#fnref1" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn2">&lt;p>The van Gough example is slightly misleading; in practice, van Gough didn’t paint enough pictures to train a GAN. Training a GAN from scratch doesn’t require labeled data, but it still requires many images. There are tricks that could help a GAN, but simply training his images would likely be insufficient.&lt;a href="#fnref2" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn3">&lt;p>Having not owned a car during graduate school, I found it funny that these networks learned about New England through its highways&lt;a href="#fnref3" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn4">&lt;p>But also, they provided a helpful docker image, functions to prep the data, and decent documentation. This is a good reminder about the benefits of polishing a repository.&lt;a href="#fnref4" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn5">&lt;p>After one day, the training error was still decreasing. But I was using a &lt;a href="https://cloud.google.com/compute/docs/instances/preemptible">preemptible virtual machine&lt;/a>, and so after 24 hours it was automatically shutdown.&lt;a href="#fnref5" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn6">&lt;p>I removed the text from the curated examples, but it can be seen in the preview image at the top.&lt;a href="#fnref6" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>Amazon Mechanical Turk in cMAP-CEMNL, part 1</title><link>https://psadil.github.io/psadil/post/mechanical-turk-part-i/</link><pubDate>Fri, 20 Jul 2018 00:00:00 +0000</pubDate><guid>https://psadil.github.io/psadil/post/mechanical-turk-part-i/</guid><description>
&lt;script src="https://psadil.github.io/psadil/post/mechanical-turk-part-i/index.en_files/header-attrs/header-attrs.js">&lt;/script>
&lt;div id="goals" class="section level1">
&lt;h1>Goals&lt;/h1>
&lt;ol style="list-style-type: decimal">
&lt;li>Provide high-level overview of the tools used to run an MTurk study&lt;/li>
&lt;li>Highlight steps at which to be careful while setting the study up&lt;/li>
&lt;/ol>
&lt;p>Part 2 will cover a basic project&lt;/p>
&lt;/div>
&lt;div id="the-current-lab-practice-is-to-string-together-many-different-tools" class="section level1">
&lt;h1>The current lab practice is to string together many different tools&lt;/h1>
&lt;p>Setting up a study on MTurk can roughly be divided into three needs.&lt;/p>
&lt;ul>
&lt;li>We need some way to code the experiment
&lt;ul>
&lt;li>Packages
&lt;ul>
&lt;li>&lt;a href="https://www.jspsych.org">jspsych&lt;/a>&lt;/li>
&lt;li>jquery &lt;a href="https://www.w3schools.com/jquery/default.asp">w3schools&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>Languages
&lt;ul>
&lt;li>Javascript &lt;a href="https://www.w3schools.com/js/default.asp">w3schools&lt;/a>&lt;/li>
&lt;li>CSS &lt;a href="https://www.w3schools.com/css/default.asp">w3schools&lt;/a>&lt;/li>
&lt;li>HTML &lt;a href="https://www.w3schools.com/html/default.asp">w3schools&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>Useful concepts
&lt;ul>
&lt;li>Turns out that, in javascript, line 2 might not run after line 1! It’s helpful to read about &lt;a href="https://javascript.info/callbacks">asynchronous programming in javascript&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://about.gitlab.com/2016/06/03/ssg-overview-gitlab-pages-part-1-dynamic-x-static/">Static sites, dynamic sites, and static site generators&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>We need some way to organize that experimental code such that it is advertised to turkers, Amazon is informed when a Turker finishes the experiment, their responses are stored in a database, and we can approve the turkers’ work
&lt;ul>
&lt;li>Packages
&lt;ul>
&lt;li>&lt;a href="https://psiturk.readthedocs.io/en/latest/">psiturk&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://flask.palletsprojects.com/en/2.0.x/">flask&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.palletsprojects.com/p/jinja/">jinja&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>Languages
&lt;ul>
&lt;li>&lt;a href="https://python.swaroopch.com/">Python&lt;/a>, &lt;a href="https://www.anaconda.com/download/">Anaconda distribution&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://cran.r-project.org/web/packages/dbplyr/vignettes/dbplyr.html">SQLite&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>Useful concepts
&lt;ul>
&lt;li>&lt;a href="https://taylorwhitten.github.io/blog/RSQLite1">what is a database?&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.howtogeek.com/66214/how-to-forward-ports-on-your-router/">what is an ip address?&lt;/a>&lt;/li>
&lt;li>Psiturk runs on python 2, not python 3. You may want to be using an &lt;a href="https://conda.io/docs/user-guide/tasks/manage-environments.html">Anaconda virtual environment&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>We need a server to host the above components such that they are accessible via an internet connection
&lt;ul>
&lt;li>Services
&lt;ul>
&lt;li>Amazon Mechanical Turk: follow the &lt;a href="https://psiturk.readthedocs.io/en/latest/amt_setup.html#">psiturk documentation!&lt;/a>&lt;/li>
&lt;li>Amazon EC2: follow the &lt;a href="https://psiturk.readthedocs.io/en/latest/amazon_ec2.html">psiturk documentation!&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>useful concepts
&lt;ul>
&lt;li>definitely keep the psiturk documentation open in a tab somewhere&lt;/li>
&lt;/ul>&lt;/li>
&lt;/ul>&lt;/li>
&lt;/ul>
&lt;p>In reverse, the EC2 instance is the server that will host the experiment. Psiturk organizes a bunch of packages (which are primarily written in python) to function as the ‘backend,’ coordinating the different webpages that the turk worker will see. Although psiturk also includes a javascript library that could be used to write the actual experiment, it will probably be much easier to write the experiment with the jspsych package.&lt;/p>
&lt;p>At the next deeper level, it’s useful to know how HTML, javascript, and CSS work to make websites run.&lt;/p>
&lt;p>Next, the template part comes from the idea of a static site generator. The particular SSG is jinja. It’s useful to know about Flask, because the Flask syntax defines custom routes (e.g., helping determine which stimuli to show a given participant). Jinja might be useful, but I haven’t found it necessary to know more than the basics of what is an SSG.&lt;/p>
&lt;p>&lt;a href="https://gureckislab.org/courses/spring14/online_data_collection/">Todd Gureckis’ videotapped course lectures&lt;/a>
&lt;a href="https://bradylab.ucsd.edu/ttt/index.html">Tim Brady’s Mechanical Turk Tutorial&lt;/a>
&lt;a href="https://wilmabainbridge.com/bigdataclass.html">Wilma Bainbridge organized a Big Data Tutorial at VSS 2018&lt;/a>. Note, this involves the package psitoolkit. That seemed like an okay alternative, but I worried that I would encounter a situation that psitoolkit wasn’t equipped to handle and would be stuck. Working with the psiturk + jspsych ensured that there would be the flexibility to run pretty much any kind of experiment&lt;/p>
&lt;/div>
&lt;div id="gotchas-and-extra-notes" class="section level1">
&lt;h1>Gotcha’s and extra notes&lt;/h1>
&lt;ul>
&lt;li>&lt;p>Psiturk runs on python 2, not 3. If you’re trying to install psiturk and you immediately start getting errors, make sure to check which python version you’re on&lt;/p>&lt;/li>
&lt;li>&lt;p>when setting up EC2, pay careful attention to the IP configuration settings. To be able to use ssh to access the instance, you’ll need to have your IP address match the IP it’s expecting. Or, just set it to receive traffic from ‘Anywhere.’ Likewise, make sure that the custom TCP is set to receive traffic from Anywhere&lt;/p>&lt;/li>
&lt;li>&lt;p>Do read the tutorial on &lt;a href="https://javascript.info/callbacks">asynchronous programming in javascript&lt;/a>. It can be really confusing when you’re trying to debug and variables aren’t defined&lt;/p>&lt;/li>
&lt;li>&lt;p>Speaking of debugging, your friend will be the ‘developer tools’ in whatever browser you’re using. Right click on the experiment and check ‘view source’ to get access to a console&lt;/p>&lt;/li>
&lt;li>&lt;p>Using the psiturk &lt;code>debug&lt;/code> command will attempt to open up the experiment in the browser. This will only work if no browser is currently open.&lt;/p>&lt;/li>
&lt;li>&lt;p>if an experiment involves a lot of media, the media can be optimized a bit using something like &lt;a href="http://optipng.sourceforge.net/">optipng&lt;/a>. This keeps the png looking fine but decreases load time&lt;/p>&lt;/li>
&lt;/ul>
&lt;/div></description></item></channel></rss>