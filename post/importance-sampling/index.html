<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.2.0 for Hugo"><meta name=author content="Patrick Sadil"><meta name=description content="This post attempts to provide a brief intro to importance sampling, presenting a relatively simple situation in which importance sampling enables a speedup in Monte Carlo by reducing the variance of the estimate.
A lot of the material will be an effort to remember material that was taught to be in a Stochastic Processes and Applications class. This post should be though of as my notes on the course notes, with supplementation by https://www."><link rel=alternate hreflang=en-us href=https://psadil.github.io/psadil/post/importance-sampling/><link rel=preconnect href=https://fonts.gstatic.com crossorigin><meta name=theme-color content="#1565c0"><script src=/psadil/js/mathjax-config.js></script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/atom-one-dark.min.css crossorigin=anonymous title=hl-light media=print onload="this.media='all'"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/atom-one-dark.min.css crossorigin=anonymous title=hl-dark media=print onload="this.media='all'" disabled><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js integrity crossorigin=anonymous async></script><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Atkinson+Hyperlegible:ital,wght@0,400;0,700;1,400;1,700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Atkinson+Hyperlegible:ital,wght@0,400;0,700;1,400;1,700&display=swap" media=print onload="this.media='all'"><link rel=stylesheet href=/psadil/css/wowchemy.8bc78ecea639e288ab749bc0faab617c.css><script async src="https://www.googletagmanager.com/gtag/js?id=G-SP0TP763NP"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}function trackOutboundLink(a,b){gtag('event','click',{event_category:'outbound',event_label:a,transport_type:'beacon',event_callback:function(){b!=='_blank'&&(document.location=a)}}),console.debug("Outbound link clicked: "+a)}function onClickCallback(a){if(a.target.tagName!=='A'||a.target.host===window.location.host)return;trackOutboundLink(a.target,a.target.getAttribute('target'))}gtag('js',new Date),gtag('config','G-SP0TP763NP',{}),gtag('set',{cookie_flags:'SameSite=None;Secure'}),document.addEventListener('click',onClickCallback,!1)</script><link rel=manifest href=/psadil/index.webmanifest><link rel=icon type=image/png href=/psadil/media/icon_hu3896c2ce465988ba1fc8077f9a6388c6_268630_32x32_fill_lanczos_center_2.png><link rel=apple-touch-icon type=image/png href=/psadil/media/icon_hu3896c2ce465988ba1fc8077f9a6388c6_268630_180x180_fill_lanczos_center_2.png><link rel=canonical href=https://psadil.github.io/psadil/post/importance-sampling/><meta property="twitter:card" content="summary_large_image"><meta property="twitter:site" content="@psadil"><meta property="twitter:creator" content="@psadil"><meta property="og:site_name" content="psadil"><meta property="og:url" content="https://psadil.github.io/psadil/post/importance-sampling/"><meta property="og:title" content="Basic Importance Sampling for Variance Reduction | psadil"><meta property="og:description" content="This post attempts to provide a brief intro to importance sampling, presenting a relatively simple situation in which importance sampling enables a speedup in Monte Carlo by reducing the variance of the estimate.
A lot of the material will be an effort to remember material that was taught to be in a Stochastic Processes and Applications class. This post should be though of as my notes on the course notes, with supplementation by https://www."><meta property="og:image" content="https://psadil.github.io/psadil/post/importance-sampling/featured.png"><meta property="twitter:image" content="https://psadil.github.io/psadil/post/importance-sampling/featured.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2018-11-10T00:00:00+00:00"><meta property="article:modified_time" content="2024-11-27T08:15:32-05:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://psadil.github.io/psadil/post/importance-sampling/"},"headline":"Basic Importance Sampling for Variance Reduction","image":["https://psadil.github.io/psadil/post/importance-sampling/featured.png"],"datePublished":"2018-11-10T00:00:00Z","dateModified":"2024-11-27T08:15:32-05:00","author":{"@type":"Person","name":"Patrick Sadil"},"publisher":{"@type":"Organization","name":"psadil","logo":{"@type":"ImageObject","url":"https://psadil.github.io/psadil/media/icon_hu3896c2ce465988ba1fc8077f9a6388c6_268630_192x192_fill_lanczos_center_2.png"}},"description":"This post attempts to provide a brief intro to importance sampling, presenting a relatively simple situation in which importance sampling enables a speedup in Monte Carlo by reducing the variance of the estimate.\nA lot of the material will be an effort to remember material that was taught to be in a Stochastic Processes and Applications class. This post should be though of as my notes on the course notes, with supplementation by https://www."}</script><title>Basic Importance Sampling for Variance Reduction | psadil</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=ffe423688d255c6db762ff940a720600><script src=/psadil/js/wowchemy-init.min.b8153d4570dcbb34350a2a846dba8c03.js></script><div class=page-header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/psadil/>psadil</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/psadil/>psadil</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/psadil/publication><span>Publications</span></a></li><li class=nav-item><a class="nav-link active" href=/psadil/post><span>Blog</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></div><div class=page-body><article class=article><div class="article-container pt-3"><h1>Basic Importance Sampling for Variance Reduction</h1><div class=article-metadata><span class=article-date>Last updated on
Nov 27, 2024</span>
<span class=middot-divider></span><span class=article-reading-time>7 min read</span>
<span class=middot-divider></span><span class=article-categories><i class="fas fa-folder mr-1"></i><a href=/psadil/category/comps/>comps</a></span></div></div><div class="article-header article-container featured-image-wrapper mt-4 mb-4" style=max-width:480px;max-height:480px><div style=position:relative><img src=/psadil/post/importance-sampling/featured.png alt class=featured-image></div></div><div class=article-container><div class=article-style><script src=https://psadil.github.io/psadil/post/importance-sampling/index.en_files/header-attrs/header-attrs.js></script><p>This post attempts to provide a brief intro to importance sampling, presenting a relatively simple situation in which importance sampling enables a speedup in Monte Carlo by reducing the variance of the estimate.</p><p>A lot of the material will be an effort to remember material that was taught to be in a Stochastic Processes and Applications class. This post should be though of as my notes on the <a href=http://people.math.umass.edu/~lr7q/m697u-spring2018/m697uhome.html>course notes</a>, with supplementation by <a href=https://www.statlect.com/asymptotic-theory/importance-sampling class=uri>https://www.statlect.com/asymptotic-theory/importance-sampling</a>. That is, it can safely be assumed that any content comes from those notes, excluding errors (which will be due to my translation)<a href=#fn1 class=footnote-ref id=fnref1><sup>1</sup></a>.</p><div id=what-is-importance-sampling class="section level1"><h1>What is importance sampling?</h1><p>Importance sampling comes up in the context of Monte Carlo, (MC) applications as a technique for reducing the variability in the MC estimator. As a reminder, MC can be used to estimate the expected value, <span class="math inline">\(\mu\)</span>, of a random variable, <span class="math inline">\(X\)</span>.</p><p><span class="math display">\[
\mu = \mathbb{E}[h(x)] = \int h(x)p_X(x)\,dx
\]</span></p><p>The idea of Monte Carlo is that this expectation can be estimated by drawing <span class="math inline">\(S\)</span> samples from the distribution <span class="math inline">\(p_X\)</span>, where <span class="math inline">\(X \sim p_X\)</span></p><p><span class="math display">\[
\hat{\mu} =\frac{1}{S}\sum_{s=1}^S h(x_s)
\]</span></p><p>where the subscript on <span class="math inline">\(x\)</span> implies the <span class="math inline">\(s^th\)</span> draw of <span class="math inline">\(X\)</span>, and the hat over <span class="math inline">\(\mu\)</span> indicates that the result is an estimate. Note that, since we’re starting from that equation, it is assumed that there is some way to produce random samples from the distribution <span class="math inline">\(p_X\)</span>, and that the function, <span class="math inline">\(h\)</span> is calculable for any <span class="math inline">\(X\)</span>. Also, <span class="math inline">\(h\)</span> might be something as simple as <span class="math inline">\(h(x) = x\)</span> if the expectation should correspond to the mean of <span class="math inline">\(x\)</span>].</p><p>This is a powerful idea, though a general downside is that some <span class="math inline">\(\mu\)</span> require many samples to obtain a reasonable estimate. The MC Standard Error (MCSE) provides an estimate of the approximation error, which is <span class="math inline">\(\frac{1}{n} Var(h(X))\)</span>. This provides a metric by which to gauge different algorithms. In particular, if all else is equal, the estimator with lower variance will can obtain a given level of precision with a lower <span class="math inline">\(S\)</span>.</p><p>The basic idea with Importance Sampling (IS) is to draw samples from some alternative distribution, <span class="math inline">\(p_Y\)</span>, which has the same support as <span class="math inline">\(p_X\)</span>, then reweight those samples in accordance with the difference between <span class="math inline">\(p_X\)</span> and <span class="math inline">\(p_Y\)</span>.</p><p><span class="math display">\[
\begin{aligned}
\mathbb{E}[h(x)] & = \int h(x)p_X(x) \,dx & \textrm{definition of expectation} \\
& = \int h(x)\frac{p_X(x)}{p_Y(x)}p_Y(x) \,dx & \textrm{multiplication by 1, assuming same support} \\
& \int h(y)\frac{p_X(y)}{p_Y(y)}p_Y(y) \,dy & \textrm{assuming same support} \\
& = \mathbb{E} \left[h(y)\frac{p_X(y)}{p_Y(y)} \right] & \textrm{our new importance sampling estimator}
\end{aligned}
\]</span></p><p>Recognize that there will often not be a single unique <span class="math inline">\(p_Y\)</span>. The goal is to find a <span class="math inline">\(p_Y\)</span> that results in lower MCSE. The MCSE for the importance sampling estimator is <span class="math inline">\(\frac{1}{n}Var\left[h(y)\frac{p_X(y)}{p_Y(y)} \right]\)</span>. That will be used to gain an intuition for how to choose a useful <span class="math inline">\(p_Y\)</span>.</p></div><div id=why-does-importance-sampling-work class="section level1"><h1>Why does importance sampling work?</h1><p>One way to think about importance sampling is that, if we could sample from <span class="math inline">\(h(y)\frac{p_X(y)}{p_Y(y)}\)</span> such that the result were constant, the variance would be 0, and so we’d only need a single sample. To see why, note that, for a constant <span class="math inline">\(c\)</span></p><p><span class="math display">\[
\begin{aligned}
h(y)\frac{p_X(y)}{p_Y(y)} & = c \\
\implies p_Y(y)c & = h(y)p_X(y) \\
\implies p_Y(y) & \propto h(y)p_X(y) \\
\end{aligned}
\]</span></p><p>That is, <span class="math inline">\(h(y)\frac{p_X(y)}{p_Y(y)}\)</span> will be constant whenever <span class="math inline">\(p_Y(y)\)</span> is proportional to <span class="math inline">\(h(y)p_X(x)\)</span>.</p><p><span class="math display">\[
\begin{aligned}
p_Y(y) & = \frac{h(y)p_X(y)}{\int h(y)p_X(y)\,dy} \\
\implies p_Y(y) & = \frac{h(y)p_X(y)}{\mathbb{E}[h(X)]} \\
& = \frac{h(y)p_X(y)}{\mu} & \textrm {definition of }\mu
\end{aligned}
\]</span></p><p>Plugging this distribution into the IS estimator</p><p><span class="math display">\[
\begin{aligned}
\frac{1}{S} \sum_{s=1}^S \frac{h(Y_s)p_X(Y_s)}{p_Y(Y_s)} & = \frac{1}{S} \sum_{s=1}^S \frac{h(Y_s)p_X(Y_s)}{\frac{h(Y_s)p_X(Y_s)}{\mathbb{E}[h(X_s)]}} \\
& = \frac{1}{S} S\mu \\
& = \mu
\end{aligned}
\]</span></p><p>So, regardless of <span class="math inline">\(S\)</span>, the resulting estimator is always <span class="math inline">\(\mu\)</span>.</p><p>That’s almost useful, but this means that to get an optimal <span class="math inline">\(p_Y\)</span> we need to know <span class="math inline">\(\mathbb{E}[h(X)]\)</span>, which is by definition the <span class="math inline">\(\mu\)</span> that we’re trying to estimate. Still, knowing what is optimal provides the intuition for why importance sampling works.</p><p>There are two ideas going on here. First, the optimal <span class="math inline">\(p_Y\)</span> is one which places higher density on regions where <span class="math inline">\(h(X)\)</span> is high, as compared to <span class="math inline">\(p_X\)</span>. Those “important” values are the ones that will determine the result of <span class="math inline">\(h(x)\)</span>, so those are the ones that need to be altered the most (going from <span class="math inline">\(p_X\)</span> to <span class="math inline">\(p_Y\)</span>). Then, the second idea is that when averaging, we must account for the oversampling of these points – downweighting them by however much they were oversampled, which is the ratio <span class="math inline">\(\frac{p_X(y)}{p_Y(y)}\)</span>.</p></div><div id=using-is-to-reduce-variance class="section level1"><h1>Using IS to reduce variance</h1><p>Here’s an example of this working out. The value we’re trying to estimate will be, for <span class="math inline">\(X \sim N(0,1)\)</span></p><p><span class="math display">\[
\mu = \int \phi(x-4)p_X(x)\,dx
\]</span></p><p>where <span class="math inline">\(\phi\)</span> is the standard normal density function. This <span class="math inline">\(h\)</span> is such that only values near 4 provide much contribution to the average.</p><pre class=r><code>set.seed(1234)
hx &lt;- function(x) {
    return(dnorm(x - 4))
}
x &lt;- seq(-5, 10, length.out = 1000)
plot(x, hx(x), type = &quot;l&quot;)
points(x, dnorm(x), col = &quot;blue&quot;, type = &quot;l&quot;)</code></pre><div class=figure><span id=fig:mismatch></span><img src=https://psadil.github.io/psadil/post/importance-sampling/index.en_files/figure-html/mismatch-1.png alt="h(x) (black) has mostly 0 around where p_X (blue) has high density. The mismatch of these distributions will contribute to unnecessarily high variance of the estimator." width=672><p class=caption>Figure 1: h(x) (black) has mostly 0 around where p_X (blue) has high density. The mismatch of these distributions will contribute to unnecessarily high variance of the estimator.</p></div><p>However, <span class="math inline">\(X\)</span> will only rarely contain values that are near 4. Hence, the variance of the estimation will be relatively high. We can get an explicit formula for the variance.</p><p><span class="math display">\[
Var(h(x)) = \mathbb{E}[h(x)^2] - \mathbb{E}[h(x)]^2
\]</span></p><p>A formula that involves calculating the expected value of this function</p><p><span class="math display">\[
\begin{aligned}
\mathbb{E}[h(x)] & = \int_{-\infty}^{\infty} \left(\frac{\exp(- \frac{x^2}{2})}{\sqrt{2\pi}} \right)\left(\frac{\exp(- \frac{x^2}{2})}{\sqrt{2\pi}} \right) dx \\
& = \int_{-\infty}^{\infty} \frac{\exp (- x^2 + 4x - 8 )}{2\pi} dx \\
& = \frac{1}{2\pi} \int_{-\infty}^{\infty} \exp (- x^2 + 4x - 8 ) dx\\
& = \frac{1}{2\pi} \sqrt{\pi}\exp \left(\frac{4^2}{4}-8 \right) & \textrm{en.wikipedia.org/wiki/Gaussian_function} \\
& = \frac{1}{2 \exp(4) \sqrt{\pi}}
\end{aligned}
\]</span></p><p>Which we’ll save for now to use later</p><pre class=r><code>mu &lt;- 1/(2 * exp(4) * sqrt(pi))
mu</code></pre><pre><code>## [1] 0.005166746</code></pre><p>Returning to the variance calculation</p><p><span class="math display">\[
\begin{aligned}
Var(h(x)) & = \left[ \int_{-\infty}^{\infty} \left(\frac{\exp(- \frac{(x-2)^2}{2})}{\sqrt{2\pi}} \right)^2 \left(\frac{\exp(- \frac{x^2}{2})}{\sqrt{2\pi}} \right) \,dx \right] - \mu^2 \\
& = \frac{1}{2\sqrt{2}\pi^{3/2}} \int_{-\infty}^{\infty} \exp\left(-\frac{3}{2}x^2+8x-16 \right) \,dx - \mu^2 \\
& = \frac{1}{2\sqrt{2}\pi^{3/2}} \sqrt{\frac{\pi}{3/2}}\exp \left(\frac{8^2}{6} -16 \right) \\
& = \frac{1}{2 \pi \sqrt{3} \exp(16/3)} - \mu^2
\end{aligned}
\]</span></p><pre class=r><code>1/(2 * pi * sqrt(3) * exp(16/3)) - mu^2</code></pre><pre><code>## [1] 0.0004169361</code></pre><div id=standard-mc-estimate-is-accurate-but-with-relatively-high-variance class="section level3"><h3>Standard MC estimate is accurate, but with relatively high variance</h3><p>Using an MC estimate,</p><pre class=r><code>x &lt;- rnorm(1e+06)
y &lt;- hx(x)
var(y)</code></pre><pre><code>## [1] 0.0004270083</code></pre><p>Note also that the estimate (our target), is also accurate</p><pre class=r><code>mean(y) - mu</code></pre><pre><code>## [1] 4.582938e-05</code></pre></div><div id=using-a-distribution-that-simply-matches-hx-is-also-not-so-great class="section level3"><h3>Using a distribution that simply matches h(x) is also not-so-great</h3><p>Now, let’s instead construct and IS estimator. One intuitive (but by no means optimal) attempt would be to use <span class="math inline">\(Y \sim N(4,1)\)</span>, a distribution that matches with <span class="math inline">\(h(x)\)</span> perfectly. Indeed, that will provide an accurate answer</p><pre class=r><code>y &lt;- rnorm(1e+06, mean = 4)
h &lt;- hx(y)
IS &lt;- h * dnorm(y, 0, 1)/dnorm(y, 4, 1)
mean(IS)</code></pre><pre><code>## [1] 0.005194268</code></pre><p>But, it turns out that the variance is about the same as before.</p><pre class=r><code>var(IS)</code></pre><pre><code>## [1] 0.0004204215</code></pre></div><div id=the-proposal-distribution-needs-to-be-tuned-to-both-p_x-and-hx class="section level3"><h3>The proposal distribution needs to be tuned to both p_X and h(x)</h3><p>This is a somewhat subtle point of the derivation provided above. We <em>don’t</em> just want a distribution that will be highest here <span class="math inline">\(h(x)\)</span> is high. Instead, what we actually need is a distribution that will be highest when <span class="math inline">\(h(x)p_X(x)\)</span> is high. That will be exactly where the two distributions intersect, at 2.</p><pre class=r><code>x &lt;- seq(-5, 10, length.out = 1000)
plot(x, hx(x), type = &quot;l&quot;)
points(x, dnorm(x), col = &quot;blue&quot;, type = &quot;l&quot;)
abline(v = 2)</code></pre><div class=figure><span id=fig:matching></span><img src=https://psadil.github.io/psadil/post/importance-sampling/index.en_files/figure-html/matching-1.png alt="Same as above, but with line demonstrating intersection at x=2" width=672><p class=caption>Figure 2: Same as above, but with line demonstrating intersection at x=2</p></div><pre class=r><code>y &lt;- rnorm(1e+06, mean = 2)
h &lt;- hx(y)
IS &lt;- h * dnorm(y, 0)/dnorm(y, 2)
mean(IS)</code></pre><pre><code>## [1] 0.005165774</code></pre><pre class=r><code>var(IS)</code></pre><pre><code>## [1] 4.131799e-06</code></pre><p>The answer is still accurate, but the variance has been reduced by many factors. This means that a desired level of MCSE could be achieved with many fewer samples.</p><p>One final demonstration, remember that <span class="math inline">\(h(x)p_X(x)\)</span> describes a distribution. Hence it would be a mistake to try a <span class="math inline">\(p_Y\)</span> that placed all of the density around that point of intersection. For example, let’s try <span class="math inline">\(Y \sim N(2,0.1)\)</span>. Although that is centered on the region that contributes the largest values to the expectation, the largest values alone do not define the expectation; too much of the tails of <span class="math inline">\(h(x)\)</span> are not included. Using this results is the worst variance.</p><pre class=r><code>y &lt;- rnorm(1e+06, mean = 2, 0.1)
h &lt;- hx(y)
IS &lt;- h * dnorm(y, 0)/dnorm(y, 2, 0.1)
mean(IS)</code></pre><pre><code>## [1] 0.002601838</code></pre><pre class=r><code>var(IS)</code></pre><pre><code>## [1] 0.006540712</code></pre></div></div><div id=other-references class="section level1"><h1>Other References</h1><ul><li><a href=https://en.wikipedia.org/wiki/Gaussian_function#Integral_of_a_Gaussian_function>integral of Gaussian Function</a></li><li><a href=http://people.math.umass.edu/~lr7q/m697u-spring2018/m697uhome.html>course notes</a></li><li><a href=https://www.statlect.com/asymptotic-theory/importance-sampling>statlect</a></li></ul></div><div class=footnotes><hr><ol><li id=fn1><p>this page is mostly a study page for upcoming comprehensive exams<a href=#fnref1 class=footnote-back>↩︎</a></p></li></ol></div></div><p class=edit-page><a href=https://github.com/psadil/psadil/blob/main/content/post/2018-11-10-importance-sampling/index.en.Rmd>View Page Source</a></p><div class=article-tags><a class="badge badge-light" href=/psadil/tag/mc/>MC</a></div><div class=share-box aria-hidden=true><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https://psadil.github.io/psadil/post/importance-sampling/&text=Basic%20Importance%20Sampling%20for%20Variance%20Reduction" target=_blank rel=noopener class=share-btn-twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https://psadil.github.io/psadil/post/importance-sampling/&t=Basic%20Importance%20Sampling%20for%20Variance%20Reduction" target=_blank rel=noopener class=share-btn-facebook><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=Basic%20Importance%20Sampling%20for%20Variance%20Reduction&body=https://psadil.github.io/psadil/post/importance-sampling/" target=_blank rel=noopener class=share-btn-email><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https://psadil.github.io/psadil/post/importance-sampling/&title=Basic%20Importance%20Sampling%20for%20Variance%20Reduction" target=_blank rel=noopener class=share-btn-linkedin><i class="fab fa-linkedin-in"></i></a></li><li><a href="whatsapp://send?text=Basic%20Importance%20Sampling%20for%20Variance%20Reduction%20https://psadil.github.io/psadil/post/importance-sampling/" target=_blank rel=noopener class=share-btn-whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=https://psadil.github.io/psadil/post/importance-sampling/&title=Basic%20Importance%20Sampling%20for%20Variance%20Reduction" target=_blank rel=noopener class=share-btn-weibo><i class="fab fa-weibo"></i></a></li></ul></div><div class="media author-card content-widget-hr"><a href=https://psadil.github.io/psadil/><img class="avatar mr-3 avatar-circle" src="https://s.gravatar.com/avatar/3c65844f0569ce42e93153c270c2daed?s=200" alt="Patrick Sadil"></a><div class=media-body><h5 class=card-title><a href=https://psadil.github.io/psadil/>Patrick Sadil</a></h5><h6 class=card-subtitle>Research Associate; Biostatistics Faculty</h6><ul class=network-icon aria-hidden=true><li><a href=mailto:psadil1@jh.edu><i class="fas fa-envelope"></i></a></li><li><a href=https://psadil.github.io/cv/cv.pdf target=_blank rel=noopener><i class="ai ai-cv"></i></a></li><li><a href=//orcid.org/0000-0003-4141-1343><i class="ai ai-orcid"></i></a></li><li><a href="https://scholar.google.co.uk/citations?user=HhPeElcAAAAJ" target=_blank rel=noopener><i class="ai ai-google-scholar"></i></a></li><li><a href=https://github.com/psadil target=_blank rel=noopener><i class="fab fa-github"></i></a></li></ul></div></div></div></article></div><div class=page-footer><div class=container><footer class=site-footer><p class=powered-by><a href=/psadil/privacy/>Privacy Policy</a>
&#183;
<a href=/psadil/terms/>Terms</a></p><p class=powered-by>© 2024 Patrick Sadil</p><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-modules target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/matlab.min.js></script><script src=/psadil/js/bootstrap.bundle.min.6aed84840afc03ab4d5750157f69c120.js></script><script src=/psadil/en/js/wowchemy.min.c7dbcd0a03997d954a216caf59fc7681.js></script></body></html>