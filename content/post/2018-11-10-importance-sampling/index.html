---
title: Basic Importance Sampling for Variance Reduction
author: Patrick Sadil
date: '2018-11-10'
slug: importance-sampling
categories:
  - comps
tags:
  - MC
math: true
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<p>This post attempts to provide a brief intro to importance sampling, presenting a relatively simple situation in which importance sampling enables a speedup in Monte Carlo by reducing the variance of the estimate.</p>
<p>A lot of the material will be an effort to remember material that was taught to be in a Stochastic Processes and Applications class. This post should be though of as my notes on the <a href="http://people.math.umass.edu/~lr7q/m697u-spring2018/m697uhome.html">course notes</a>, with supplementation by <a href="https://www.statlect.com/asymptotic-theory/importance-sampling" class="uri">https://www.statlect.com/asymptotic-theory/importance-sampling</a>. That is, it can safely be assumed that any content comes from those notes, excluding errors (which will be due to my translation)<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>.</p>
<div id="what-is-importance-sampling" class="section level1">
<h1>What is importance sampling?</h1>
<p>Importance sampling comes up in the context of Monte Carlo, (MC) applications as a technique for reducing the variability in the MC estimator. As a reminder, MC can be used to estimate the expected value, <span class="math inline">\(\mu\)</span>, of a random variable, <span class="math inline">\(X\)</span>.</p>
<p><span class="math display">\[
\mu = \mathbb{E}[h(x)] = \int h(x)p_X(x)\,dx
\]</span></p>
<p>The idea of Monte Carlo is that this expectation can be estimated by drawing <span class="math inline">\(S\)</span> samples from the distribution <span class="math inline">\(p_X\)</span>, where <span class="math inline">\(X \sim p_X\)</span></p>
<p><span class="math display">\[
\hat{\mu} =\frac{1}{S}\sum_{s=1}^S h(x_s)
\]</span></p>
<p>where the subscript on <span class="math inline">\(x\)</span> implies the <span class="math inline">\(s^th\)</span> draw of <span class="math inline">\(X\)</span>, and the hat over <span class="math inline">\(\mu\)</span> indicates that the result is an estimate. Note that, since we’re starting from that equation, it is assumed that there is some way to produce random samples from the distribution <span class="math inline">\(p_X\)</span>, and that the function, <span class="math inline">\(h\)</span> is calculable for any <span class="math inline">\(X\)</span>. Also, <span class="math inline">\(h\)</span> might be something as simple as <span class="math inline">\(h(x) = x\)</span> if the expectation should correspond to the mean of <span class="math inline">\(x\)</span>].</p>
<p>This is a powerful idea, though a general downside is that some <span class="math inline">\(\mu\)</span> require many samples to obtain a reasonable estimate. The MC Standard Error (MCSE) provides an estimate of the approximation error, which is <span class="math inline">\(\frac{1}{n} Var(h(X))\)</span>. This provides a metric by which to gauge different algorithms. In particular, if all else is equal, the estimator with lower variance will can obtain a given level of precision with a lower <span class="math inline">\(S\)</span>.</p>
<p>The basic idea with Importance Sampling (IS) is to draw samples from some alternative distribution, <span class="math inline">\(p_Y\)</span>, which has the same support as <span class="math inline">\(p_X\)</span>, then reweight those samples in accordance with the difference between <span class="math inline">\(p_X\)</span> and <span class="math inline">\(p_Y\)</span>.</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}[h(x)] &amp; = \int h(x)p_X(x) \,dx &amp; \textrm{definition of expectation} \\
 &amp; = \int h(x)\frac{p_X(x)}{p_Y(x)}p_Y(x) \,dx &amp; \textrm{multiplication by 1, assuming same support} \\
 &amp;  \int h(y)\frac{p_X(y)}{p_Y(y)}p_Y(y) \,dy &amp; \textrm{assuming same support} \\
 &amp; = \mathbb{E} \left[h(y)\frac{p_X(y)}{p_Y(y)} \right] &amp; \textrm{our new importance sampling estimator}
\end{aligned}
\]</span></p>
<p>Recognize that there will often not be a single unique <span class="math inline">\(p_Y\)</span>. The goal is to find a <span class="math inline">\(p_Y\)</span> that results in lower MCSE. The MCSE for the importance sampling estimator is <span class="math inline">\(\frac{1}{n}Var\left[h(y)\frac{p_X(y)}{p_Y(y)} \right]\)</span>. That will be used to gain an intuition for how to choose a useful <span class="math inline">\(p_Y\)</span>.</p>
</div>
<div id="why-does-importance-sampling-work" class="section level1">
<h1>Why does importance sampling work?</h1>
<p>One way to think about importance sampling is that, if we could sample from <span class="math inline">\(h(y)\frac{p_X(y)}{p_Y(y)}\)</span> such that the result were constant, the variance would be 0, and so we’d only need a single sample. To see why, note that, for a constant <span class="math inline">\(c\)</span></p>
<p><span class="math display">\[
\begin{aligned}
h(y)\frac{p_X(y)}{p_Y(y)} &amp; = c \\
\implies p_Y(y)c &amp; = h(y)p_X(y) \\
\implies p_Y(y) &amp; \propto h(y)p_X(y) \\
\end{aligned}
\]</span></p>
<p>That is, <span class="math inline">\(h(y)\frac{p_X(y)}{p_Y(y)}\)</span> will be constant whenever <span class="math inline">\(p_Y(y)\)</span> is proportional to <span class="math inline">\(h(y)p_X(x)\)</span>.</p>
<p><span class="math display">\[
\begin{aligned}
p_Y(y) &amp; = \frac{h(y)p_X(y)}{\int h(y)p_X(y)\,dy} \\
\implies p_Y(y) &amp; = \frac{h(y)p_X(y)}{\mathbb{E}[h(X)]} \\
 &amp; = \frac{h(y)p_X(y)}{\mu} &amp; \textrm {definition of }\mu
\end{aligned}
\]</span></p>
<p>Plugging this distribution into the IS estimator</p>
<p><span class="math display">\[
\begin{aligned}
\frac{1}{S} \sum_{s=1}^S \frac{h(Y_s)p_X(Y_s)}{p_Y(Y_s)} &amp; = \frac{1}{S} \sum_{s=1}^S \frac{h(Y_s)p_X(Y_s)}{\frac{h(Y_s)p_X(Y_s)}{\mathbb{E}[h(X_s)]}} \\
&amp; = \frac{1}{S} S\mu \\
&amp; = \mu
\end{aligned}
\]</span></p>
<p>So, regardless of <span class="math inline">\(S\)</span>, the resulting estimator is always <span class="math inline">\(\mu\)</span>.</p>
<p>That’s almost useful, but this means that to get an optimal <span class="math inline">\(p_Y\)</span> we need to know <span class="math inline">\(\mathbb{E}[h(X)]\)</span>, which is by definition the <span class="math inline">\(\mu\)</span> that we’re trying to estimate. Still, knowing what is optimal provides the intuition for why importance sampling works.</p>
<p>There are two ideas going on here. First, the optimal <span class="math inline">\(p_Y\)</span> is one which places higher density on regions where <span class="math inline">\(h(X)\)</span> is high, as compared to <span class="math inline">\(p_X\)</span>. Those “important” values are the ones that will determine the result of <span class="math inline">\(h(x)\)</span>, so those are the ones that need to be altered the most (going from <span class="math inline">\(p_X\)</span> to <span class="math inline">\(p_Y\)</span>). Then, the second idea is that when averaging, we must account for the oversampling of these points – downweighting them by however much they were oversampled, which is the ratio <span class="math inline">\(\frac{p_X(y)}{p_Y(y)}\)</span>.</p>
</div>
<div id="using-is-to-reduce-variance" class="section level1">
<h1>Using IS to reduce variance</h1>
<p>Here’s an example of this working out. The value we’re trying to estimate will be, for <span class="math inline">\(X \sim N(0,1)\)</span></p>
<p><span class="math display">\[
\mu = \int \phi(x-4)p_X(x)\,dx
\]</span></p>
<p>where <span class="math inline">\(\phi\)</span> is the standard normal density function. This <span class="math inline">\(h\)</span> is such that only values near 4 provide much contribution to the average.</p>
<pre class="r"><code>hx &lt;- function(x) {
    return(dnorm(x - 4))
}
x &lt;- seq(-5, 10, length.out = 1000)
plot(x, hx(x), type = &quot;l&quot;)
points(x, dnorm(x), col = &quot;blue&quot;, type = &quot;l&quot;)</code></pre>
<div class="figure"><span id="fig:mismatch"></span>
<img src="{{< blogdown/postref >}}index_files/figure-html/mismatch-1.png" alt="h(x) (black) has mostly 0 around where p_X (blue) has high density. The mismatch of these distributions will contribute to unnecessarily high variance of the estimator." width="672" />
<p class="caption">
Figure 1: h(x) (black) has mostly 0 around where p_X (blue) has high density. The mismatch of these distributions will contribute to unnecessarily high variance of the estimator.
</p>
</div>
<p>However, <span class="math inline">\(X\)</span> will only rarely contain values that are near 4. Hence, the variance of the estimation will be relatively high. We can get an explicit formula for the variance.</p>
<p><span class="math display">\[
Var(h(x)) = \mathbb{E}[h(x)^2] - \mathbb{E}[h(x)]^2
\]</span></p>
<p>A formula that involves calculating the expected value of this function</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}[h(x)] &amp; =  \int_{-\infty}^{\infty} \left(\frac{\exp(- \frac{x^2}{2})}{\sqrt{2\pi}} \right)\left(\frac{\exp(- \frac{x^2}{2})}{\sqrt{2\pi}} \right) dx  \\
&amp; = \int_{-\infty}^{\infty} \frac{\exp (- x^2 + 4x - 8 )}{2\pi} dx \\
&amp; = \frac{1}{2\pi} \int_{-\infty}^{\infty} \exp (- x^2 + 4x - 8 ) dx\\
&amp; = \frac{1}{2\pi} \sqrt{\pi}\exp \left(\frac{4^2}{4}-8 \right)  &amp; \textrm{en.wikipedia.org/wiki/Gaussian_function} \\
&amp; = \frac{1}{2 \exp(4) \sqrt{\pi}}
\end{aligned}
\]</span></p>
<p>Which we’ll save for now to use later</p>
<pre class="r"><code>mu &lt;- 1/(2 * exp(4) * sqrt(pi))
mu</code></pre>
<pre><code>## [1] 0.005166746</code></pre>
<p>Returning to the variance calculation</p>
<p><span class="math display">\[
\begin{aligned}
Var(h(x)) &amp; = \left[ \int_{-\infty}^{\infty} \left(\frac{\exp(- \frac{(x-2)^2}{2})}{\sqrt{2\pi}} \right)^2 \left(\frac{\exp(- \frac{x^2}{2})}{\sqrt{2\pi}} \right) \,dx \right] - \mu^2 \\
&amp; = \frac{1}{2\sqrt{2}\pi^{3/2}} \int_{-\infty}^{\infty} \exp\left(-\frac{3}{2}x^2+8x-16 \right)  \,dx - \mu^2 \\
&amp; = \frac{1}{2\sqrt{2}\pi^{3/2}} \sqrt{\frac{\pi}{3/2}}\exp \left(\frac{8^2}{6} -16 \right)  \\
&amp; = \frac{1}{2 \pi \sqrt{3} \exp(16/3)} - \mu^2
\end{aligned}
\]</span></p>
<pre class="r"><code>1/(2 * pi * sqrt(3) * exp(16/3)) - mu^2</code></pre>
<pre><code>## [1] 0.0004169361</code></pre>
<div id="standard-mc-estimate-is-accurate-but-with-relatively-high-variance" class="section level3">
<h3>Standard MC estimate is accurate, but with relatively high variance</h3>
<p>Using an MC estimate,</p>
<pre class="r"><code>x &lt;- rnorm(1e+06)
y &lt;- hx(x)
var(y)</code></pre>
<pre><code>## [1] 0.0004171788</code></pre>
<p>Note also that the estimate (our target), is also accurate</p>
<pre class="r"><code>mean(y) - mu</code></pre>
<pre><code>## [1] 7.12682e-06</code></pre>
</div>
<div id="using-a-distribution-that-simply-matches-hx-is-also-not-so-great" class="section level3">
<h3>Using a distribution that simply matches h(x) is also not-so-great</h3>
<p>Now, let’s instead construct and IS estimator. One intuitive (but by no means optimal) attempt would be to use <span class="math inline">\(Y \sim N(4,1)\)</span>, a distribution that matches with <span class="math inline">\(h(x)\)</span> perfectly. Indeed, that will provide an accurate answer</p>
<pre class="r"><code>y &lt;- rnorm(1e+06, mean = 4)
h &lt;- hx(y)
IS &lt;- h * dnorm(y, 0, 1)/dnorm(y, 4, 1)
mean(IS)</code></pre>
<pre><code>## [1] 0.005151634</code></pre>
<p>But, it turns out that the variance is about the same as before.</p>
<pre class="r"><code>var(IS)</code></pre>
<pre><code>## [1] 0.0004129693</code></pre>
</div>
<div id="the-proposal-distribution-needs-to-be-tuned-to-both-p_x-and-hx" class="section level3">
<h3>The proposal distribution needs to be tuned to both p_X and h(x)</h3>
<p>This is a somewhat subtle point of the derivation provided above. We <em>don’t</em> just want a distribution that will be highest here <span class="math inline">\(h(x)\)</span> is high. Instead, what we actually need is a distribution that will be highest when <span class="math inline">\(h(x)p_X(x)\)</span> is high. That will be exactly where the two distributions intersect, at 2.</p>
<pre class="r"><code>x &lt;- seq(-5, 10, length.out = 1000)
plot(x, hx(x), type = &quot;l&quot;)
points(x, dnorm(x), col = &quot;blue&quot;, type = &quot;l&quot;)
abline(v = 2)</code></pre>
<div class="figure"><span id="fig:matching"></span>
<img src="{{< blogdown/postref >}}index_files/figure-html/matching-1.png" alt="Same as above, but with line demonstrating intersection at x=2" width="672" />
<p class="caption">
Figure 2: Same as above, but with line demonstrating intersection at x=2
</p>
</div>
<pre class="r"><code>y &lt;- rnorm(1e+06, mean = 2)
h &lt;- hx(y)
IS &lt;- h * dnorm(y, 0)/dnorm(y, 2)
mean(IS)</code></pre>
<pre><code>## [1] 0.005168203</code></pre>
<pre class="r"><code>var(IS)</code></pre>
<pre><code>## [1] 4.126951e-06</code></pre>
<p>The answer is still accurate, but the variance has been reduced by many factors. This means that a desired level of MCSE could be achieved with many fewer samples.</p>
<p>One final demonstration, remember that <span class="math inline">\(h(x)p_X(x)\)</span> describes a distribution. Hence it would be a mistake to try a <span class="math inline">\(p_Y\)</span> that placed all of the density around that point of intersection. For example, let’s try <span class="math inline">\(Y \sim N(2,0.1)\)</span>. Although that is centered on the region that contributes the largest values to the expectation, the largest values alone do not define the expectation; too much of the tails of <span class="math inline">\(h(x)\)</span> are not included. Using this results is the worst variance.</p>
<pre class="r"><code>y &lt;- rnorm(1e+06, mean = 2, 0.1)
h &lt;- hx(y)
IS &lt;- h * dnorm(y, 0)/dnorm(y, 2, 0.1)
mean(IS)</code></pre>
<pre><code>## [1] 0.002719062</code></pre>
<pre class="r"><code>var(IS)</code></pre>
<pre><code>## [1] 0.014524</code></pre>
</div>
</div>
<div id="other-references" class="section level1">
<h1>Other References</h1>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Gaussian_function#Integral_of_a_Gaussian_function">integral of Gaussian Function</a></li>
<li><a href="http://people.math.umass.edu/~lr7q/m697u-spring2018/m697uhome.html">course notes</a></li>
<li><a href="https://www.statlect.com/asymptotic-theory/importance-sampling">statlect</a></li>
</ul>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>this page is mostly a study page for upcoming comprehensive exams<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
