<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>psadil</title><link>https://psadil.github.io/psadil/</link><atom:link href="https://psadil.github.io/psadil/index.xml" rel="self" type="application/rss+xml"/><description>psadil</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© 2024 Patrick Sadil</copyright><lastBuildDate>Fri, 11 Aug 2023 14:35:30 -0500</lastBuildDate><image><url>https://psadil.github.io/psadil/media/icon_hu3896c2ce465988ba1fc8077f9a6388c6_268630_512x512_fill_lanczos_center_2.png</url><title>psadil</title><link>https://psadil.github.io/psadil/</link></image><item><title>The Push–Pull of Serial Dependence Effects: Attraction to the Prior Response and Repulsion from the Prior Stimulus</title><link>https://psadil.github.io/psadil/publication/sadil-2021-serialdependence/</link><pubDate>Fri, 11 Aug 2023 14:35:30 -0500</pubDate><guid>https://psadil.github.io/psadil/publication/sadil-2021-serialdependence/</guid><description/></item><item><title>Hinting Cache Decorator Types</title><link>https://psadil.github.io/psadil/post/typehint-callable/</link><pubDate>Sun, 23 Oct 2022 00:00:00 +0000</pubDate><guid>https://psadil.github.io/psadil/post/typehint-callable/</guid><description>&lt;p>Recently, I encountered a python function that I didn&amp;rsquo;t want to modify, but whose output I wanted cached. Providing typehints for this seemed a bit complicated, so I&amp;rsquo;m posting this gist as a reminder. The main trick involves extensions from &lt;a href="https://peps.python.org/pep-0612/" target="_blank" rel="noopener">PEP 612&lt;/a>.&lt;/p>
&lt;p>In words, the decorator will take some function &lt;code>f&lt;/code> that takes parameters &lt;code>P&lt;/code> and produces a &lt;code>str&lt;/code>. I want to be able to call &lt;code>f&lt;/code> and additionally provide a filename that will point to a file in which the string will be written. The cache will be super basic, checking just for the existence of the file and skipping &lt;code>f&lt;/code> if that file exists.&lt;/p>
&lt;p>In code, that looks like the following&lt;/p>
&lt;pre>&lt;code class="language-python">from pathlib import Path
import typing
P = typing.ParamSpec(&amp;quot;P&amp;quot;)
def cache_str(f: typing.Callable[P, str]) -&amp;gt; typing.Callable[typing.Concatenate[Path, P], str]:
def wrapper(_filename: Path, *args: P.args, **kwargs: P.kwargs) -&amp;gt; str:
if _filename.exists():
print(f&amp;quot;Found cached result {_filename}. Skipping!&amp;quot;)
i = _filename.read_text()
else:
# run!
i = f(*args, **kwargs)
_filename.touch()
_filename.write_text(i)
return i
return wrapper
&lt;/code>&lt;/pre>
&lt;p>And then the decorator could be used like&lt;/p>
&lt;pre>&lt;code class="language-python">@cache_str
def get_str(msg: str) -&amp;gt; str:
print(&amp;quot;This function takes so long to run--I hope that I don't need to run it twice&amp;quot;)
return msg
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">import tempfile
msg = &amp;quot;cached message&amp;quot;
with tempfile.TemporaryDirectory() as tmpdir:
tmp_path = Path(tmpdir) / &amp;quot;cache.txt&amp;quot;
get_str(tmp_path, msg=msg)
print(&amp;quot;trying to read message from cache...&amp;quot;)
print(f&amp;quot;{tmp_path.read_text()=}&amp;quot;)
get_str(tmp_path, msg=msg)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>This function takes so long to run--I hope that I don't need to run it twice
trying to read message from cache...
tmp_path.read_text()='cached message'
Found cached result /tmp/tmpp44byo4s/cache.txt. Skipping!
&lt;/code>&lt;/pre>
&lt;p>This is neat, but although my editor recognizes that the decorated function can accept a path as the first argument, that first argument could not be named. This appears to have been an explicit choice in PEP 612, made to avoid potential clashes with keyword arguments from the decorated function.&lt;/p></description></item><item><title>Reliability of Effect Sizes and Spatial Localization with Population-Level Sample Sizes</title><link>https://psadil.github.io/psadil/publication/sadil-2022-population/</link><pubDate>Mon, 20 Jun 2022 00:00:00 +0000</pubDate><guid>https://psadil.github.io/psadil/publication/sadil-2022-population/</guid><description/></item><item><title>How to store a NifTi as a TFRecord</title><link>https://psadil.github.io/psadil/post/tf-dataset-from-3d-nifti/</link><pubDate>Sat, 23 Apr 2022 00:00:00 +0000</pubDate><guid>https://psadil.github.io/psadil/post/tf-dataset-from-3d-nifti/</guid><description>&lt;h1 id="how-to-store-a-nifti-as-a-tfrecord">How to store a NifTi as a TFRecord&lt;/h1>
&lt;p>Patrick Sadil
2022-04-23&lt;/p>
&lt;p>A recent project required sending brain images to TensorFlow.
Unfortunately, the data exceeded memory and so during training would
need to be read from the disk. Poking around the TF documentation, it
seems like a recommended way to do this is to store the images as a
&lt;a href="https://www.tensorflow.org/tutorials/load_data/tfrecord" target="_blank" rel="noopener">TFRecord&lt;/a>. The
steps for doing that are collected in this gist&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>There are five main steps&lt;/p>
&lt;ol>
&lt;li>load the images as numpy array,&lt;/li>
&lt;li>perform an preprocessing (e.g., resizing, masking),&lt;/li>
&lt;li>serialize the preprocessed images and store with their labels (all
in memory) as a
&lt;a href="https://www.tensorflow.org/api_docs/python/tf/train/Example" target="_blank" rel="noopener">&lt;code>tf.train.Example&lt;/code>&lt;/a>,&lt;/li>
&lt;li>store (on disk) the examples as
&lt;a href="https://www.tensorflow.org/tutorials/load_data/tfrecord" target="_blank" rel="noopener">TFRecord&lt;/a>,
and finally&lt;/li>
&lt;li>create a
&lt;a href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset" target="_blank" rel="noopener">&lt;code>tf.Data.Dataset&lt;/code>&lt;/a>
pipeline that serves the examples to the model.&lt;/li>
&lt;/ol>
&lt;pre>&lt;code class="language-python">import math
from numbers import Number
import numpy as np
import tensorflow as tf
import nibabel as nb
from nilearn import plotting
from typing import List
import numpy.typing as npt
&lt;/code>&lt;/pre>
&lt;h2 id="preprocess">Preprocess&lt;/h2>
&lt;p>A full implementation might involve several preprocessing steps (e.g.,
registration, masking, cropping), but in this gist we&amp;rsquo;ll just rescale
the images to have intensity values that range from 0-1.&lt;/p>
&lt;pre>&lt;code class="language-python">def to_numpy(img: nb.Nifti1Image) -&amp;gt; np.ndarray:
return np.asanyarray(img.dataobj)
def load_and_preprocess(img: str) -&amp;gt; np.ndarray:
# convert to numpy array and preprocess
nii = to_numpy(nb.load(img))
# rescale to 0-1
preprocessed = (nii - nii.min())/(nii.max() - nii.min()).astype(np.float32)
return preprocessed
&lt;/code>&lt;/pre>
&lt;p>Since this method of storing data was new for me, I wanted to ensure
that I didn&amp;rsquo;t mess up the data. For that, I mainly relied on a basic
plot of the images.&lt;/p>
&lt;pre>&lt;code class="language-python">def plot_array(img: np.ndarray) -&amp;gt; None:
nii = nb.Nifti1Image(img, affine=np.eye(4)*2)
plotting.plot_anat(nii)
&lt;/code>&lt;/pre>
&lt;p>This first step is standard neuroimaging, but just to check that the
functions are working preprocess an example brain and see how it looks.&lt;/p>
&lt;pre>&lt;code class="language-python"># example images packaged with fsl, found at $FSLDIR/data/standard
img = ['MNI152lin_T1_2mm.nii.gz']
preprocesed = load_and_preprocess(img[0])
plot_array(preprocesed)
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="index.en_files/figure-gfm/cell-5-output-1.png" width="653"
height="269" />&lt;/p>
&lt;p>Great! That looks like a brain. I&amp;rsquo;ll use it as a reference to ensure
that the roundtrip processing, serializing and unserializing, returns
the arrays we need.&lt;/p>
&lt;h2 id="serialize">Serialize&lt;/h2>
&lt;p>Here&amp;rsquo;s where TensorFlow starts. As those that came before have always
done, &lt;a href="https://www.tensorflow.org/tutorials/load_data/tfrecord" target="_blank" rel="noopener">we&amp;rsquo;ll rely on these
incantations&lt;/a>.&lt;/p>
&lt;pre>&lt;code class="language-python">def _bytes_feature(value: bytes):
&amp;quot;&amp;quot;&amp;quot;
Returns a bytes_list from a string / byte.
Example:
_bytes_feature(b'\x00')
_bytes_feature(b'a')
&amp;quot;&amp;quot;&amp;quot;
return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))
def _float_feature(value: Number):
&amp;quot;&amp;quot;&amp;quot;
Returns a float_list from a float / double.
Example:
_float_feature(2)
_float_feature(2.)
&amp;quot;&amp;quot;&amp;quot;
return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))
&lt;/code>&lt;/pre>
&lt;p>Those functions seem to help ensure that the serialized data ends up
with appropriate types. But the full steps involve serializing both the
image and the label together.&lt;/p>
&lt;pre>&lt;code class="language-python">def serialize_example(img: str, label: Number):
# convert to numpy array and preprocess
preprocessed = load_and_preprocess(img)
# Notice how the numpy array is converted into a byte_string. That's serialization in memory!
feature = {
'label': _float_feature(label),
'image_raw': _bytes_feature(tf.io.serialize_tensor(preprocessed).numpy())
}
return tf.train.Example(features=tf.train.Features(feature=feature))
&lt;/code>&lt;/pre>
&lt;p>To get the image back out, we need two more functions: one to
unpack/parse the example (&lt;code>decode_example&lt;/code>) and another to
unserialize/parse (&lt;code>parse_1_example&lt;/code>) the unpacked example.&lt;/p>
&lt;pre>&lt;code class="language-python">def decode_example(record_bytes) -&amp;gt; dict:
example = tf.io.parse_example(
record_bytes,
features = {
&amp;quot;label&amp;quot;: tf.io.FixedLenFeature([], dtype=tf.float32),
'image_raw': tf.io.FixedLenFeature([], dtype=tf.string)
}
)
return example
def parse_1_example(example):
&amp;quot;&amp;quot;&amp;quot;
Note that the network I was using worked with 3D images, and so the batches of data were of shape `(batch_size, x_dim, y_dim, z_dim, 1)`, rather than what is typical for 2d images: `(batch_size, x_dim, y_dim, n_channels)`.
&amp;quot;&amp;quot;&amp;quot;
X = tf.io.parse_tensor(example['image_raw'], out_type=tf.float32)
# the images output by tf.io.parse_tensor will have shape (x_dim, y_dim, z_dim), which is to say that they're missing the channels dimension. expand_dims is used to indicate channel (i.e., be explicit about grayscale)
return tf.expand_dims(X, 3), example['label']
&lt;/code>&lt;/pre>
&lt;p>At this point, we have functions for preprocessing the images,
serializing them&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>, and packing each of them along with their labels
into a
&lt;a href="https://www.tensorflow.org/api_docs/python/tf/train/Example" target="_blank" rel="noopener">&lt;code>tf.train.Example&lt;/code>&lt;/a>.
This is all for converting the data into a format that can then be more
easily written to and read from the disk.&lt;/p>
&lt;h2 id="write-serialized-examples-as-tfrecords">Write serialized examples as TFRecords&lt;/h2>
&lt;p>So far, everything has been in memory. Next comes a function that
performs the above steps sequentially on several examples, and along the
way writes the examples as a
&lt;a href="https://www.tensorflow.org/tutorials/load_data/tfrecord" target="_blank" rel="noopener">&lt;code>TFRecord&lt;/code>&lt;/a>.&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>&lt;/p>
&lt;pre>&lt;code class="language-python">def write_records(niis: List[str], labels: npt.ArrayLike, n_per_record: int, outfile: str) -&amp;gt; None:
&amp;quot;&amp;quot;&amp;quot;
store list of niftis (and associated label) into tfrecords for use as dataset
Args:
niis: files that will be preprocessed and stored in record
labels: true label associated with each element in niis. these are the &amp;quot;y&amp;quot;
n_per_record: number of examples to store in each record. TF documentation advises that the files are +100Mb. Around 400 images cropped images at 2mm resolution seems to work.
outfile: base prefix to use when writing the records
Returns:
Nothing, but the records will be written to disk.
&amp;quot;&amp;quot;&amp;quot;
n_niis = len(niis)
n_records = math.ceil(len(niis) / n_per_record)
for i, shard in enumerate(range(0, n_niis, n_per_record)):
print(f&amp;quot;writing record {i} of {n_records-1}&amp;quot;)
with tf.io.TFRecordWriter(
f&amp;quot;{outfile}_{i:0&amp;gt;3}-of-{n_records-1:0&amp;gt;3}.tfrecords&amp;quot;,
options= tf.io.TFRecordOptions(compression_type=&amp;quot;GZIP&amp;quot;)
) as writer:
for nii, label in zip(niis[shard:shard+n_per_record], labels[shard:shard+n_per_record]):
example = serialize_example(img=nii, label=label)
writer.write(example.SerializeToString())
&lt;/code>&lt;/pre>
&lt;h2 id="create-dataset">Create Dataset&lt;/h2>
&lt;p>For this gist, let&amp;rsquo;s store several copies of the MNI 2mm brain.&lt;/p>
&lt;pre>&lt;code class="language-python"># (e.g., put nifti of label MNI152_T1_1mm_brain.nii.gz in the working directory)
n_imgs = 3
mni_nii = ['MNI152lin_T1_2mm.nii.gz'] * n_imgs
# store examples in each tfrecord. number of examples per record is configurable.
# aim for as many examples as produces files of size &amp;gt; 100M
prefix = &amp;quot;tmp&amp;quot;
write_records(mni_nii, np.arange(n_imgs), n_imgs, prefix)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>writing record 0 of 0
2022-05-13 15:00:56.962382: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
&lt;/code>&lt;/pre>
&lt;p>Calling the above will write a TFRecord file to disk. To read that
record, define a pipeline that will create a &lt;code>tf.Data.dataset&lt;/code>.&lt;/p>
&lt;pre>&lt;code class="language-python">def get_batched_dataset(files, batch_size: int = 32) -&amp;gt; tf.data.Dataset:
# Note: an actual pipeline would probably include at least the shuffle and prefetch methods
dataset = (
tf.data.Dataset.list_files(files) # note shuffling is on by default, changes order when there are multiple records
.flat_map(lambda x: tf.data.TFRecordDataset(x, compression_type=&amp;quot;GZIP&amp;quot;))
.map(decode_example)
.map(parse_1_example)
.batch(batch_size)
)
return dataset
&lt;/code>&lt;/pre>
&lt;p>Now, use that function to read the records back.&lt;/p>
&lt;pre>&lt;code class="language-python"># a full dataset will have a list with many records
list_of_records=[f'{prefix}*.tfrecords']
ds = get_batched_dataset(list_of_records, batch_size=2)
&lt;/code>&lt;/pre>
&lt;p>That dataset, &lt;code>ds&lt;/code>, was our goal in the gist. It can be passed to
methods of the &lt;code>tf.keras.Model&lt;/code>, including
[&lt;code>tf.keras.Model.fit()&lt;/code>(&lt;a href="https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit">https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit&lt;/a>)],
serving up more data than can fit in memory.&lt;/p>
&lt;p>But first, the serialization is a lot, so it is a good idea to verify
that the images look okay when loaded. To do so, pluck a single example
from the dataset.&lt;/p>
&lt;pre>&lt;code class="language-python">(Xs, Ys) = next(ds.as_numpy_iterator())
&lt;/code>&lt;/pre>
&lt;p>The dimensions of the labels are relatively easy. It&amp;rsquo;s a 1d array with
as many elements as are in the batch.&lt;/p>
&lt;pre>&lt;code class="language-python"># (batch_size, )
Ys.shape
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>(2,)
&lt;/code>&lt;/pre>
&lt;p>The order will depend on what shuffling is embedded in the dataset
pipeline. In this case, there was no shuffling and so we should expect
that the order is preserved.&lt;/p>
&lt;pre>&lt;code class="language-python">Ys
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>array([0., 1.], dtype=float32)
&lt;/code>&lt;/pre>
&lt;p>The data, &lt;code>Xs&lt;/code>, also has a predictable shape.&lt;/p>
&lt;pre>&lt;code class="language-python"># (batch_size, x_dim, y_dim, z_dim, 1)
Xs.shape
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>(2, 91, 109, 91, 1)
&lt;/code>&lt;/pre>
&lt;p>Take that first element in the batch and plot.&lt;/p>
&lt;pre>&lt;code class="language-python">parsed_img = np.squeeze(Xs[0,])
plot_array(parsed_img)
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="index.en_files/figure-gfm/cell-17-output-1.png" width="653"
height="269" />&lt;/p>
&lt;p>That looks great! Just in case, let&amp;rsquo;s check more explicitly&lt;/p>
&lt;pre>&lt;code class="language-python">np.array_equal(preprocesed, parsed_img)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>True
&lt;/code>&lt;/pre>
&lt;p>Yay! done&lt;/p>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Inspiration for writing came from responding to &lt;a href="https://neurostars.org/t/tensorflow-issue-when-trying-to-use-nibabel-in-dataset/22410/2" target="_blank" rel="noopener">this question on
NeuroStars&lt;/a>,
and also from an urge to try a python-based post.&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>This serializing stuff is spooky black magic. I&amp;rsquo;m going to skip
over those details and instead leave this reference, a journey
through serializing in &lt;code>R&lt;/code>
&lt;a href="https://blog.djnavarro.net/posts/2021-11-15_serialisation-with-rds/">https://blog.djnavarro.net/posts/2021-11-15_serialisation-with-rds/&lt;/a>.&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>It seems to me that this storage on disk involves a &lt;em>second&lt;/em>
serialization step (where the first was done by
&lt;code>tf.io.serialize_tensor&lt;/code> in &lt;code>serialize_example&lt;/code>). Even so, I assume
the roundtrip isn&amp;rsquo;t so much of a big deal, considering that the data
can be served to the model in parallel.&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Spurious Serial Dependencies</title><link>https://psadil.github.io/psadil/post/2021-05-07-spurious-serial-dependencies/</link><pubDate>Fri, 07 May 2021 00:00:00 +0000</pubDate><guid>https://psadil.github.io/psadil/post/2021-05-07-spurious-serial-dependencies/</guid><description>
&lt;script src="https://psadil.github.io/psadil/post/2021-05-07-spurious-serial-dependencies/index.en_files/header-attrs/header-attrs.js">&lt;/script>
&lt;pre class="r">&lt;code>knitr::opts_chunk$set(echo = TRUE)
set.seed(1234)
library(dplyr)
library(ggplot2)
library(tidyr)&lt;/code>&lt;/pre>
&lt;p>I’m working on a project involving &lt;a href="https://psadil.github.io/psadil/post/serial-dependence/">serial dependence&lt;/a>. The project involves disentangling a dependence on the previous orientation from a dependence on the previous response. Unfortunately, there is a common way for a dependence on the previous response to be spurious, due to the oblique effect&lt;a href="#fn1" class="footnote-ref" id="fnref1">&lt;sup>1&lt;/sup>&lt;/a>. The first reference I’ve seen for this is a master’s thesis by &lt;span class="citation">&lt;a href="#ref-fritsche2016smooth" role="doc-biblioref">Fritsche&lt;/a> (&lt;a href="#ref-fritsche2016smooth" role="doc-biblioref">2016&lt;/a>)&lt;/span>. I didn’t follow that explanation, and so I’m using this post to explain how the oblique effect causes a spurious dependence on the previous response.&lt;/p>
&lt;p>First, let’s show that the confound is real. Data will be generated with an oblique effect, and there will be no dependence between trials – neither on the previous orientation nor the previous response. There will be no response variability, meaning that errors will only be caused by the oblique effect. Since the data are simulated without dependencies, any dependence that emerges will necessarily be spurious.&lt;/p>
&lt;pre class="r">&lt;code># helper functions for converting between angles and degrees
rad &amp;lt;- function(degree) degree * pi / 180
deg &amp;lt;- function(radian) radian * 180 / pi
# magnitude of oblique effect
oblique &amp;lt;- rad(-22.5)
d0 &amp;lt;- tibble(
orientation = runif(5000, 0, pi)) %&amp;gt;%
mutate(
trial = 1:n(),
oblique = oblique*sin(orientation*4),
response = rnorm(n(), orientation, 0) + oblique) &lt;/code>&lt;/pre>
&lt;p>To help generate the data, define a helper functions that calculates the signed, shortest angle between two angles (measured in radians).&lt;/p>
&lt;pre class="r">&lt;code>#&amp;#39; @param deg1 numeric degree
#&amp;#39; @param deg2 numeric degree
#&amp;#39;
#&amp;#39; @return
#&amp;#39; signed difference between inputs, wrapped to +-pi
#&amp;#39; output is shortest distance to the first input
#&amp;#39;
#&amp;#39; @examples
#&amp;#39; # pi/2 is 45 degrees clockwise from pi, so the output is pi/2
#&amp;#39; ang_diff(pi/2, pi)
#&amp;#39; # pi is 45 degrees counterclockwise from pi/2, so the output is -pi/2
#&amp;#39; ang_diff(pi, pi/2)
#&amp;#39; # notice the discontinuity when the shortest angle switches direction
#&amp;#39; ang_diff(pi/2 - .01, 0)
#&amp;#39; ang_diff(pi/2 + .01, 0)
ang_diff &amp;lt;- function(deg1, deg2){
stopifnot(length(deg1) == length(deg2))
diff &amp;lt;- ( deg1 - deg2 + pi/2 ) %% pi - pi/2
out &amp;lt;- dplyr::if_else(diff &amp;lt; -pi/2, diff + pi, diff)
return(out)
}&lt;/code>&lt;/pre>
&lt;p>Then use the function &lt;code>ang_diff&lt;/code> to calculate errors, and to calculate the relative orientation difference between the current trial and either the previous orientation or the previous response.&lt;/p>
&lt;pre class="r">&lt;code>d &amp;lt;- d0 %&amp;gt;%
mutate(
prev_response = lag(response),
prev_orientation = lag(orientation),
error = ang_diff(orientation, response),
orientation_diff = ang_diff(orientation, prev_orientation),
response_diff = ang_diff(orientation, prev_response)) %&amp;gt;%
filter(trial &amp;gt; 1) %&amp;gt;%
mutate(across(where(is.double), deg))&lt;/code>&lt;/pre>
&lt;p>Plot errors as a function of the current orientation to confirm that there is an oblique effect.&lt;/p>
&lt;pre class="r">&lt;code>d %&amp;gt;%
ggplot(aes(x=orientation, y=error)) +
geom_point() +
geom_smooth(
formula = y ~ sin(rad(x)*4),
method = &amp;quot;lm&amp;quot;,
se = FALSE) +
scale_y_continuous(
breaks = c(-20, -10, 0, 10, 20),
labels = c(&amp;quot;CCW&amp;quot;, -10, 0, 10, &amp;quot;CW&amp;quot;)) +
scale_x_continuous(
name = &amp;quot;orientation on current trial&amp;quot;,
labels = c(0, 90, 180),
breaks = c(0, 90, 180))&lt;/code>&lt;/pre>
&lt;div class="figure">&lt;span style="display:block;" id="fig:oblique">&lt;/span>
&lt;img src="https://psadil.github.io/psadil/post/2021-05-07-spurious-serial-dependencies/index.en_files/figure-html/oblique-1.png" alt="The simulated data exhibit a clear oblique effect." width="672" />
&lt;p class="caption">
Figure 1: The simulated data exhibit a clear oblique effect.
&lt;/p>
&lt;/div>
&lt;p>Is there a dependence on either the previous response or previous orientation?&lt;/p>
&lt;pre class="r">&lt;code>d %&amp;gt;%
pivot_longer(
cols=c(orientation_diff, response_diff),
names_to = &amp;quot;covariate&amp;quot;,
names_pattern = &amp;quot;(orientation|response)&amp;quot;,
values_to = &amp;quot;x&amp;quot;) %&amp;gt;%
ggplot(aes(x=x, y=error)) +
geom_point() +
facet_wrap(~covariate) +
geom_smooth(
method = &amp;quot;gam&amp;quot;,
formula = y ~ s(x, bs = &amp;quot;cc&amp;quot;, k=9)) +
scale_x_continuous(
name = &amp;quot;relative orientation/response on previous trial&amp;quot;,
labels = c(-90, 0, 90),
breaks = c(-90, 0, 90))&lt;/code>&lt;/pre>
&lt;div class="figure">&lt;span style="display:block;" id="fig:spurious">&lt;/span>
&lt;img src="https://psadil.github.io/psadil/post/2021-05-07-spurious-serial-dependencies/index.en_files/figure-html/spurious-1.png" alt="Errors do not depend on the previous orientation, but they do depend on the previous response. Since the data were generated without that dependence, it must be spurious." width="672" />
&lt;p class="caption">
Figure 2: Errors do not depend on the previous orientation, but they do depend on the previous response. Since the data were generated without that dependence, it must be spurious.
&lt;/p>
&lt;/div>
&lt;p>What’s going on? There are two key factors: first, the oblique effect operates on the previous trial to make some previous responses more likely than others, and second the oblique effect operates on the current trial to make certain previous responses more likely to have errors in a consistent direction. To be precise, I’ll use the following terminology.&lt;/p>
&lt;div id="terminology" class="section level2">
&lt;h2>Terminology&lt;/h2>
&lt;p>Trials will be indexed by natural numbers. The “current trial” will be referred to as trial &lt;span class="math inline">\(n\)&lt;/span>, and the “previous trial” as trial &lt;span class="math inline">\(n-1\)&lt;/span>. The orientation and responses on each trial will be thought of as sequences&lt;a href="#fn2" class="footnote-ref" id="fnref2">&lt;sup>2&lt;/sup>&lt;/a>. The variable &lt;span class="math inline">\(O_n\)&lt;/span> means the orientation on trial &lt;span class="math inline">\(n\)&lt;/span> (i.e., the current trial), whereas the variable &lt;span class="math inline">\(O_{n-1}\)&lt;/span> means the orientation on trial &lt;span class="math inline">\(n-1\)&lt;/span> (i.e., the previous trial). Similarly, the variable &lt;span class="math inline">\(R_n\)&lt;/span> means the response on trial &lt;span class="math inline">\(n\)&lt;/span>, whereas the variable &lt;span class="math inline">\(R_{n-1}\)&lt;/span> means the response on trial &lt;span class="math inline">\(n-1\)&lt;/span>.&lt;/p>
&lt;p>All angles (e.g., &lt;span class="math inline">\(O_n\)&lt;/span> and &lt;span class="math inline">\(R_n\)&lt;/span>) use the convention that &lt;span class="math inline">\(0^\circ\)&lt;/span> is horizontal, &lt;span class="math inline">\(45^\circ\)&lt;/span> is one quarter rotation counterclockwise from horizontal (e.g., at 1:30 on a clock), &lt;span class="math inline">\(90^\circ\)&lt;/span> is vertical, etc. However, differences between angles are reported such that a positive value implies a clockwise shift (i.e., moving forward on the clock) and a negative value implies a counterclockwise shift. For example, an error of &lt;span class="math inline">\(10^\circ\)&lt;/span> means that &lt;span class="math inline">\(R_n\)&lt;/span> is &lt;span class="math inline">\(10^\circ\)&lt;/span> clockwise from &lt;span class="math inline">\(O_n\)&lt;/span>. This means that we can determine an “attraction” effect based on whether the sign of the error on trial &lt;span class="math inline">\(n\)&lt;/span> matches the sign of the difference between &lt;span class="math inline">\(O_n\)&lt;/span> and either &lt;span class="math inline">\(O_{n-1}\)&lt;/span> or &lt;span class="math inline">\(R_{n-1}\)&lt;/span>. Conversely, a “repulsive” effect is when the error and differences have mismatched signs.&lt;/p>
&lt;/div>
&lt;div id="explanation" class="section level2">
&lt;h2>Explanation&lt;/h2>
&lt;p>First, consider a specific sequence of trials that could produce a spurious effect. To help with the explanation, the trials are colored based on the current trial.&lt;/p>
&lt;pre class="r">&lt;code>d %&amp;gt;%
pivot_longer(
cols=c(orientation_diff, response_diff),
names_to = &amp;quot;covariate&amp;quot;,
names_pattern = &amp;quot;(orientation|response)&amp;quot;,
values_to = &amp;quot;x&amp;quot;) %&amp;gt;%
ggplot(aes(x=x, y=error)) +
geom_point(aes(color=oblique)) +
scale_color_gradient2(low = scales::muted(&amp;quot;blue&amp;quot;), high = scales::muted(&amp;quot;red&amp;quot;)) +
facet_wrap(~covariate) +
geom_smooth(
method = &amp;quot;gam&amp;quot;,
formula = y ~ s(x, bs = &amp;quot;cc&amp;quot;, k=9)) +
scale_y_continuous(
breaks = c(-20, -10, 0, 10, 20),
labels = c(&amp;quot;CCW&amp;quot;, -10, 0, 10, &amp;quot;CW&amp;quot;)) +
scale_x_continuous(
name = &amp;quot;relative orientation/response on previous trial&amp;quot;,
labels = c(-90, 0, 90),
breaks = c(-90, 0, 90))&lt;/code>&lt;/pre>
&lt;div class="figure">&lt;span style="display:block;" id="fig:spuriouscol">&lt;/span>
&lt;img src="https://psadil.github.io/psadil/post/2021-05-07-spurious-serial-dependencies/index.en_files/figure-html/spuriouscol-1.png" alt="Same figure as above, but colored based on the magnitude and direction of the oblique effect." width="672" />
&lt;p class="caption">
Figure 3: Same figure as above, but colored based on the magnitude and direction of the oblique effect.
&lt;/p>
&lt;/div>
&lt;p>When &lt;span class="math inline">\(O_{n-1}\)&lt;/span> is &lt;span class="math inline">\(0^\circ\)&lt;/span>, the oblique effect will have not caused an error. So, for &lt;span class="math inline">\(R_{n-1}\)&lt;/span> to be &lt;span class="math inline">\(22.5^\circ\)&lt;/span> clockwise to &lt;span class="math inline">\(O_n\)&lt;/span>, then &lt;span class="math inline">\(O_n\)&lt;/span> could be, itself &lt;span class="math inline">\(22.5^\circ\)&lt;/span>. But when &lt;span class="math inline">\(O_n\)&lt;/span> is &lt;span class="math inline">\(22.5^\circ\)&lt;/span>, the oblique effect will cause an error; &lt;span class="math inline">\(R_n\)&lt;/span> will be a clockwise error, in the same direction as &lt;span class="math inline">\(R_{n-1}\)&lt;/span>. Since &lt;span class="math inline">\(R_n\)&lt;/span> exhibits an error in the direction of &lt;span class="math inline">\(R_{n-1}\)&lt;/span>, it will look like &lt;span class="math inline">\(R_{n-1}\)&lt;/span> caused an attraction.&lt;/p>
&lt;p>More importantly, when the oblique effect acts on trial &lt;span class="math inline">\(n-1\)&lt;/span>, it will cause responses to collect along the cardinal axes. That is, regardless of the orientation on trial &lt;span class="math inline">\(n-1\)&lt;/span>, &lt;span class="math inline">\(R_{n-1}\)&lt;/span> will be close to either &lt;span class="math inline">\(0^\circ\)&lt;/span> or &lt;span class="math inline">\(90^\circ\)&lt;/span>. This means that, whenever &lt;span class="math inline">\(O_n\)&lt;/span> is close to &lt;span class="math inline">\(22.5^\circ\)&lt;/span>, the oblique effect’s influence on the previous response, &lt;span class="math inline">\(R_{n-1}\)&lt;/span>, makes it more likely that &lt;span class="math inline">\(R_{n-1}\)&lt;/span> will be approximately &lt;span class="math inline">\(22.5^\circ\)&lt;/span> clockwise from &lt;span class="math inline">\(O_n\)&lt;/span>, and then the oblique effect on trial &lt;span class="math inline">\(n\)&lt;/span> will further push the response toward &lt;span class="math inline">\(R_{n-1}\)&lt;/span>.&lt;/p>
&lt;p>We can see this play out empirically by looking at &lt;span class="math inline">\(O_n\)&lt;/span> as a function of &lt;span class="math inline">\(R_{n-1}\)&lt;/span>; when &lt;span class="math inline">\(R_{n-1}\)&lt;/span> is close to &lt;span class="math inline">\(22.5^\circ\)&lt;/span>, there is an over-representation of orientations for which the oblique effect will bias responses toward the previous response.&lt;/p>
&lt;pre class="r">&lt;code>d %&amp;gt;%
filter(between(response_diff, 21, 24)) %&amp;gt;%
ggplot(aes(x=orientation)) +
geom_histogram(bins=30) +
scale_x_continuous(
name = &amp;quot;orientation on current trial&amp;quot;,
labels = c(0, 90, 180),
breaks = c(0, 90, 180)) +
geom_vline(xintercept = c(22.5, 112.5), color=&amp;quot;blue&amp;quot;) &lt;/code>&lt;/pre>
&lt;div class="figure">&lt;span style="display:block;" id="fig:dep">&lt;/span>
&lt;img src="https://psadil.github.io/psadil/post/2021-05-07-spurious-serial-dependencies/index.en_files/figure-html/dep-1.png" alt="When the previous orientation is 22.5 clockwise, the current orientation tends to be either 22.5 or 112.5, which is when the oblique effect causes maximal error." width="672" />
&lt;p class="caption">
Figure 4: When the previous orientation is 22.5 clockwise, the current orientation tends to be either 22.5 or 112.5, which is when the oblique effect causes maximal error.
&lt;/p>
&lt;/div>
&lt;p>We can think about this from the other direction, too; when &lt;span class="math inline">\(R_{n-1}\)&lt;/span> is &lt;span class="math inline">\(22.5^\circ\)&lt;/span> clockwise from &lt;span class="math inline">\(O_n\)&lt;/span>, it’s relatively difficult for &lt;span class="math inline">\(O_n\)&lt;/span> to be around &lt;span class="math inline">\(67.5^\circ\)&lt;/span>. For example, when &lt;span class="math inline">\(O_n=67.5\)&lt;/span>, the previous response could be &lt;span class="math inline">\(22.5^\circ\)&lt;/span> if &lt;span class="math inline">\(O_n=45^\circ\)&lt;/span>, but nearly no other orientation would work; when &lt;span class="math inline">\(O_n\)&lt;/span> is near but not exactly &lt;span class="math inline">\(45^\circ\)&lt;/span>, the oblique effect on trial &lt;span class="math inline">\(n-1\)&lt;/span> will push &lt;span class="math inline">\(R_{n-1}\)&lt;/span> away from &lt;span class="math inline">\(45^\circ\)&lt;/span>, away from a response that could be &lt;span class="math inline">\(22.5^\circ\)&lt;/span> clockwise to &lt;span class="math inline">\(O_n\)&lt;/span>. This is important because, if it is rare for trial &lt;span class="math inline">\(n\)&lt;/span> to have both &lt;span class="math inline">\(O_n=67.5\)&lt;/span> and &lt;span class="math inline">\(R_{n-1}\)&lt;/span> be &lt;span class="math inline">\(22.5^\circ\)&lt;/span> clockwise from &lt;span class="math inline">\(O_n\)&lt;/span>, then the oblique effect will be imbalanced.&lt;/p>
&lt;p>Together, this means that when the oblique effect on trial &lt;span class="math inline">\(n\)&lt;/span> causes a maximal clockwise error, the oblique effect on trial &lt;span class="math inline">\(n-1\)&lt;/span> makes it more likely that the previous response is also clockwise and less likely that it’s counterclockwise. The result is a spurious dependence on the previous response.&lt;/p>
&lt;p>We can see this play out more generally by looking at the current orientation as a function of the previous orientations and responses.&lt;/p>
&lt;pre class="r">&lt;code>d %&amp;gt;%
pivot_longer(
cols = c(orientation_diff, response_diff),
names_to = &amp;quot;covariate&amp;quot;,
names_pattern = &amp;quot;(response|orientation)&amp;quot;) %&amp;gt;%
ggplot(aes(x=orientation, y=value)) +
facet_wrap(~covariate) +
geom_point() +
coord_fixed() +
scale_y_continuous(
name = &amp;quot;relative orientation/response on previous trial&amp;quot;,
labels = c(-90, 0, 90),
breaks = c(-90, 0, 90)) +
scale_x_continuous(
name = &amp;quot;orientation on current trial&amp;quot;,
labels = c(0, 90, 180),
breaks = c(0, 90, 180))&lt;/code>&lt;/pre>
&lt;div class="figure">&lt;span style="display:block;" id="fig:currbyprev">&lt;/span>
&lt;img src="https://psadil.github.io/psadil/post/2021-05-07-spurious-serial-dependencies/index.en_files/figure-html/currbyprev-1.png" alt="The current orientation is unrelated to the previous orientation, but there is a dependency on the previous response. " width="672" />
&lt;p class="caption">
Figure 5: The current orientation is unrelated to the previous orientation, but there is a dependency on the previous response.
&lt;/p>
&lt;/div>
&lt;p>As expected, there is no relationship between &lt;span class="math inline">\(O_n\)&lt;/span> and &lt;span class="math inline">\(O_{n-1}\)&lt;/span>, but there is a strong relationship between &lt;span class="math inline">\(O_n\)&lt;/span> and &lt;span class="math inline">\(R_{n-1}\)&lt;/span>. When &lt;span class="math inline">\(O_n \in (0,45)\)&lt;/span>, then it’s likely that &lt;span class="math inline">\(R_{n-1} \in (0,22.5)\)&lt;/span> (clockwise), or &lt;span class="math inline">\(R_{n-1} \in (-90, -67.5)\)&lt;/span> (counterclockwise). The figure below shows the same data, but now the data are colored according to how the oblique effect will cause errors on trial &lt;span class="math inline">\(n\)&lt;/span>.&lt;/p>
&lt;pre class="r">&lt;code>d %&amp;gt;%
na.omit() %&amp;gt;%
select(-response, -error) %&amp;gt;%
pivot_longer(
cols = c(orientation_diff, response_diff),
names_to = &amp;quot;covariate&amp;quot;,
names_pattern = &amp;quot;(response|orientation)&amp;quot;) %&amp;gt;%
ggplot(aes(x=orientation, y=value)) +
facet_wrap(~covariate) +
geom_point(aes(color=oblique)) +
scale_color_gradient2(low = scales::muted(&amp;quot;blue&amp;quot;), high = scales::muted(&amp;quot;red&amp;quot;)) +
coord_fixed() +
scale_y_continuous(
name = &amp;quot;relative orientation/response on previous trial&amp;quot;,
labels = c(-90, 0, 90),
breaks = c(-90, 0, 90)) +
scale_x_continuous(
name = &amp;quot;orientation on current trial&amp;quot;,
labels = c(0, 90, 180),
breaks = c(0, 90, 180))&lt;/code>&lt;/pre>
&lt;div class="figure">&lt;span style="display:block;" id="fig:currbyprevcolor">&lt;/span>
&lt;img src="https://psadil.github.io/psadil/post/2021-05-07-spurious-serial-dependencies/index.en_files/figure-html/currbyprevcolor-1.png" alt="This is the same plot as above, but now the data have been colored according to how the oblique effect will cause errors. The point is that the errors caused by the oblique effect on trial $n$ are balanced when looking at the orientation on the previous trial (consider a horizontal slice), but no such balancing happens when looking at the response on the previous trial." width="672" />
&lt;p class="caption">
Figure 6: This is the same plot as above, but now the data have been colored according to how the oblique effect will cause errors. The point is that the errors caused by the oblique effect on trial &lt;span class="math inline">\(n\)&lt;/span> are balanced when looking at the orientation on the previous trial (consider a horizontal slice), but no such balancing happens when looking at the response on the previous trial.
&lt;/p>
&lt;/div>
&lt;p>Fortunately, this spurious bias isn’t too hard to adjust for&lt;a href="#fn3" class="footnote-ref" id="fnref3">&lt;sup>3&lt;/sup>&lt;/a>. But the point is that it would be a mistake to look at just a dependence on the previous orientation if there is an oblique effect; analyses must adjust for the oblique effect.&lt;/p>
&lt;p>I’m not sure if there is a similar issue with other domains (e.g., when participants discriminate tones, pain, faces, etc). Perhaps edge effects could cause a similar issue (e.g., if people are more likely to respond to the ends or middle of the scale)?&lt;/p>
&lt;div id="refs" class="references csl-bib-body hanging-indent">
&lt;div id="ref-fritsche2016smooth" class="csl-entry">
Fritsche, Matthias. 2016. &lt;span>“To Smooth or Not to Smooth: Investigating the Role of Serial Dependence in Stabilising Visual Perception.”&lt;/span> Master’s thesis, Radboud University.
&lt;/div>
&lt;div id="ref-wei2015bayesian" class="csl-entry">
Wei, Xue-Xin, and Alan A Stocker. 2015. &lt;span>“A Bayesian Observer Model Constrained by Efficient Coding Can Explain’anti-Bayesian’percepts.”&lt;/span> &lt;em>Nature Neuroscience&lt;/em> 18 (10): 1509.
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="footnotes">
&lt;hr />
&lt;ol>
&lt;li id="fn1">&lt;p>This effect occurs when participants are asked to report orientations. Participants are differently accurate across the range of orientations; they are maximally accurate when reporting &lt;span class="math inline">\(0^\circ\)&lt;/span>, &lt;span class="math inline">\(45^\circ\)&lt;/span>, &lt;span class="math inline">\(90^\circ\)&lt;/span>, and &lt;span class="math inline">\(135^\circ\)&lt;/span>, but minimally accurate at intermediate orientations (&lt;span class="math inline">\(22.5^\circ\)&lt;/span>, &lt;span class="math inline">\(67.5^\circ\)&lt;/span>, etc). The errors can either be clockwise or counterclockwise, depending on the experiment. For an overview, see &lt;span class="citation">&lt;a href="#ref-wei2015bayesian" role="doc-biblioref">Wei and Stocker&lt;/a> (&lt;a href="#ref-wei2015bayesian" role="doc-biblioref">2015&lt;/a>)&lt;/span>.&lt;a href="#fnref1" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn2">&lt;p>This won’t be used, but sequence of orientations could be written &lt;span class="math inline">\((O_n)_{n\in\mathbb{N}}\)&lt;/span>, and the sequence of responses &lt;span class="math inline">\((R_n)_{n\in\mathbb{N}}\)&lt;/span>. Selecting a particular trial involves dropping the parentheses; &lt;span class="math inline">\((O_n)_{n\in\mathbb{N}}\)&lt;/span> emphasizes the whole sequence, whereas &lt;span class="math inline">\(O_n\)&lt;/span> means take a particular (but arbitrary) element of the sequence. I am not a mathematician, and this post is a quick and dirty explanation mostly meant for later me, so don’t expect formality.&lt;a href="#fnref2" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn3">&lt;p>In a regression model of the errors, it would suffice to include a sinusoidal term.&lt;a href="#fnref3" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>NeuroModulation Modeling (NMM): Inferring the form of neuromodulation from fMRI tuning functions</title><link>https://psadil.github.io/psadil/publication/sadil-2021-nmm/</link><pubDate>Fri, 05 Mar 2021 14:14:56 -0500</pubDate><guid>https://psadil.github.io/psadil/publication/sadil-2021-nmm/</guid><description/></item><item><title>New England GAN</title><link>https://psadil.github.io/psadil/post/2021-01-02-gan-mass/</link><pubDate>Sun, 17 Jan 2021 00:00:00 +0000</pubDate><guid>https://psadil.github.io/psadil/post/2021-01-02-gan-mass/</guid><description>
&lt;script src="https://psadil.github.io/psadil/post/2021-01-02-gan-mass/index.en_files/header-attrs/header-attrs.js">&lt;/script>
&lt;p>A friend recently moved away from Western Mass, so I wanted to send them a gift to help remind them of the area. I also happened to want to learn a bit more about Generative Adversarial Networks (GANs).&lt;/p>
&lt;div id="goals" class="section level1">
&lt;h1>Goals&lt;/h1>
&lt;ul>
&lt;li>Briefly overview GANs&lt;/li>
&lt;li>Link to fun code for acquiring data from Google Street View&lt;/li>
&lt;li>Share a neat set of photographs&lt;/li>
&lt;/ul>
&lt;p>Note that this isn’t an ‘intro to GANs’. If that’s what you’re after, keep browsing.&lt;/p>
&lt;/div>
&lt;div id="overview" class="section level1">
&lt;h1>Overview&lt;/h1>
&lt;p>Computers have gotten very good at extracting information from images, particularly at identifying images’ contents. Such categorization is powerful, but it often requires access to labeled data – hundreds of thousands of pictures for which we can tell the computer: this is a cat, that is a dog, no that’s also a dog, yes that’s a dog but it’s also a husky. However, many applications remain where computer-aided categorization would be invaluable, but for which there isn’t sufficient labeled data. If an algorithm can learn to recognize the subtle features distinguishing &lt;a href="https://en.wikipedia.org/wiki/ImageNet">120 dog breeds&lt;/a>, it could probably learn visual features that help radiologists locate potential anomalies. But the guess-and-check strategy, despite being sufficient for many advanced computer vision algorithms, flounders when it has access to only a few hundred training examples. Computers have the potential to do some very clever things, but there is not always enough data to supervise their training.&lt;/p>
&lt;p>To mitigate a lack of data, one developing solution is a GAN. A common analogy for these networks envisions art forgery (&lt;a href="https://www.tensorflow.org/tutorials/generative/dcgan">e.g.&lt;/a>), a forger and a critic collaborating to learn about an artist. The forger paints fake works in the style of van Gough, while the critic distinguishes the fake from the real van Goughs. For the forger to succeed, it must paint the essences of van Gough: the reductionist features like the strokes and the yellows, and the holistic feelings of urgency and presence. For the critic to succeed, it must identify those essences, learning the sharp boundaries between longing and yearning. As the forgeries improve, the critic becomes more discerning, further inspiring the forger. Although the networks are taught the essences – the labels – explicitly, the two together learn about van Gough. And they’ll learn without supervision.&lt;/p>
&lt;p>After learning, the critic can be deployed for standard categorization tasks (e.g., aiding medical diagnoses). But the training also produces another useful machine, a machine that is capable of generating images. Predictably, there are challenges to training a generator that is capable of producing good quality, large, and diverse images. But I didn’t need the images to be stellar, so long as their content was clear (to a human). A lack of photorealism – imperfect training – could make the pictures more interesting&lt;a href="#fn1" class="footnote-ref" id="fnref1">&lt;sup>1&lt;/sup>&lt;/a>. To make a gift, I wanted a forger that could paint New England.&lt;/p>
&lt;/div>
&lt;div id="setup" class="section level1">
&lt;h1>Setup&lt;/h1>
&lt;p>I wanted the forger to generate images of New England, so I first needed a bunch of pictures of New England. I have photographed a few hundred pictures, but this wouldn’t be nearly enough&lt;a href="#fn2" class="footnote-ref" id="fnref2">&lt;sup>2&lt;/sup>&lt;/a>. Instead, I relied on a combination of Google’s &lt;a href="https://developers.google.com/maps/documentation/streetview/overview">Street View Static&lt;/a> and &lt;a href="https://developers.google.com/maps/documentation/directions/overview">Directions&lt;/a> APIs. The Street View API gives a picture associated with a location, and those locations were provided by the Directions API. &lt;a href="https://github.com/psadil/gan-mass">The repository&lt;/a> for the network has the details, but the result was that I could input an origin and a destination – meandering through a few waypoints – and download whatever the Street View Car recorded when it traveled along those directions&lt;a href="#fn3" class="footnote-ref" id="fnref3">&lt;sup>3&lt;/sup>&lt;/a>. In the end, I collected ~25,000 images.&lt;/p>
&lt;p>25k may feel like a lot of images. But skimming online suggested that &lt;a href="https://blogs.nvidia.com/blog/2020/12/07/neurips-research-limited-data-gan/">even 25k would not have been enough to adequately constrain the networks&lt;/a>. GANs may not require labeled examples, but they are still data-hungry. Given my relatively small dataset, I picked an adversarial architecture that incorporates a few extra tricks to glean information from smaller datasets: &lt;a href="https://github.com/NVlabs/stylegan2-ada">stylegan2-ada&lt;/a>&lt;a href="#fn4" class="footnote-ref" id="fnref4">&lt;sup>4&lt;/sup>&lt;/a>. To train the network, I used free credits on the Google’s cloud console.&lt;/p>
&lt;/div>
&lt;div id="curated-samples" class="section level1">
&lt;h1>Curated Samples&lt;/h1>
&lt;p>After one day of training&lt;a href="#fn5" class="footnote-ref" id="fnref5">&lt;sup>5&lt;/sup>&lt;/a>, the network started producing some useful images.&lt;/p>
&lt;div class="figure">
&lt;img src="print_5x7_256.png" alt="" />
&lt;p class="caption">These six images are fake, produced from a collaboration to learn about New England.&lt;/p>
&lt;/div>
&lt;p>I chose these six – and the seventh at the top – because they illustrate a few fun features of what the GAN learned. For example, the GAN learned, very early, that pictures of New England always have, in the bottom corners, the word “Google”&lt;a href="#fn6" class="footnote-ref" id="fnref6">&lt;sup>6&lt;/sup>&lt;/a>. That machine learning can produce realistic text surprises me (e.g., &lt;a href="https://www.facebook.com/botsofnewyork/photos/a.2028566864113743/2490502274586864/?type=3&amp;amp;theater">if the face is weird, how are all of the pixels in place to spell out a word&lt;/a>?!). I assume that text comes out clean because most lettering is tightly constrained. That is, when the forger paints something that could be categorized as lettering, the critic severely constrains those pixels; fuzzy letters betray forgery, and real photographs don’t have nonsense like UNS;QD*LKJ. So if the training images contain enough text that the generator starts producing letters, there is also enough text for the critic to learn what text is realistic.&lt;/p>
&lt;p>The forger had difficulty with buildings. I downloaded mostly images of the highways connecting cities. This means that there were enough cityscapes for the GAN to generate buildings, but relative to a road, it was much slower at learning the intricacies of a building. Of course, the roads are imperfect, too (the telephone pole in the upper middle ripples, the upper left has too many roads, the colors of the painted lines mismatch, etc). But unlike, say, a bad photoshop, these errors have a kind of global coherence that, subjectively, allows the images to seem not fake but instead surreal.&lt;/p>
&lt;/div>
&lt;div class="footnotes">
&lt;hr />
&lt;ol>
&lt;li id="fn1">&lt;p>If I wanted perfect pictures, I could have just used a camera.&lt;a href="#fnref1" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn2">&lt;p>The van Gough example is slightly misleading; in practice, van Gough didn’t paint enough pictures to train a GAN. Training a GAN from scratch doesn’t require labeled data, but it still requires many images. There are tricks that could help a GAN, but simply training his images would likely be insufficient.&lt;a href="#fnref2" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn3">&lt;p>Having not owned a car during graduate school, I found it funny that these networks learned about New England through its highways&lt;a href="#fnref3" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn4">&lt;p>But also, they provided a helpful docker image, functions to prep the data, and decent documentation. This is a good reminder about the benefits of polishing a repository.&lt;a href="#fnref4" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn5">&lt;p>After one day, the training error was still decreasing. But I was using a &lt;a href="https://cloud.google.com/compute/docs/instances/preemptible">preemptible virtual machine&lt;/a>, and so after 24 hours it was automatically shutdown.&lt;a href="#fnref5" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn6">&lt;p>I removed the text from the curated examples, but it can be seen in the preview image at the top.&lt;a href="#fnref6" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>counterbalanced continuous designs with eulerian walks</title><link>https://psadil.github.io/psadil/post/counterbalanced-continuous-designs-with-eulerian-walks/</link><pubDate>Sat, 14 Mar 2020 00:00:00 +0000</pubDate><guid>https://psadil.github.io/psadil/post/counterbalanced-continuous-designs-with-eulerian-walks/</guid><description>
&lt;script src="https://psadil.github.io/psadil/post/counterbalanced-continuous-designs-with-eulerian-walks/index.en_files/header-attrs/header-attrs.js">&lt;/script>
&lt;script src="https://psadil.github.io/psadil/post/counterbalanced-continuous-designs-with-eulerian-walks/index.en_files/htmlwidgets/htmlwidgets.js">&lt;/script>
&lt;script src="https://psadil.github.io/psadil/post/counterbalanced-continuous-designs-with-eulerian-walks/index.en_files/viz/viz.js">&lt;/script>
&lt;link href="https://psadil.github.io/psadil/post/counterbalanced-continuous-designs-with-eulerian-walks/index.en_files/DiagrammeR-styles/styles.css" rel="stylesheet" />
&lt;script src="https://psadil.github.io/psadil/post/counterbalanced-continuous-designs-with-eulerian-walks/index.en_files/grViz-binding/grViz.js">&lt;/script>
&lt;p>Many experiments require counterbalancing sequences of trials. For example, I’m currently running an experiment on &lt;a href="https://psadil.github.io/psadil/post/serial-dependence/">serial dependence&lt;/a>&lt;a href="#fn1" class="footnote-ref" id="fnref1">&lt;sup>1&lt;/sup>&lt;/a>. In my experiment, participants report the orientation of a grating&lt;a href="#fn2" class="footnote-ref" id="fnref2">&lt;sup>2&lt;/sup>&lt;/a> stimulus on each trial. The serial dependence effect is how their responses on one trial depend on either the orientation of the previous trial or their response on that trial. To tease apart the effects of prior stimuli from prior responses, I’m manipulating the visual contrast of the gratings ( &lt;a href="https://en.wikipedia.org/wiki/Contrast_(vision)#Michelson_contrast">Michelson contrast&lt;/a> ). There are three levels of contrast: high, low, and zero (at zero contrast, there is no grating stimulus). This experiment will only need a few of the eight possible pairs of contrasts, and I’d like a sequence of trials that does not have any filler trials. So I need a flexible way to generate sequences of contrast.&lt;/p>
&lt;p>It turns out that this problem can be formulated as constructing an &lt;a href="https://en.wikipedia.org/wiki/Eulerian_path">Eulerian, directed cycle&lt;/a>. There are likely other ways &lt;a href="#fn3" class="footnote-ref" id="fnref3">&lt;sup>3&lt;/sup>&lt;/a>, but I think this is a neat approach. I won’t talk much about why any of this works, primarily because I don’t feel qualified to do so. However, the post includes a script that implements the algorithm, and checks that it has worked. So, hopefully it’ll be useful to at least a future me. But before discussing an Eulerian circuit, let’s talk about formulating the stimulus conditions as a graph.&lt;/p>
&lt;div id="trials-can-be-represented-with-a-graph" class="section level1">
&lt;h1>Trials can be represented with a graph&lt;/h1>
&lt;p>All potential sequences of trials will be represented as a graph. The graphs nodes will correspond to conditions, and edges between the nodes will correspond to allowable transitions. To represent these graphs, I’ll use the &lt;a href="http://visualizers.co/diagrammer/">&lt;code>DiagrammeR&lt;/code> package&lt;/a>.&lt;/p>
&lt;pre class="r">&lt;code># library(DiagrammeR)
library(magrittr)
# library(dplyr)&lt;/code>&lt;/pre>
&lt;p>In the graph of my experiment, there will be three nodes for each of the three conditions (Figure &lt;a href="#fig:nodes">1&lt;/a>).&lt;/p>
&lt;pre class="r">&lt;code>nodes &amp;lt;- DiagrammeR::create_node_df(
n = 3,
label = c(&amp;quot;zero&amp;quot;,&amp;quot;low&amp;quot;,&amp;quot;high&amp;quot;))
DiagrammeR::create_graph(nodes_df = nodes) %&amp;gt;%
DiagrammeR::render_graph(layout = &amp;quot;tree&amp;quot;)&lt;/code>&lt;/pre>
&lt;div class="figure">&lt;span id="fig:nodes">&lt;/span>
&lt;div id="htmlwidget-1" style="width:672px;height:480px;" class="grViz html-widget">&lt;/div>
&lt;script type="application/json" data-for="htmlwidget-1">{"x":{"diagram":"digraph {\n\ngraph [layout = \"neato\",\n outputorder = \"edgesfirst\",\n bgcolor = \"white\"]\n\nnode [fontname = \"Helvetica\",\n fontsize = \"10\",\n shape = \"circle\",\n fixedsize = \"true\",\n width = \"0.5\",\n style = \"filled\",\n fillcolor = \"aliceblue\",\n color = \"gray70\",\n fontcolor = \"gray50\"]\n\nedge [fontname = \"Helvetica\",\n fontsize = \"8\",\n len = \"1.5\",\n color = \"gray80\",\n arrowsize = \"0.5\"]\n\n \"1\" [label = \"zero\", fillcolor = \"#F0F8FF\", fontcolor = \"#000000\", pos = \"0,1!\"] \n \"2\" [label = \"low\", fillcolor = \"#F0F8FF\", fontcolor = \"#000000\", pos = \"1,1!\"] \n \"3\" [label = \"high\", fillcolor = \"#F0F8FF\", fontcolor = \"#000000\", pos = \"2,1!\"] \n}","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}&lt;/script>
&lt;p class="caption">
Figure 1: Node represent experimental conditions.
&lt;/p>
&lt;/div>
&lt;p>In my experiment, I want trials to go from low to high, zero to high, or high to high, high to low, and high to zero (Figure &lt;a href="#fig:edges">2&lt;/a>). Including only these five types of transitions means excluding a few of the possible edges that could be in the graph. For example, I do not want any zero contrast trials to follow any other zero contrast trials, nor do I want a low contrast trial to follow a zero contrast trial.&lt;/p>
&lt;pre class="r">&lt;code>edges &amp;lt;- DiagrammeR::create_edge_df(
from = c(1,2,3,3,3),
to = c(3,3,3,1,2))
DiagrammeR::create_graph(
nodes_df = nodes,
edges_df = edges) %&amp;gt;%
DiagrammeR::render_graph(
layout = &amp;quot;tree&amp;quot;)&lt;/code>&lt;/pre>
&lt;div class="figure">&lt;span id="fig:edges">&lt;/span>
&lt;div id="htmlwidget-2" style="width:672px;height:480px;" class="grViz html-widget">&lt;/div>
&lt;script type="application/json" data-for="htmlwidget-2">{"x":{"diagram":"digraph {\n\ngraph [layout = \"neato\",\n outputorder = \"edgesfirst\",\n bgcolor = \"white\"]\n\nnode [fontname = \"Helvetica\",\n fontsize = \"10\",\n shape = \"circle\",\n fixedsize = \"true\",\n width = \"0.5\",\n style = \"filled\",\n fillcolor = \"aliceblue\",\n color = \"gray70\",\n fontcolor = \"gray50\"]\n\nedge [fontname = \"Helvetica\",\n fontsize = \"8\",\n len = \"1.5\",\n color = \"gray80\",\n arrowsize = \"0.5\"]\n\n \"1\" [label = \"zero\", fillcolor = \"#F0F8FF\", fontcolor = \"#000000\", pos = \"0,2!\"] \n \"2\" [label = \"low\", fillcolor = \"#F0F8FF\", fontcolor = \"#000000\", pos = \"1,2!\"] \n \"3\" [label = \"high\", fillcolor = \"#F0F8FF\", fontcolor = \"#000000\", pos = \"0.5,1!\"] \n \"1\"->\"3\" \n \"2\"->\"3\" \n \"3\"->\"3\" \n \"3\"->\"1\" \n \"3\"->\"2\" \n}","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}&lt;/script>
&lt;p class="caption">
Figure 2: Directed edges between nodes represent allowable transitions.
&lt;/p>
&lt;/div>
&lt;p>Constructing a sequence of trials will correspond to walking along the edges, from node to node. That walk will be Eulerian if each edge is be visited exactly once. With so few edges, it’s easy enough to visualize an Eulerian walk through the edges. One possible Eulerian walk (a cycle&lt;a href="#fn4" class="footnote-ref" id="fnref4">&lt;sup>4&lt;/sup>&lt;/a>, even) is shown in Figure &lt;a href="#fig:smallwalk">3&lt;/a>.&lt;/p>
&lt;pre class="r">&lt;code>edges_labelled &amp;lt;- DiagrammeR::create_edge_df(
from = c(3,2,3,3,1),
to = c(2,3,3,1,3),
label = as.character(1:5))
DiagrammeR::create_graph(
nodes_df = nodes,
edges_df = edges_labelled) %&amp;gt;%
DiagrammeR::render_graph(
layout = &amp;quot;tree&amp;quot;)&lt;/code>&lt;/pre>
&lt;div class="figure">&lt;span id="fig:smallwalk">&lt;/span>
&lt;div id="htmlwidget-3" style="width:672px;height:480px;" class="grViz html-widget">&lt;/div>
&lt;script type="application/json" data-for="htmlwidget-3">{"x":{"diagram":"digraph {\n\ngraph [layout = \"neato\",\n outputorder = \"edgesfirst\",\n bgcolor = \"white\"]\n\nnode [fontname = \"Helvetica\",\n fontsize = \"10\",\n shape = \"circle\",\n fixedsize = \"true\",\n width = \"0.5\",\n style = \"filled\",\n fillcolor = \"aliceblue\",\n color = \"gray70\",\n fontcolor = \"gray50\"]\n\nedge [fontname = \"Helvetica\",\n fontsize = \"8\",\n len = \"1.5\",\n color = \"gray80\",\n arrowsize = \"0.5\"]\n\n \"1\" [label = \"zero\", fillcolor = \"#F0F8FF\", fontcolor = \"#000000\", pos = \"0,2!\"] \n \"2\" [label = \"low\", fillcolor = \"#F0F8FF\", fontcolor = \"#000000\", pos = \"1,2!\"] \n \"3\" [label = \"high\", fillcolor = \"#F0F8FF\", fontcolor = \"#000000\", pos = \"0.5,1!\"] \n\"3\"->\"2\" [label = \"1\"] \n\"2\"->\"3\" [label = \"2\"] \n\"3\"->\"3\" [label = \"3\"] \n\"3\"->\"1\" [label = \"4\"] \n\"1\"->\"3\" [label = \"5\"] \n}","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}&lt;/script>
&lt;p class="caption">
Figure 3: The numbers trace an Eulerian cycle on this graph.
&lt;/p>
&lt;/div>
&lt;p>The cycle in Figure &lt;a href="#fig:smallwalk">3&lt;/a> implies a workable sequence of six trials, but the use of this Eulerian conceptualization will be how it automates creating much longer sequences. For example, to achieve 21 trials the edges could be replicated four times. Figure &lt;a href="#fig:messy">4&lt;/a> shows the graph with replicated edges, and already it looks too messy to traverse by sight. A real experiment will involve hundreds of trials, meaning that we’d like a way to automatically traverse an Eulerian circuit.&lt;/p>
&lt;pre class="r">&lt;code>edges_messy &amp;lt;- DiagrammeR::create_edge_df(
from = rep(c(3,2,3,3,1), each=4),
to = rep(c(2,3,3,1,3), each=4))
DiagrammeR::create_graph(
nodes_df = nodes,
edges_df = edges_messy) %&amp;gt;%
DiagrammeR::render_graph(layout = &amp;quot;tree&amp;quot;)&lt;/code>&lt;/pre>
&lt;div class="figure">&lt;span id="fig:messy">&lt;/span>
&lt;div id="htmlwidget-4" style="width:672px;height:480px;" class="grViz html-widget">&lt;/div>
&lt;script type="application/json" data-for="htmlwidget-4">{"x":{"diagram":"digraph {\n\ngraph [layout = \"neato\",\n outputorder = \"edgesfirst\",\n bgcolor = \"white\"]\n\nnode [fontname = \"Helvetica\",\n fontsize = \"10\",\n shape = \"circle\",\n fixedsize = \"true\",\n width = \"0.5\",\n style = \"filled\",\n fillcolor = \"aliceblue\",\n color = \"gray70\",\n fontcolor = \"gray50\"]\n\nedge [fontname = \"Helvetica\",\n fontsize = \"8\",\n len = \"1.5\",\n color = \"gray80\",\n arrowsize = \"0.5\"]\n\n \"1\" [label = \"zero\", fillcolor = \"#F0F8FF\", fontcolor = \"#000000\", pos = \"0,2!\"] \n \"2\" [label = \"low\", fillcolor = \"#F0F8FF\", fontcolor = \"#000000\", pos = \"1,2!\"] \n \"3\" [label = \"high\", fillcolor = \"#F0F8FF\", fontcolor = \"#000000\", pos = \"0.5,1!\"] \n \"3\"->\"2\" \n \"3\"->\"2\" \n \"3\"->\"2\" \n \"3\"->\"2\" \n \"2\"->\"3\" \n \"2\"->\"3\" \n \"2\"->\"3\" \n \"2\"->\"3\" \n \"3\"->\"3\" \n \"3\"->\"3\" \n \"3\"->\"3\" \n \"3\"->\"3\" \n \"3\"->\"1\" \n \"3\"->\"1\" \n \"3\"->\"1\" \n \"3\"->\"1\" \n \"1\"->\"3\" \n \"1\"->\"3\" \n \"1\"->\"3\" \n \"1\"->\"3\" \n}","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}&lt;/script>
&lt;p class="caption">
Figure 4: Replicating edges quickly complicates the graph.
&lt;/p>
&lt;/div>
&lt;/div>
&lt;div id="hierholzers-algorithm-automates-eulerian-cycles" class="section level1">
&lt;h1>Hierholzer’s algorithm automates Eulerian cycles&lt;/h1>
&lt;p>Fortunately, there exists and algorithm for making Eulerian cycles that is both simple to implement and quick to run. First, here is a helper function to replicate edges, &lt;code>replicate_edges&lt;/code>.&lt;/p>
&lt;pre class="r">&lt;code>#&amp;#39; replicate_edges
#&amp;#39;
#&amp;#39; @param edge_df output of DiagrammeR::create_edge_df (will only need columns `to` and `from`)
#&amp;#39; @param n_reps integer number of times that the edges should be replicated
#&amp;#39;
#&amp;#39; @return replicated edge dataframe
replicate_edges &amp;lt;- function(edge_df, n_reps){
replicate(n_reps, edge_df, simplify = FALSE) %&amp;gt;%
dplyr::bind_rows() %&amp;gt;%
dplyr::mutate(id = 1:dplyr::n())
}&lt;/code>&lt;/pre>
&lt;p>The next function will generate the Eulerian circuit, &lt;code>walk_circuit&lt;/code>. It will take in an edge dataframe (possibly replicated) and output a vector containing the nodes listed in the order that they were reached. Again, I won’t spend too long explaining why this works. But the basic idea is to traverse the edges, deleting edges as you walk along them. You’ll eventually reach a dead-end. If there are still more edges, then backtrack until you can travel along an edge that will result in a different dead-end. Save a list of the nodes that were traveled while backtracking, and these nodes will contain the circuit.&lt;/p>
&lt;pre class="r">&lt;code>#&amp;#39; walk_circuit
#&amp;#39;
#&amp;#39; @param edge_df edge dataframes
#&amp;#39; @param curr_v vertex at which to start the circuit
#&amp;#39;
#&amp;#39; @return vector consisting of Eulerian circuit along edges
#&amp;#39;
#&amp;#39; @details modified python script from https://gregorulm.com/finding-an-eulerian-path/.
walk_circuit &amp;lt;- function(edge_df, curr_v){
# helpful to have the edges stored by node
adj &amp;lt;- edge_df %&amp;gt;%
dplyr::group_split(from)
# vector to store final circuit
circuit &amp;lt;- c()
# Maintain a stack to keep vertices
# start from given node
curr_path &amp;lt;- curr_v
while (length(curr_path)){
# If there&amp;#39;s a remaining edge
if (nrow(adj[[curr_v]])){
# Push the vertex
curr_path &amp;lt;- c(curr_path,curr_v)
# Find the next vertex using an edge
next_v_ind &amp;lt;- sample.int(nrow(adj[[curr_v]]), size=1)
next_v &amp;lt;- adj[[curr_v]]$to[next_v_ind]
# and remove that edge
adj[[curr_v]] &amp;lt;- adj[[curr_v]][-next_v_ind,]
# Move to next vertex
curr_v &amp;lt;- next_v
} else{ # back-track to find remaining circuit
circuit &amp;lt;- c(circuit, curr_v)
# Back-tracking
curr_v &amp;lt;- tail(curr_path, n = 1)
curr_path &amp;lt;- head(curr_path, n = -1)
}
}
return(rev(circuit))
}&lt;/code>&lt;/pre>
&lt;p>Now replicate the edges twice and go for and Eulerian tour.&lt;/p>
&lt;pre class="r">&lt;code>edges_twice &amp;lt;- replicate_edges(edges, 2)
walk_circuit(edges_twice, 3)&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] 3 1 3 2 3 1 3 3 2 3 3&lt;/code>&lt;/pre>
&lt;p>This sequence is small enough that it’s feasible to verify the Eulerian property by hand, but it’ll be nice to have automate the checking. That is the purpose of this next function, &lt;code>check_blocking&lt;/code>.&lt;/p>
&lt;pre class="r">&lt;code>#&amp;#39; check_blocking
#&amp;#39;
#&amp;#39; @param circuit output of walk_circuit
#&amp;#39; @param nodes nodes_df, output of DiagrammeR::create_node_df. Used to label which nodes were visited during the walk
#&amp;#39;
#&amp;#39; @return tbl containing the counts of each transition type contained in the circuit.
#&amp;#39; If all went well, the counts should be equal
check_blocking &amp;lt;- function(circuit, nodes){
tibble::tibble(contrast = circuit, .name_repair = &amp;quot;check_unique&amp;quot;) %&amp;gt;%
dplyr::mutate(
trial = 1:dplyr::n(),
contrast = nodes$label[contrast],
last_contrast = dplyr::lag(contrast)) %&amp;gt;%
dplyr::filter(trial &amp;gt; 1) %&amp;gt;%
dplyr::group_by(contrast, last_contrast) %&amp;gt;%
dplyr::summarise(n = dplyr::n(), .groups = &amp;quot;drop&amp;quot;)
}&lt;/code>&lt;/pre>
&lt;p>Now, generate a sequence of 101 trials,&lt;/p>
&lt;pre class="r">&lt;code>edges_large &amp;lt;- replicate_edges(edges, n_reps = 20)
circuit &amp;lt;- walk_circuit(edges_large, 3)
circuit&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] 3 2 3 3 1 3 2 3 2 3 3 1 3 1 3 3 1 3 3 2 3 1 3 2 3 2 3 1 3 3 1 3 3 2 3 3 3
## [38] 1 3 1 3 1 3 3 2 3 2 3 3 2 3 2 3 1 3 1 3 3 2 3 1 3 1 3 3 2 3 1 3 3 3 3 3 2
## [75] 3 3 1 3 2 3 3 2 3 1 3 1 3 3 1 3 2 3 2 3 1 3 3 2 3 2 3&lt;/code>&lt;/pre>
&lt;p>and check that each transition happened equally often&lt;/p>
&lt;pre class="r">&lt;code>check_blocking(circuit, nodes) %&amp;gt;%
knitr::kable()&lt;/code>&lt;/pre>
&lt;table>
&lt;thead>
&lt;tr class="header">
&lt;th align="left">contrast&lt;/th>
&lt;th align="left">last_contrast&lt;/th>
&lt;th align="right">n&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr class="odd">
&lt;td align="left">high&lt;/td>
&lt;td align="left">high&lt;/td>
&lt;td align="right">20&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="left">high&lt;/td>
&lt;td align="left">low&lt;/td>
&lt;td align="right">20&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td align="left">high&lt;/td>
&lt;td align="left">zero&lt;/td>
&lt;td align="right">20&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="left">low&lt;/td>
&lt;td align="left">high&lt;/td>
&lt;td align="right">20&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td align="left">zero&lt;/td>
&lt;td align="left">high&lt;/td>
&lt;td align="right">20&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;/div>
&lt;div class="footnotes">
&lt;hr />
&lt;ol>
&lt;li id="fn1">&lt;p>Well, I was running. Out of precaution for COVID-19, it currently seems like a bad idea to try to collect more participants. And UMass is closed for the rest of the semester.&lt;a href="#fnref1" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn2">&lt;p>adjacent black and white lines cropped to a circle, where the transitions between luminance follows a sinusoid&lt;a href="#fnref2" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn3">&lt;p>In this particular case, a simpler solution would be to assign each pair of contrasts a number. For example,&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>high -&amp;gt; high&lt;/li>
&lt;li>low -&amp;gt; high&lt;/li>
&lt;li>zero -&amp;gt; high&lt;/li>
&lt;/ol>
&lt;p>An appropriate sequence could be generated by simply permuting the numbers. For example 2, 3, 1, 3, 2 In that case, the sequence of trials would be &lt;code>low high zero high high high zero high low high&lt;/code>. This works because the second trial of each of the transitions are &lt;code>high&lt;/code>. But what if you also wanted a few &lt;code>low-&amp;gt;low&lt;/code> and &lt;code>zero-&amp;gt;zero&lt;/code> transitions, but wanted neither &lt;code>low-&amp;gt;zero&lt;/code> nor &lt;code>zero-&amp;gt;low&lt;/code>? By simply permuting the number codes, a &lt;code>zero-&amp;gt;zero&lt;/code> transition could appear right after a &lt;code>low-&amp;gt;low&lt;/code> transition, but to do that would require a filler &lt;code>low-zero&lt;/code>.&lt;a href="#fnref3" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn4">&lt;p>a cycle or circuit is a walk that starts and ends at the same node&lt;a href="#fnref4" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>thoughts on eye movement</title><link>https://psadil.github.io/psadil/post/thoughts-on-eye-movement/</link><pubDate>Thu, 21 Nov 2019 00:00:00 +0000</pubDate><guid>https://psadil.github.io/psadil/post/thoughts-on-eye-movement/</guid><description>
&lt;script src="https://psadil.github.io/psadil/post/thoughts-on-eye-movement/index.en_files/header-attrs/header-attrs.js">&lt;/script>
&lt;p>Visual perception research has produced many illusions. Stare at a waterfall for a minute, then look away and the whole world appears in motion, traveling upwards &lt;span class="citation">(&lt;a href="#ref-addams1834" role="doc-biblioref">Addams 1834&lt;/a>)&lt;/span>. Given proper lighting, black paper can appear white, but placing a white piece of paper nearby colors the first dark gray &lt;span class="citation">(&lt;a href="#ref-gelb1929" role="doc-biblioref">Gelb 1929&lt;/a>; as cited by &lt;a href="#ref-cataliotti1995" role="doc-biblioref">Cataliotti and Gilchrist 1995&lt;/a>)&lt;/span>. Inspecting two sets of black lines – horizontal lines that obscure a solid red field like a picket fence, along with vertical lines that obscure a solid green field – causes the black lines alone to induce a perception of color, an illusory shading that can last for days &lt;span class="citation">(&lt;a href="#ref-mccollough1965" role="doc-biblioref">McCollough 1965&lt;/a>)&lt;/span>. Such illusions reveal the intricacies of visual perception, kludges and all&lt;a href="#fn1" class="footnote-ref" id="fnref1">&lt;sup>1&lt;/sup>&lt;/a>. The pizzazz of the illusions affords visual perception a kind of scientific rigor; the effects are obviously real and reproducible, so to satisfyingly explain how visual perception works &lt;em>so well&lt;/em> also requires explaining how atypical visual environments can so often dupe vision.&lt;/p>
&lt;p>However, research on visual perception inevitably strays from fascinating and easily demonstrable illusions. Of course, even without the glamour of classic visual illusions an effect can still be a reliable and valid object of research. But as the effect becomes more subtle, observing the effect requires increasingly complex analyses. Unfortunately, the most complex analyses, when misapplied, can also transmute noise into something that appears noteworthy&lt;a href="#fn2" class="footnote-ref" id="fnref2">&lt;sup>2&lt;/sup>&lt;/a>. So when a subtle effect relies entirely on a complex analysis, the effect becomes suspect. To highlight the obviousness of an effect, it can help to visualize and revisualize the data.&lt;/p>
&lt;p>A few weeks ago, &lt;a href="https://psadil.github.io/psadil/post/serial-dependence/">I posted&lt;/a> about an effect that warrants revisualizing, the apparent stability of visual perception. I discussed this stability as one of the reasons that perception exhibits serial dependence &lt;span class="citation">(&lt;a href="#ref-fischer2014" role="doc-biblioref">Fischer and Whitney 2014&lt;/a>)&lt;/span>, which is the tendency of visual perception to be slightly erroneous, resembling not an accurate reproduction of the input it receives but a mixture of current input and input from the past. This dependence may occur during perception to refine the inherently erratic input provided by the retina. I attempt to demonstrate the need for refinement with Figures 1-3. Figure &lt;a href="#fig:density">1&lt;/a> shows a stimulus from an ongoing experiment. In the experiment, the participant was required to simply hold their gaze still. The figure is a heatmap, showing that this participant successfully fixated on a small region of the stimulus. However, the heatmap obscures how fixating on a “small” region implies ample movement. Figure &lt;a href="#fig:centric1">2&lt;/a> recapitulates, in real time, how the gaze wandered during fixation. But then Figure &lt;a href="#fig:centric1">2&lt;/a> obscures what that wandering means for the visual system; whenever the eye moves, the retina receives (and so must then output) a different image. With Figure &lt;a href="#fig:final">3&lt;/a>, I attempt to visualize what these eye movements mean for the retinal image. In Figure &lt;a href="#fig:final">3&lt;/a>, the movements of the gaze are transferred to the stimulus, revealing how the retina receives a twitching stimulus. The effect to explain here is why fixating at the dot in Figure &lt;a href="#fig:density">1&lt;/a> – given that the eyes move as shown in Figure &lt;a href="#fig:centric1">2&lt;/a> – does not elicit the jumpy movie depicted in Figure &lt;a href="#fig:final">3&lt;/a>, but instead elicits the stable image of Figure &lt;a href="#fig:density">1&lt;/a>.&lt;/p>
&lt;div class="figure">&lt;span id="fig:density">&lt;/span>
&lt;img src="https://psadil.github.io/psadil/post/thoughts-on-eye-movement/index.en_files/figure-html/density-1.png" alt="Example stimulus, with heatmap of eye positions during one trial overlayed. Participants viewed such grating stimuli, each for five seconds. They were instructed to fixate on the central magenta dot. The blob to the left of the dot indicates where this participant looked during the trial; the brightest regions held their gaze for the most time." width="672" />
&lt;p class="caption">
Figure 1: Example stimulus, with heatmap of eye positions during one trial overlayed. Participants viewed such grating stimuli, each for five seconds. They were instructed to fixate on the central magenta dot. The blob to the left of the dot indicates where this participant looked during the trial; the brightest regions held their gaze for the most time.
&lt;/p>
&lt;/div>
&lt;div class="figure">&lt;span id="fig:centric1">&lt;/span>
&lt;img src="https://psadil.github.io/psadil/post/thoughts-on-eye-movement/index.en_files/figure-html/centric1-1.gif" alt="Time course of fixations from Figure 1. The blue dot shows where the participant looked at each moment. Fixations were recorded at 1000 Hz, but this video has been downsampled to 10 Hz." />
&lt;p class="caption">
Figure 2: Time course of fixations from Figure 1. The blue dot shows where the participant looked at each moment. Fixations were recorded at 1000 Hz, but this video has been downsampled to 10 Hz.
&lt;/p>
&lt;/div>
&lt;div class="figure">&lt;span id="fig:final">&lt;/span>
&lt;img src="https://psadil.github.io/psadil/post/thoughts-on-eye-movement/index.en_files/figure-html/final-1.gif" alt="Approximate example of the retinal image from Figure 2. While the gaze travels throughout the visual environment, that environment is largely stable. Yet the travelling gaze constantly alters the image imprinted on the retina." />
&lt;p class="caption">
Figure 3: Approximate example of the retinal image from Figure 2. While the gaze travels throughout the visual environment, that environment is largely stable. Yet the travelling gaze constantly alters the image imprinted on the retina.
&lt;/p>
&lt;/div>
&lt;div id="references" class="section level1 unnumbered">
&lt;h1>References&lt;/h1>
&lt;div id="refs" class="references csl-bib-body hanging-indent">
&lt;div id="ref-addams1834" class="csl-entry">
Addams, Roberts. 1834. &lt;span>“An Account of a Peculiar Optical Phenomenon Seen After Having Looked at a Moving Body.”&lt;/span> &lt;em>The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science&lt;/em> 5 (29): 373–74.
&lt;/div>
&lt;div id="ref-cataliotti1995" class="csl-entry">
Cataliotti, Joseph, and Alan Gilchrist. 1995. &lt;span>“Local and Global Processes in Surface Lightness Perception.”&lt;/span> &lt;em>Perception &amp;amp; Psychophysics&lt;/em> 57 (2): 125–35.
&lt;/div>
&lt;div id="ref-fischer2014" class="csl-entry">
Fischer, Jason, and David Whitney. 2014. &lt;span>“Serial Dependence in Visual Perception.”&lt;/span> &lt;em>Nature Neuroscience&lt;/em> 17 (5): 738.
&lt;/div>
&lt;div id="ref-gelb1929" class="csl-entry">
Gelb, Adhémar. 1929. &lt;span>“Die "Farbenkonstanz" Der Sehdinge.”&lt;/span> In &lt;em>Handbuch Der Normalen Und Pathologischen Physiologie&lt;/em>, 594–678. Springer-Verlag.
&lt;/div>
&lt;div id="ref-mccollough1965" class="csl-entry">
McCollough, Celeste. 1965. &lt;span>“Color Adaptation of Edge-Detectors in the Human Visual System.”&lt;/span> &lt;em>Science&lt;/em> 149 (3688): 1115–16.
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="footnotes">
&lt;hr />
&lt;ol>
&lt;li id="fn1">&lt;p>To personally experience the illusions mentioned here – and many others – explore &lt;a href="https://michaelbach.de/ot/">Michael Bach’s website&lt;/a>.&lt;a href="#fnref1" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn2">&lt;p>Full disclosure: I write this as someone who has written &lt;a href="https://psadil.github.io/psadil/publication/sadil-2019-hierarchical/">a paper&lt;/a> on a novel development of an already obscure analysis, a development that I needed to support the claims in &lt;a href="https://psadil.github.io/psadil/publication/sadil-2019-connecting/">another paper&lt;/a>. I am a kettle.&lt;a href="#fnref2" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>Half of a parameter</title><link>https://psadil.github.io/psadil/post/half-of-a-parameter/</link><pubDate>Fri, 15 Nov 2019 00:00:00 +0000</pubDate><guid>https://psadil.github.io/psadil/post/half-of-a-parameter/</guid><description>
&lt;script src="https://psadil.github.io/psadil/post/half-of-a-parameter/index_files/header-attrs/header-attrs.js">&lt;/script>
&lt;p>Science produces models that provide parsimonious descriptions of the world. In cognitive psychology, models regularly compete to explain a few phenomena. But models can survive experiment after experiment, both because of the difficulty of capturing participant’s nuanced behavior, and because models often make highly overlapping predictions. In these cases, a model succeeds through its relative parsimony.&lt;/p>
&lt;p>In cognitive psychology, measures of information criteria, specifically Akaike’s and the Bayesian information criteria &lt;span class="citation">(&lt;a href="#ref-schwarz1978" role="doc-biblioref">Schwarz 1978&lt;/a>; &lt;a href="#ref-akaike1998" role="doc-biblioref">Akaike 1973&lt;/a>)&lt;/span>, determine a winning model. These criteria measure complexity by tallying the number of parameters in a model. Long maths and specific assumptions justify the claim that a model with more parameters is more complex than a model with fewer parameters, and so does our intuition that a model is complex if it has many moving parts. Unfortunately, the assumptions fail in common situations, such as when the models are fit in a Bayesian rather than frequentist setting. An alphabet soup of other information criteria exists (in addition to Akaike’s the Bayesian criteria, there is the DIC, WAIC, KIC, NIC, TIC, etc), and these other criteria assign complexity more complexly. These criteria are sensitive not only to the number of parameters in a model but also to the varied roles that a parameter can have. They assign the complexity of a model based on the model’s number of ‘effective parameters.’&lt;/p>
&lt;p>For intuition on why tallying the number of parameters is an insufficient measure complexity, consider two models of response time. Both models assume that response times are distributed according to a normal distribution. In this simple example, the variability of the distributions are known, and so the models have only a single free parameter, which is the average response time. In one model, that average can be any number, a value from negative to positive infinity. This is the kind of model implicitly assumed when we conduct a t-test on the averages of response times. Of course the model is a simplification of response times, but this model also has the glaring flaw that it allows the average response time to be negative; a participant cannot respond to stimulation before the stimulus appears. The second model addresses this flaw by adding the constraint that the average response time cannot be negative. Although the second model is more constrained, the models have the same number of parameters. The second model can only account for half of the patterns of data as the first; the second model is twice as parsimonious as the first &lt;span class="citation">(&lt;a href="#ref-gelman2014" role="doc-biblioref">Gelman, Hwang, and Vehtari 2014&lt;/a>)&lt;/span>. To adjudicate between these model requires a measure that is sensitive to complexity but does not simply tally the number of parameters in each model.&lt;/p>
&lt;div id="references" class="section level1 unnumbered">
&lt;h1>References&lt;/h1>
&lt;div id="refs" class="references csl-bib-body hanging-indent">
&lt;div id="ref-akaike1998" class="csl-entry">
Akaike, Hirotogu. 1973. &lt;span>“Information Theory and an Extension of the Maximum Likelihood Principle.”&lt;/span> In &lt;em>Proceedings of the Second International Symposium on Information Theory&lt;/em>, 267–81.
&lt;/div>
&lt;div id="ref-gelman2014" class="csl-entry">
Gelman, Andrew, Jessica Hwang, and Aki Vehtari. 2014. &lt;span>“Understanding Predictive Information Criteria for &lt;span>Bayesian&lt;/span> Models.”&lt;/span> &lt;em>Statistics and Computing&lt;/em> 24 (6): 997–1016. &lt;a href="https://doi.org/10.1007/s11222-013-9416-2">https://doi.org/10.1007/s11222-013-9416-2&lt;/a>.
&lt;/div>
&lt;div id="ref-schwarz1978" class="csl-entry">
Schwarz, Gideon. 1978. &lt;span>“Estimating the Dimension of a Model.”&lt;/span> &lt;em>The Annals of Statistics&lt;/em> 6 (2): 461–64. &lt;a href="https://doi.org/10.1214/aos/1176344136">https://doi.org/10.1214/aos/1176344136&lt;/a>.
&lt;/div>
&lt;/div>
&lt;/div></description></item><item><title>Staircases for Thresholds, Part 2</title><link>https://psadil.github.io/psadil/post/staircases-for-thresholds-part2/</link><pubDate>Fri, 08 Nov 2019 00:00:00 +0000</pubDate><guid>https://psadil.github.io/psadil/post/staircases-for-thresholds-part2/</guid><description>
&lt;script src="https://psadil.github.io/psadil/post/staircases-for-thresholds-part2/index.en_files/header-attrs/header-attrs.js">&lt;/script>
&lt;!--Incantation to add equation numbers
https://stackoverflow.com/questions/35026405/auto-number-equations-in-r-markdown-documents-in-rstudio-->
&lt;p>In &lt;a href="https://psadil.github.io/psadil/post/staircases-for-thresholds/">last week’s post&lt;/a>, I discussed how some experiments in cognitive psychology require researchers to pick a differently intense stimulus for each participant. In particular, I discussed a procedure for picking an intensity that elicits positive responses on approximately half of trials, the staircase procedure. In the staircase procedure, the researcher increases the intensity after every positive response and decreases the intensity after every negative response. If the participant completes another set of trials in which the intensity is fixed to the average of the intensities that were used during the staircase, the participant will provide positive responses approximately half of the time. But a researcher may want participants to give positive responses with a different proportion. A different proportion of responses can be achieved by transforming the staircase &lt;span class="citation">(&lt;a href="#ref-levitt1997" role="doc-biblioref">Levitt 1971&lt;/a>)&lt;/span>.&lt;/p>
&lt;p>The original staircase produced an intensity that elicited half positive responses by balancing the proportion of positive and negative responses. Intuitively, to elicit a higher proportion of positive responses, the transformed staircase must tilt this balance by converging on a more intense stimulus. Any staircase affects responses by changing the stimulus intensity as a function of how participants behave. The staircase that changes the intensity after every response is called a one-up, one-down staircase; one negative response causes the intensity to go up, one positive response causes the intensity to go down. A transformed staircase can make a positive response more likely by making intensity decreases less likely. The names of transformed staircases are analogous to the one-up, one-down label: a one-up, two-down staircase increases the stimulus after any negative response and decreases the intensity after two positive responses; a two-up, three-down staircase increases the intensity after two negative responses and only decreases the intensity after three positive responses; and so on. Altering when the staircase increments the intensity alters the intensity at which the staircase converges.&lt;/p>
&lt;p>We can use algebra to calculate the proportion of positive responses elicited by the intensity converged on by a staircase. As an example, consider a staircase that increases the intensity after a single positive response but decreases the intensity after two negative responses, a one-up, two-down staircase. Since the one-up, two-down regime results in fewer decreases than the one-up, one-down staircase, we should expect that the proportion of positive responses will be higher than half. For the calculation, note that there are three possible sequences of responses that result in an intensity change. Two of these sequences cause an increase, either a single negative or a positive followed by a negative. For the algebra later, let &lt;span class="math inline">\(p(x|i)\)&lt;/span> be the probability of obtaining a positive response at stimulus intensity, &lt;span class="math inline">\(i\)&lt;/span>. Our goal is to solve for this probability. Participants can only provide either positive or negative responses, so the probability of obtaining a negative response to that stimulus is &lt;span class="math inline">\(1-p(x|i)\)&lt;/span>. The probability of increasing the stimulus intensity away from intensity &lt;span class="math inline">\(i\)&lt;/span>, &lt;span class="math inline">\(p(\text{up|i})\)&lt;/span> is the sum of the probabilities for the two sequences:&lt;/p>
&lt;p>&lt;span class="math display" id="eq:pup">\[
\begin{equation}
p(\text{up|i}) = (1-p(x|i)) + p(x|i)(1-p(x|i)) \\
\tag{1}
\end{equation}
\]&lt;/span>&lt;/p>
&lt;p>There is only a single way in which the staircase decreases intensity: the participant must provide two positive responses in a row. The probability of the intensity decreasing away from &lt;span class="math inline">\(i\)&lt;/span>, &lt;span class="math inline">\(p(\text{down|i})\)&lt;/span> is equal to the probability of two positive responses to that stimulus, or&lt;/p>
&lt;p>&lt;span class="math display" id="eq:pdown">\[
\begin{equation}
p(\text{down|i}) = p(x|i)^2 \\
\tag{2}
\end{equation}
\]&lt;/span>&lt;/p>
&lt;p>Combining equations &lt;a href="#eq:pup">(1)&lt;/a> and &lt;a href="#eq:pdown">(2)&lt;/a> will give a relationship that determines the proportion of positive responses participants will tend to provide under this staircase. To see how to combine these equations, remember that the one-up, one-down staircase converged on an intensity for which the proportion of positive and negative responses were equal. This equality occurred because at that intensity, the probability of an up and down step were equally likely. So, to determine at which intensity the one-up, two-down staircase converges, we must determine the probability of a positive response that will make an up and down step likely in the one-up, two-down staircase. That is, we set the right hand sides of equations &lt;a href="#eq:pup">(1)&lt;/a> and &lt;a href="#eq:pdown">(2)&lt;/a> to be equal, and solve for &lt;span class="math inline">\(p(x|i)\)&lt;/span>&lt;a href="#fn1" class="footnote-ref" id="fnref1">&lt;sup>1&lt;/sup>&lt;/a>.&lt;/p>
&lt;p>&lt;span class="math display" id="eq:px">\[
\begin{equation}
\begin{aligned}
p(x|i)(1-p(x|i)) + (1-p(x|i)) &amp;amp; = p(x|i)^2 \\
\implies p(x|i) - p(x|i)^2 + 1-p(x|i) &amp;amp; = p(x|i)^2 \\
\implies -2 p(x|i)^2 &amp;amp; = -1 \\
\implies p(x|i)&amp;amp; = \frac{1}{\sqrt{2}} \\
&amp;amp; \approx 0.707
\end{aligned}
\tag{3}
\end{equation}
\]&lt;/span>&lt;/p>
&lt;p>Equation &lt;a href="#eq:px">(3)&lt;/a> shows that a one-up, two-down staircase will converge on a stimulus intensity that elicits approximately 70% positive responses (Figure &lt;a href="#fig:staircase">1&lt;/a>). As one way to see that this solution makes sense, relate this solution back to the probabilities of making either an up or down step. By equation &lt;a href="#eq:pdown">(2)&lt;/a>, this solution implies that at this intensity, that an up step occurs with a 50% probability. As desired, any sequence that elicits a transition has an equal chance of being one that elicits either an up or down step.&lt;/p>
&lt;p>The original staircase procedure capitalized on the idea that the stimulus intensity which elicits half positive responses can be estimated by starting from an arbitrary intensity, changing the intensity on every trial based on whether a participant responded positively or negatively, and then retroactively looking at which intensities were shown. The transformed staircase enables estimation of an intensity that elicits different behavior. Similar algebra to that outlined in this post can be used to determine the proportion of positive responses elicited by other staircases. Unfortunately, most proportions will not have an easy staircase regime. Moreover, complex staircases will only change the stimulus intensity infrequently, requiring more trials to estimate the converged upon intensity stably. However, the proportions reachable by simple staircase are often good enough; rare is the experiment that requires, not 70.7% but 73% positive responses. And the staircase procedure did not require any knowledge of the exact shape of the psychometric function, just that there was a psychometric function. The simplicity of the transformed staircase makes it an attractive way to pick an intensity.&lt;/p>
&lt;div class="figure">&lt;span style="display:block;" id="fig:staircase">&lt;/span>
&lt;img src="https://psadil.github.io/psadil/post/staircases-for-thresholds-part2/index.en_files/figure-html/staircase-1.png" alt="A sequence of trials with stimulus intensity governed by a one-up, two-down staircase. With this staircase, the intensity increases after a single negative response but decreases only after two positive responses. After enough trials, the average of the stimulus intensities shown to participants will elicit approximately 70% positive responses (dashed line). The intensity resulting from a one-up, one-down staircase is shown for comparison (solid line)." width="672" />
&lt;p class="caption">
Figure 1: A sequence of trials with stimulus intensity governed by a one-up, two-down staircase. With this staircase, the intensity increases after a single negative response but decreases only after two positive responses. After enough trials, the average of the stimulus intensities shown to participants will elicit approximately 70% positive responses (dashed line). The intensity resulting from a one-up, one-down staircase is shown for comparison (solid line).
&lt;/p>
&lt;/div>
&lt;div id="references" class="section level1 unnumbered">
&lt;h1>References&lt;/h1>
&lt;div id="refs" class="references csl-bib-body hanging-indent">
&lt;div id="ref-levitt1997" class="csl-entry">
Levitt, H. 1971. &lt;span>“Transformed up-down Methods in Psychoacoustics.”&lt;/span> &lt;em>The Journal of the Acoustical Society of America&lt;/em> 49 (2B): 467–77.
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="footnotes">
&lt;hr />
&lt;ol>
&lt;li id="fn1">&lt;p>The equation will have two solutions, but one of those solutions will be negative. A negative value is not an actual solution, because we are dealing with probabilities and so there is an additional constraint that &lt;span class="math inline">\(0 \leq p(x|i) \leq 1\)&lt;/span>&lt;a href="#fnref1" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>Staircases for Thresholds</title><link>https://psadil.github.io/psadil/post/staircases-for-thresholds/</link><pubDate>Wed, 30 Oct 2019 00:00:00 +0000</pubDate><guid>https://psadil.github.io/psadil/post/staircases-for-thresholds/</guid><description>
&lt;script src="https://psadil.github.io/psadil/post/staircases-for-thresholds/index_files/header-attrs/header-attrs.js">&lt;/script>
&lt;p>Performing any experiment on cognition requires deciding which stimuli to use. Some experiments require participants to make many errors, requiring the stimuli to be challenging. In other experiments, participants must respond accurately, requiring stimuli that are easy but not so easy that participants lose attention. Moreover, participants behave idiosyncratically, so to avoid wasting either the researchers’ or participants’ time the stimuli ought to be tailored to each participant. To decide which stimuli to use, researchers can rely on a psychometric function (Figure &lt;a href="#fig:psychometric">1&lt;/a>). These functions describe a relationship between the intensity of a stimulus and how a participant responds to that stimulus, when responses can be classified as either a positive or negative. Precisely what is meant by ‘intensity’ and ‘positive or negative’ depends on the experimental task, but they roughly correspond to the amount of stimulation on each trial and how difficult it is to notice that stimulation. In a task in which participants must detect a pure tone that is occasionally presented over white noise, the intensity could be the volume of the tone and participants’ responses are positive when they detect the tone. With a psychometric function, deciding on a stimulus translates to picking the proportion of trials that should receive positive responses – picking the desired difficulty – and then using the intensity that elicits that behavior. This replaces the task of picking stimuli with inferring participants’ psychometric functions.&lt;/p>
&lt;div class="figure">&lt;span id="fig:psychometric">&lt;/span>
&lt;img src="https://psadil.github.io/psadil/post/staircases-for-thresholds/index_files/figure-html/psychometric-1.png" alt="A psychometric curve with threshold intensity. Psychometric curves relate the intensity of stimulation to the perception of stimulation, or the proportion positive responses. There are many parameterizations of these functions, but they are typically sigmoidal. The intensity at which, on average, half of responses are positive is often of interest. This intensity is called the threshold." width="672" />
&lt;p class="caption">
Figure 1: A psychometric curve with threshold intensity. Psychometric curves relate the intensity of stimulation to the perception of stimulation, or the proportion positive responses. There are many parameterizations of these functions, but they are typically sigmoidal. The intensity at which, on average, half of responses are positive is often of interest. This intensity is called the threshold.
&lt;/p>
&lt;/div>
&lt;p>To infer psychometric functions, standard procedures exist, though these procedures have varied efficiency. In particular, when only a single stimulus intensity is required, it would be inefficient to estimate the entire function. Consider a researcher attempting to elicit half positive responses, behavior elicited by the so called threshold stimulus intensity. A simple procedure to infer the psychometric function involves presenting a wide range of stimulus intensities, fitting the function to the data, and using the estimated function to infer the threshold. Each datum increases the precision of the estimate, but some data will be more useful than others. Intensities close to the tails of the function will pin down the function at those tails, but functions with different thresholds can behave similarly in their tails (Figure &lt;a href="#fig:psychometric2">2&lt;/a>). The threshold is most tightly constrained by responses to stimuli at the threshold &lt;span class="citation">(&lt;a href="#ref-levitt1997" role="doc-biblioref">Levitt 1971&lt;/a>)&lt;/span>. Therefore, an ideal procedure to estimate the threshold intensity would involve repeatedly presenting the threshold intensity. The ideal procedure is unfeasible, since if the threshold were known there would be no need for inference. But although the exact threshold intensity cannot be presented on every trial, certain procedures enable most trials to approximate the ideal.&lt;/p>
&lt;div class="figure">&lt;span id="fig:psychometric2">&lt;/span>
&lt;img src="https://psadil.github.io/psadil/post/staircases-for-thresholds/index_files/figure-html/psychometric2-1.png" alt="Psychometric functions with different thresholds. Although behavior at the tails of these functions are similar, they have different thresholds." width="672" />
&lt;p class="caption">
Figure 2: Psychometric functions with different thresholds. Although behavior at the tails of these functions are similar, they have different thresholds.
&lt;/p>
&lt;/div>
&lt;p>One type of procedure that locates the threshold intensity, both simply and efficiently, is called a staircase. A staircase procedure changes the stimulus intensity on every trial based on how participants respond. A staircase that locates the threshold increases stimulus intensity after a participant makes a negative response and decreases the intensity after a participant responds positively. Even when the first stimulus has an intensity far from the threshold (Figure &lt;a href="#fig:staircase">3&lt;/a>), the staircase brings the intensity to the threshold, a convergence that is ensured by the psychometric function. For example, when intensity is lower than the threshold, a participant tends to make negative responses. After a negative response, the contrast is increased. With an increased contrast, the participant will be more likely to make a positive response. If the intensity is still lower then then threshold, the participant will likely provide another negative response, causing the intensity increase further. After enough trials with intensity too low, the intensity will be pushed towards the threshold. If the intensity strays from the threshold, the same dynamics push the threshold back to threshold. The staircase forces the intensity to remain close to the threshold.&lt;/p>
&lt;div class="figure">&lt;span id="fig:staircase">&lt;/span>
&lt;img src="https://psadil.github.io/psadil/post/staircases-for-thresholds/index_files/figure-html/staircase-1.png" alt="Example sequence of trials in which stimulus intensity is controlled by a staircase procedure. After each negative response, the intensity increases, and after each positive response the intensity decreases. Although the initial intensity was much lower than the threshold, the staircase brings the intensity close to threshold and then keeps it there." width="672" />
&lt;p class="caption">
Figure 3: Example sequence of trials in which stimulus intensity is controlled by a staircase procedure. After each negative response, the intensity increases, and after each positive response the intensity decreases. Although the initial intensity was much lower than the threshold, the staircase brings the intensity close to threshold and then keeps it there.
&lt;/p>
&lt;/div>
&lt;div id="references" class="section level1 unnumbered">
&lt;h1>References&lt;/h1>
&lt;div id="refs" class="references csl-bib-body hanging-indent">
&lt;div id="ref-levitt1997" class="csl-entry">
Levitt, H. 1971. &lt;span>“Transformed up-down Methods in Psychoacoustics.”&lt;/span> &lt;em>The Journal of the Acoustical Society of America&lt;/em> 49 (2B): 467–77.
&lt;/div>
&lt;/div>
&lt;/div></description></item><item><title>Forward encoding model</title><link>https://psadil.github.io/psadil/post/forward-encoding-model/</link><pubDate>Thu, 24 Oct 2019 00:00:00 +0000</pubDate><guid>https://psadil.github.io/psadil/post/forward-encoding-model/</guid><description>
&lt;script src="https://psadil.github.io/psadil/post/forward-encoding-model/index_files/header-attrs/header-attrs.js">&lt;/script>
&lt;p>Functional magnetic resonance imaging records brain activity with spatially distinct voxels, but this segmentation will be misaligned with a brain’s meaningful boundaries. The segmentation results in some voxels recording activity from different types of tissue – types that are both neural an non-neural – but even voxels that exclusively sample gray matter can span functionally distinct cortex. For example, a 3T scanner allows voxels in the range of 1.5-3 mm&lt;span class="math inline">\(^3\)&lt;/span>, but orientation columns have an average width of 0.8 mm &lt;span class="citation">(&lt;a href="#ref-yacoub2008" role="doc-biblioref">Yacoub, Harel, and Uğurbil 2008&lt;/a>)&lt;/span>. Studying orientation columns with such low resolution requires statistical tools.&lt;/p>
&lt;p>One statistical tool models voxel activity as a linear combination of the activity of a small number of neural channels &lt;span class="citation">(&lt;a href="#ref-brouwer2009" role="doc-biblioref">Brouwer and Heeger 2009&lt;/a>; &lt;a href="#ref-kay2008" role="doc-biblioref">Kay et al. 2008&lt;/a>)&lt;/span>. These models are called forward models, describing how the channel activity transforms into voxel activity. In early sensory cortex, the channels are analogous to cortical columns. In later cortex, the channels are more abstract dimensions of a representational space. Developing a forward model requires assuming not only how many channels contribute of a voxel’s activity, but also the tuning properties of those channels. With these assumptions, regression allows inferring the contribution of each channel to each voxel’s activity. Let &lt;span class="math inline">\(N\)&lt;/span> be the number of observations for each voxel, &lt;span class="math inline">\(M\)&lt;/span> be the number of voxels, and &lt;span class="math inline">\(K\)&lt;/span> be the number of channels within a voxel. The forward model specifies that the data (&lt;span class="math inline">\(B\)&lt;/span>, &lt;span class="math inline">\(M \times N\)&lt;/span>) result from a weighted combination of the assumed channels responses (&lt;span class="math inline">\(C\)&lt;/span>, &lt;span class="math inline">\(K \times N\)&lt;/span>), where the weights (&lt;span class="math inline">\(W\)&lt;/span>, &lt;span class="math inline">\(M \times K\)&lt;/span>) are unknown.&lt;/p>
&lt;p>&lt;span class="math display">\[
B = WC
\]&lt;/span>&lt;/p>
&lt;p>Taking the pseudoinverse of the channel matrix and multiplying the result by the data gives an estimate of the weight matrix:&lt;/p>
&lt;p>&lt;span class="math display">\[
\widehat{W} = BC^T(CC^T)^{-1}
\]&lt;/span>&lt;/p>
&lt;p>Assumptions about &lt;span class="math inline">\(C\)&lt;/span> are assumptions about how the channels encode stimuli. Different encoding schemes can be instantiated with different &lt;span class="math inline">\(C\)&lt;/span>, and any method for comparing linear models could be used to compare the schemes.&lt;/p>
&lt;p>The forward encoding model enables comparison of static encoding schemes, but neural encoding schemes are dynamic. Attentional fluctuations, perceptual learning, and stimulation history all modulate neural tuning functions &lt;span class="citation">(&lt;a href="#ref-mcadams1999" role="doc-biblioref">McAdams and Maunsell 1999&lt;/a>; &lt;a href="#ref-reynolds2000" role="doc-biblioref">Reynolds, Pasternak, and Desimone 2000&lt;/a>; &lt;a href="#ref-siegel2015" role="doc-biblioref">Siegel, Buschman, and Miller 2015&lt;/a>; &lt;a href="#ref-yang2004" role="doc-biblioref">Yang and Maunsell 2004&lt;/a>)&lt;/span>. To explore modulations with functional magnetic resonance imaging, some researchers have inverted the encoding model &lt;span class="citation">(&lt;a href="#ref-garcia2013" role="doc-biblioref">Garcia, Srinivasan, and Serences 2013&lt;/a>; &lt;a href="#ref-rahmati2018" role="doc-biblioref">Rahmati, Saber, and Curtis 2018&lt;/a>; &lt;a href="#ref-saproo2014" role="doc-biblioref">Saproo and Serences 2014&lt;/a>; &lt;a href="#ref-scolari2012" role="doc-biblioref">Scolari, Byers, and Serences 2012&lt;/a>; &lt;a href="#ref-sprague2013" role="doc-biblioref">Sprague and Serences 2013&lt;/a>; &lt;a href="#ref-vo2017" role="doc-biblioref">Vo, Sprague, and Serences 2017&lt;/a>)&lt;/span>. The inversion is a variation of cross validation. The method estimates the weight matrix with only some of the data (e.g., all data excluding a single run). The held out data, &lt;span class="math inline">\(B_H\)&lt;/span>, contains observations from all experimental condition across which the tuning functions might vary. The encoding model is inverted by multiplying the pseudoinverse of the weight matrix with the held out data to estimate a new channel response matrix.&lt;/p>
&lt;p>&lt;span class="math display">\[
\widehat{C} = \widehat{W}^T(\widehat{W}\widehat{W}^T)^{-1}B_H
\]&lt;/span>&lt;/p>
&lt;p>The new channel response matrix estimates how the channels respond in each experimental condition.&lt;/p>
&lt;p>Although validation studies demonstrated that the inverted encoding model enables inferences that recapitulate some modulations observed with electrophysiology &lt;span class="citation">(&lt;a href="#ref-sprague2018" role="doc-biblioref">Sprague et al. 2018&lt;/a>; &lt;a href="#ref-sprague2015" role="doc-biblioref">Sprague, Saproo, and Serences 2015&lt;/a>)&lt;/span>, the inversion also misleads inferences about certain fundamental modulations &lt;span class="citation">(&lt;a href="#ref-gardner2019" role="doc-biblioref">Gardner and Liu 2019&lt;/a>; &lt;a href="#ref-liu2018" role="doc-biblioref">Liu, Cable, and Gardner 2018&lt;/a>)&lt;/span>. In particular, increasing the contrast of an orientation increases the gain of neurons tuned to orientation without altering their tuning bandwidth &lt;span class="citation">(&lt;a href="#ref-alitto2004" role="doc-biblioref">Alitto and Usrey 2004&lt;/a>; &lt;a href="#ref-sclar1982" role="doc-biblioref">Sclar and Freeman 1982&lt;/a>; &lt;a href="#ref-skottun1987" role="doc-biblioref">Skottun et al. 1987&lt;/a>)&lt;/span>, but the inverted encoding model (incorrectly) suggests that higher contrast decreases bandwidth &lt;span class="citation">(&lt;a href="#ref-liu2018" role="doc-biblioref">Liu, Cable, and Gardner 2018&lt;/a>)&lt;/span>. Inferences are misled because the estimated channel responses are constrained by the initial assumptions about &lt;span class="math inline">\(C\)&lt;/span> &lt;span class="citation">(&lt;a href="#ref-gardner2019" role="doc-biblioref">Gardner and Liu 2019&lt;/a>)&lt;/span>. Using the encoding model to study modulations requires a way to estimate the contribution of each channel without assuming a fixed channel response function.&lt;/p>
&lt;div id="references" class="section level1 unnumbered">
&lt;h1>References&lt;/h1>
&lt;div id="refs" class="references csl-bib-body hanging-indent">
&lt;div id="ref-alitto2004" class="csl-entry">
Alitto, Henry J, and W Martin Usrey. 2004. &lt;span>“Influence of Contrast on Orientation and Temporal Frequency Tuning in Ferret Primary Visual Cortex.”&lt;/span> &lt;em>Journal of Neurophysiology&lt;/em> 91 (6): 2797–2808.
&lt;/div>
&lt;div id="ref-brouwer2009" class="csl-entry">
Brouwer, Gijs Joost, and David J Heeger. 2009. &lt;span>“Decoding and Reconstructing Color from Responses in Human Visual Cortex.”&lt;/span> &lt;em>Journal of Neuroscience&lt;/em> 29 (44): 13992–4003.
&lt;/div>
&lt;div id="ref-garcia2013" class="csl-entry">
Garcia, Javier O, Ramesh Srinivasan, and John T Serences. 2013. &lt;span>“Near-Real-Time Feature-Selective Modulations in Human Cortex.”&lt;/span> &lt;em>Current Biology&lt;/em> 23 (6): 515–22.
&lt;/div>
&lt;div id="ref-gardner2019" class="csl-entry">
Gardner, Justin L, and Taosheng Liu. 2019. &lt;span>“Inverted Encoding Models Reconstruct an Arbitrary Model Response, Not the Stimulus.”&lt;/span> &lt;em>eNeuro&lt;/em> 6 (2).
&lt;/div>
&lt;div id="ref-kay2008" class="csl-entry">
Kay, Kendrick N, Thomas Naselaris, Ryan J Prenger, and Jack L Gallant. 2008. &lt;span>“Identifying Natural Images from Human Brain Activity.”&lt;/span> &lt;em>Nature&lt;/em> 452 (7185): 352.
&lt;/div>
&lt;div id="ref-liu2018" class="csl-entry">
Liu, Taosheng, Dylan Cable, and Justin L Gardner. 2018. &lt;span>“Inverted Encoding Models of Human Population Response Conflate Noise and Neural Tuning Width.”&lt;/span> &lt;em>Journal of Neuroscience&lt;/em> 38 (2): 398–408.
&lt;/div>
&lt;div id="ref-mcadams1999" class="csl-entry">
McAdams, Carrie J, and John HR Maunsell. 1999. &lt;span>“Effects of Attention on Orientation-Tuning Functions of Single Neurons in Macaque Cortical Area V4.”&lt;/span> &lt;em>Journal of Neuroscience&lt;/em> 19 (1): 431–41.
&lt;/div>
&lt;div id="ref-rahmati2018" class="csl-entry">
Rahmati, Masih, Golbarg T Saber, and Clayton E Curtis. 2018. &lt;span>“Population Dynamics of Early Visual Cortex During Working Memory.”&lt;/span> &lt;em>Journal of Cognitive Neuroscience&lt;/em> 30 (2): 219–33.
&lt;/div>
&lt;div id="ref-reynolds2000" class="csl-entry">
Reynolds, John H, Tatiana Pasternak, and Robert Desimone. 2000. &lt;span>“Attention Increases Sensitivity of V4 Neurons.”&lt;/span> &lt;em>Neuron&lt;/em> 26 (3): 703–14.
&lt;/div>
&lt;div id="ref-saproo2014" class="csl-entry">
Saproo, Sameer, and John T Serences. 2014. &lt;span>“Attention Improves Transfer of Motion Information Between V1 and MT.”&lt;/span> &lt;em>Journal of Neuroscience&lt;/em> 34 (10): 3586–96.
&lt;/div>
&lt;div id="ref-sclar1982" class="csl-entry">
Sclar, G, and RD Freeman. 1982. &lt;span>“Orientation Selectivity in the Cat’s Striate Cortex Is Invariant with Stimulus Contrast.”&lt;/span> &lt;em>Experimental Brain Research&lt;/em> 46 (3): 457–61.
&lt;/div>
&lt;div id="ref-scolari2012" class="csl-entry">
Scolari, Miranda, Anna Byers, and John T Serences. 2012. &lt;span>“Optimal Deployment of Attentional Gain During Fine Discriminations.”&lt;/span> &lt;em>Journal of Neuroscience&lt;/em> 32 (22): 7723–33.
&lt;/div>
&lt;div id="ref-siegel2015" class="csl-entry">
Siegel, Markus, Timothy J Buschman, and Earl K Miller. 2015. &lt;span>“Cortical Information Flow During Flexible Sensorimotor Decisions.”&lt;/span> &lt;em>Science&lt;/em> 348 (6241): 1352–55.
&lt;/div>
&lt;div id="ref-skottun1987" class="csl-entry">
Skottun, Bernt C, Arthur Bradley, Gary Sclar, Izumi Ohzawa, and Ralph D Freeman. 1987. &lt;span>“The Effects of Contrast on Visual Orientation and Spatial Frequency Discrimination: A Comparison of Single Cells and Behavior.”&lt;/span> &lt;em>Journal of Neurophysiology&lt;/em> 57 (3): 773–86.
&lt;/div>
&lt;div id="ref-sprague2018" class="csl-entry">
Sprague, Thomas C, Kirsten CS Adam, Joshua J Foster, Masih Rahmati, David W Sutterer, and Vy A Vo. 2018. &lt;span>“Inverted Encoding Models Assay Population-Level Stimulus Representations, Not Single-Unit Neural Tuning.”&lt;/span> &lt;em>eNeuro&lt;/em> 5 (3).
&lt;/div>
&lt;div id="ref-sprague2015" class="csl-entry">
Sprague, Thomas C, Sameer Saproo, and John T Serences. 2015. &lt;span>“Visual Attention Mitigates Information Loss in Small-and Large-Scale Neural Codes.”&lt;/span> &lt;em>Trends in Cognitive Sciences&lt;/em> 19 (4): 215–26.
&lt;/div>
&lt;div id="ref-sprague2013" class="csl-entry">
Sprague, Thomas C, and John T Serences. 2013. &lt;span>“Attention Modulates Spatial Priority Maps in the Human Occipital, Parietal and Frontal Cortices.”&lt;/span> &lt;em>Nature Neuroscience&lt;/em> 16 (12): 1879.
&lt;/div>
&lt;div id="ref-vo2017" class="csl-entry">
Vo, Vy A, Thomas C Sprague, and John T Serences. 2017. &lt;span>“Spatial Tuning Shifts Increase the Discriminability and Fidelity of Population Codes in Visual Cortex.”&lt;/span> &lt;em>Journal of Neuroscience&lt;/em> 37 (12): 3386–3401.
&lt;/div>
&lt;div id="ref-yacoub2008" class="csl-entry">
Yacoub, Essa, Noam Harel, and Kâmil Uğurbil. 2008. &lt;span>“High-Field fMRI Unveils Orientation Columns in Humans.”&lt;/span> &lt;em>Proceedings of the National Academy of Sciences&lt;/em> 105 (30): 10607–12.
&lt;/div>
&lt;div id="ref-yang2004" class="csl-entry">
Yang, Tianming, and John HR Maunsell. 2004. &lt;span>“The Effect of Perceptual Learning on Neuronal Responses in Monkey Visual Area V4.”&lt;/span> &lt;em>Journal of Neuroscience&lt;/em> 24 (7): 1617–26.
&lt;/div>
&lt;/div>
&lt;/div></description></item><item><title>Modulations to tuning functions can bias evidence accumulation</title><link>https://psadil.github.io/psadil/post/neural-modulation-for-serial-dependence/</link><pubDate>Fri, 18 Oct 2019 00:00:00 +0000</pubDate><guid>https://psadil.github.io/psadil/post/neural-modulation-for-serial-dependence/</guid><description>
&lt;script src="https://psadil.github.io/psadil/post/neural-modulation-for-serial-dependence/index_files/header-attrs/header-attrs.js">&lt;/script>
&lt;p>Perceptual decisions can be deconstructed with evidence accumulation models. These models formalize expectations about how participants behave, when that behavior involves repeatedly sampling information towards until surpassing a necessary threshold of information. At a cognitive level, the different models instantiate the components differently, but at a neural level the models rely on common mechanisms. To accumulate evidence the models assume two distinct populations of neurons. One population responds to available information. This population can be thought of as a sensory population, such that each neuron in the population represents one of the available options. The second population listens to the first, transforming the sensory activity into evidence for each decision and accumulating the evidence through time. This second population can be called an integrating population. While the location of the sensory population depends on the information that needs to be represented, the location of the integrating population depends on the required behavior. If participants must make decisions about orientations, the sensory population might be striatal neurons tuned to different orientations. If participants make decisions with saccades, the integrating population might be in the frontal eye fields. Understanding how these two populations reveals different ways that decisions can be biased.&lt;/p>
&lt;div class="figure">&lt;span id="fig:readout">&lt;/span>
&lt;img src="https://psadil.github.io/psadil/post/neural-modulation-for-serial-dependence/index_files/figure-html/readout-1.png" alt="Sensory channels evenly represent orientations. The curves from the sensory population represent the average activity of each neuron to a given orientation. Although the curves show average activity, at any given moment the actual activity of each neuron may be higher or lower. The integrating curve reflects the average evidence that the integrating population will record. The flatness of the integrating curve reflects an unbiased representation. Only eight neurons from the sensory population are shown to avoid overcrowding." width="672" />
&lt;p class="caption">
Figure 1: Sensory channels evenly represent orientations. The curves from the sensory population represent the average activity of each neuron to a given orientation. Although the curves show average activity, at any given moment the actual activity of each neuron may be higher or lower. The integrating curve reflects the average evidence that the integrating population will record. The flatness of the integrating curve reflects an unbiased representation. Only eight neurons from the sensory population are shown to avoid overcrowding.
&lt;/p>
&lt;/div>
&lt;p>For the integrating population to accumulate evidence, it must transform the activity of the sensory population into a meaningful signal. Figure &lt;a href="#fig:readout">1&lt;/a> depicts that transformation when participants must report orientations. Each neuron in the sensory population responds most strongly to a specific orientation, but all of them are active whenever an orientation is present. The function describing how a sensory neuron respond to different orientations is called the neuron’s tuning function. The integrating population will respond according to some other function of that sensory activity. One simple integrating function associates each neuron with its preferred orientation; the integrating population then tallies evidence based on whichever neuron is most active. This function requires the sensory population to represent each orientation with at least one neuron. If there are enough sensory neurons, the integrating population will be able to accumulate evidence for each orientation without bias.&lt;/p>
&lt;div class="figure">&lt;span id="fig:modulations">&lt;/span>
&lt;img src="https://psadil.github.io/psadil/post/neural-modulation-for-serial-dependence/index_files/figure-html/modulations-1.png" alt="Modulations to activity of the sensory population will provide the integrating population a biased representation of the orientation. Unlike in Figure 1, the sensory population in both panels provides the integrating population with an uneven representation of orientation." width="672" />
&lt;p class="caption">
Figure 2: Modulations to activity of the sensory population will provide the integrating population a biased representation of the orientation. Unlike in Figure 1, the sensory population in both panels provides the integrating population with an uneven representation of orientation.
&lt;/p>
&lt;/div>
&lt;p>The tuning characteristics of sensory neurons are variable, and this variability causes biases to emerge in the evidence accumulation process (Figure &lt;a href="#fig:modulations">2&lt;/a>). One common alteration is an increased gain, whereby the tuning function is multiplied by some value. When the gains of tuning functions are altered heterogeneously, a neuron may have a higher activity even when the orientation it is responsible for is not present. The neurons with the highest gain will bias the evidence gathered by the integrating population. Alternatively, tuning functions might shift, along with the orientation each neuron signals. The shift causes the sensory population to over-represent of some orientations and leave others underrepresented. These alterations can provide advantages in certain circumstances. For example, a heterogeneously increased gain will be useful when some orientations are known to be more likely than others, and a shift will be useful when different orientations require differently precise responses. But to accumulate evidence without bias, the sensory population must restore more uniform tuning.&lt;/p></description></item><item><title>serial dependence reflects a preference for low variability</title><link>https://psadil.github.io/psadil/post/serial-dependence-reflects-a-preference-for-low-variability/</link><pubDate>Fri, 11 Oct 2019 00:00:00 +0000</pubDate><guid>https://psadil.github.io/psadil/post/serial-dependence-reflects-a-preference-for-low-variability/</guid><description>
&lt;script src="https://psadil.github.io/psadil/post/serial-dependence-reflects-a-preference-for-low-variability/index_files/header-attrs/header-attrs.js">&lt;/script>
&lt;p>One framework for understanding perception casts it as inference: just as a statistician uncovers noisy data to uncover patterns, an organism perceives when it converts sensations into guesses about its environment. The framework not concrete enough to be called a theory of perception, since it is not clear what data could falsify it&lt;a href="#fn1" class="footnote-ref" id="fnref1">&lt;sup>1&lt;/sup>&lt;/a>. But the framework can remind perceptual researchers about the many strategies available for modeling the world. Do perceiving organisms employ similar strategies?&lt;/p>
&lt;p>One property that distinguishes many statistical strategies is a tradeoff between bias and variability Consider this tradeoff with an example. A statistician must estimate the average height of college-level soccer players. The true average could be uncovered by measuring the height of every player at every college–the statistician would not need inference. But the statistician is constrained by limited resources. They can only measure the players from a single college, though they may measure the heights of any student at the college. The statistician must now decide between an unbiased but variable or biased but precise strategy. Measuring only the soccer players gives an unbiased estimate, but with so few players the team’s average may be far from the true average. The statistician cannot be confident that the single team resembles all teams. Alternatively, the statistician may supplement their estimate with the heights of players from another, related sport. Since ultimate frisbee players may have similar heights to soccer players, incorporating their heights into the estimate may counteract any anomalously sized soccer players. However, incorporating even a single player from another sport biases the estimate, in the sense that the average height of all soccer players will not equal the average height of all soccer players and the one ultimate player&lt;a href="#fn2" class="footnote-ref" id="fnref2">&lt;sup>2&lt;/sup>&lt;/a>. One strategy – measure only soccer players – would give the true answer if there were enough resources, but the other strategy – measure everyone that is similar to a soccer player – may approximate the truth well with limited resources.&lt;/p>
&lt;p>The bias-variability tradeoff gives a functional interpretation to the perceptual effect called serial dependence &lt;span class="citation">(&lt;a href="#ref-fischer2014" role="doc-biblioref">Fischer and Whitney 2014&lt;/a>; &lt;a href="#ref-cicchini2018" role="doc-biblioref">Cicchini, Mikellidou, and Burr 2018&lt;/a>)&lt;/span>. Serial dependence occurs when participants judge perceptual stimuli across many trials, and their judgments on one trial depend on their immediately preceding judgment. Like the constrained statistician, participants may not process each stimulus completely: participants only see stimuli for brief durations, their judgments are made after the stimuli are masked, and their attention fluctuates throughout the hundreds of trials. The bias is often attractive, meaning that participants’ judgments reflect a blending of the stimuli on the current and previous trials. Serial dependence may reflect a strategy – not necessarily intentional – that reduces variability across judgments by combining information. Although the strategy biases the judgments, it may help each individual estimate approach truth.&lt;/p>
&lt;div id="references" class="section level1 unnumbered">
&lt;h1>References&lt;/h1>
&lt;div id="refs" class="references csl-bib-body hanging-indent">
&lt;div id="ref-cicchini2018" class="csl-entry">
Cicchini, G. M., K. Mikellidou, and D. C Burr. 2018. &lt;span>“The Functional Role of Serial Dependence.”&lt;/span> &lt;em>Proceedings of the Royal Society B&lt;/em> 285 (1890): 20181722.
&lt;/div>
&lt;div id="ref-fischer2014" class="csl-entry">
Fischer, Jason, and David Whitney. 2014. &lt;span>“Serial Dependence in Visual Perception.”&lt;/span> &lt;em>Nature Neuroscience&lt;/em> 17 (5): 738.
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="footnotes">
&lt;hr />
&lt;ol>
&lt;li id="fn1">&lt;p>A perceptual system does need to be limited to the statistical tools that have already been developed, so even a demonstration that organisms don’t employ any known statistical tool would not rule out the framework.&lt;a href="#fnref1" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn2">&lt;p>Assume that the ultimate player is not as tall as the average soccer player&lt;a href="#fnref2" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>derivative of gaussian for serial dependence</title><link>https://psadil.github.io/psadil/post/derivative-of-gaussian-for-serial-dependence/</link><pubDate>Thu, 03 Oct 2019 00:00:00 +0000</pubDate><guid>https://psadil.github.io/psadil/post/derivative-of-gaussian-for-serial-dependence/</guid><description>
&lt;script src="https://psadil.github.io/psadil/post/derivative-of-gaussian-for-serial-dependence/index_files/header-attrs/header-attrs.js">&lt;/script>
&lt;p>Cognitive experiments can require participants to complete hundreds of trials, but completing so many trials invariably alters participants’ behavior. Their behavior late in the experiment can depend on their behavior early in the experiment. Although such dependence can be an experimental confound,
the dependence itself can provide clues about cognition. One simple kind of dependence occurs through learning; hundreds of trials provides participants ample practice. A more subtle dependence can emerge between sequential trials, an effect called serial dependence. Theoretical interpretations of serial dependence vary, and some of that variability may relate to how the dependence is measured. In this post, I review a statistical method commonly used to analyze serial dependence and discuss one way that method can fail.&lt;/p>
&lt;p>I will focus on the analysis of an orientation judgment task, in which participants simply see an oriented bar on each trial, remember the bar’s orientation for a short period, and then report the orientation. Participants’ responses on one trial can depend on the orientation they saw in the previous trial. The dependence follows a Gaussian’s derivative function. Figure &lt;a href="#fig:dog0">1&lt;/a>A shows a Gaussian function with its derivative, and Figure &lt;a href="#fig:dog0">1&lt;/a>B shows the derivative modeling a range of different serial dependence patterns. The derivative captures three key features of the data. First, different changes in orientation between trials result in serial dependencies of different magnitude. The responsiveness of dependence is captured by the width of the derivative. Second, serial dependence can have a different magnitude. The magnitude is captured by the amplitude of the derivative. Finally, responses on the current trial can either be attracted towards or repulsed away from the orientation of the previous trial. The direction of the effect is captured with the sign of the amplitude. The direction of the effect–and the experimental manipulations that change that direction–are often critical to different theoretical interpretations of serial dependence.&lt;/p>
&lt;div class="figure">&lt;span id="fig:dog0">&lt;/span>
&lt;img src="https://psadil.github.io/psadil/post/derivative-of-gaussian-for-serial-dependence/index_files/figure-html/dog0-1.png" alt="A) A Gaussian function and its derivative. B) The derivative captures how errors on the current trial can depend on how the relationship between the orientation seen in the current and previous trials. Positive values on the horizontal axis signify a clockwise difference and negative values a counterclockwise difference. Likewise, positive errors signify responses on the current trial which were clockwise to the true orientation, and negative errors are counterclockwise. When errors are in the same direction as the difference in orientations, the error is said to be attractive. Otherwise, the error is repulsive. Whether errors are attractive or repulsive is given by the sign of the derivative's amplitude." width="672" />
&lt;p class="caption">
Figure 1: A) A Gaussian function and its derivative. B) The derivative captures how errors on the current trial can depend on how the relationship between the orientation seen in the current and previous trials. Positive values on the horizontal axis signify a clockwise difference and negative values a counterclockwise difference. Likewise, positive errors signify responses on the current trial which were clockwise to the true orientation, and negative errors are counterclockwise. When errors are in the same direction as the difference in orientations, the error is said to be attractive. Otherwise, the error is repulsive. Whether errors are attractive or repulsive is given by the sign of the derivative’s amplitude.
&lt;/p>
&lt;/div>
&lt;p>Although the Gaussian’s derivative adequately models the serial dependence between trials with similar orientations (less than 45 degree differences), the derivative fits poorly the dependencies following large changes. When sequential trials have a large orientation difference, the sign of the dependence often changes; small orientation differences can elicit an attractive dependence even while large differences are repulsive. These sign flips are called the peripheral bumps, and they are not captured by the Gaussian’s derivative. If the bumps are large enough, they can interpretations about the sign to of dependencies following small changes can be inverted (Figure &lt;a href="#fig:bumps">2&lt;/a>). Unfortunately, noticing the peripheral bumps can be hard with sparse data. But even with sparse data, the width of the best-fitting derivative can help identify bumps. If the best-fitting derivative is abnormally wide (with peaks larger than approximately 35 degrees), then the derivative is tracking dependencies wider than it should. In that circumstance, it may be best to focus analyses on only the trials with smaller orientation differences.&lt;/p>
&lt;div class="figure">&lt;span id="fig:bumps">&lt;/span>
&lt;img src="https://psadil.github.io/psadil/post/derivative-of-gaussian-for-serial-dependence/index_files/figure-html/bumps-1.png" alt="Misfits of the Gaussian's derivative. The dots give hypothetical data. The data were generated with a function whose average is traced by the dashed line. The data were fit with a derivative of Guassian function, and the best-fitting derivative is shown with a solid line. The derivative does not match the data-generating function." width="672" />
&lt;p class="caption">
Figure 2: Misfits of the Gaussian’s derivative. The dots give hypothetical data. The data were generated with a function whose average is traced by the dashed line. The data were fit with a derivative of Guassian function, and the best-fitting derivative is shown with a solid line. The derivative does not match the data-generating function.
&lt;/p>
&lt;/div></description></item><item><title>an overview of population receptive field mapping</title><link>https://psadil.github.io/psadil/post/population-receptive-field-mapping/</link><pubDate>Fri, 27 Sep 2019 00:00:00 +0000</pubDate><guid>https://psadil.github.io/psadil/post/population-receptive-field-mapping/</guid><description>
&lt;script src="https://psadil.github.io/psadil/post/population-receptive-field-mapping/index.en_files/header-attrs/header-attrs.js">&lt;/script>
&lt;p>Perceiving the world requires representing the world in neural tissue. A neuron is &lt;em>tuned&lt;/em> to perceivable information when different values of that information cause the neuron to fire at a different rate. For example, most visual neurons are tuned to spatial location. The spatial tuning could be measured by placing a recording electrode in a neuron in a macaque’s visual cortex while the macaque fixated on the center of a computer monitor and a picture moved across that monitor. The electrode would report higher activity only when the picture was in certain parts of the macaque’s visual field. The function relating the position of the picture to the neuron’s activity is the tuning function. Such functions often resembles a bivariate Gaussian (Figure &lt;a href="#fig:prf">1&lt;/a>). To study these tuning functions is to study how these neurons represent the world.&lt;/p>
&lt;div class="figure">&lt;span id="fig:prf">&lt;/span>
&lt;img src="https://psadil.github.io/psadil/post/population-receptive-field-mapping/index.en_files/figure-html/prf-1.png" alt="The bivariate Gaussian represents a neuron's receptive field. The neuron will be most responsive to information that overlaps with the bright yellow regions. Since this the upper left portion of the box is slightly encompassed by the receptive field, the neuron' might fire slightly more rapidly as compared to its baseline rate." width="672" />
&lt;p class="caption">
Figure 1: The bivariate Gaussian represents a neuron’s receptive field. The neuron will be most responsive to information that overlaps with the bright yellow regions. Since this the upper left portion of the box is slightly encompassed by the receptive field, the neuron’ might fire slightly more rapidly as compared to its baseline rate.
&lt;/p>
&lt;/div>
&lt;p>Sensory neurons are tuned to many other features such as orientation, color, pitch, direction of motion. Most neurons tuned to one visual feature are also tuned to spatial location, so understanding a neuron’s spatial can facilitate understanding its other sensitivities&lt;a href="#fn1" class="footnote-ref" id="fnref1">&lt;sup>1&lt;/sup>&lt;/a>. The tuning functions of neurons even have a special name, their receptive field. However, it is often unfeasible to record from individual neurons in humans, and instead only non-invasive neuroimaging methods are available. But these non-invasive methods have low spatial resolution. Even the relatively well spatially resolved technique of functional magnetic resonance imaging reflects the aggregated activity of 10e5 - 10e6 neurons.&lt;/p>
&lt;p>Fortunately, the &lt;em>retinotopic&lt;/em> arrangement of visual neurons facilitates relating the spatial tuning of a voxel&lt;a href="#fn2" class="footnote-ref" id="fnref2">&lt;sup>2&lt;/sup>&lt;/a> to the receptive field of individual neurons. A retinotopic arrangement means that neighboring neurons are tuned to neighboring locations in the visual field; for example, neurons tuned to things in the fovea cluster together and neurons tuned to peripheral locations surround that cluster. This retinotopic arrangement implies that all neurons sampled by a voxel represent nearby regions in the visual environment. Referring to the neurons in a voxel as a population, the receptive field of a voxel is called a population receptive field. Studying population receptive fields alone cannot reveal how individual neurons contribute to a coherent perceptual experience, but studying them can reveal how the populations respond as a group.&lt;/p>
&lt;p>To chart out all of the mountainous population receptive fields in visual cortex is called population receptive field mapping &lt;span class="citation">(&lt;a href="#ref-dumoulin2008" role="doc-biblioref">Dumoulin and Wandell 2008&lt;/a>)&lt;/span>. The receptive fields can be mapped by recording the activity of each voxel while a human participant is shown some visually salient movie. A mathematical model – such as a bivariate Gaussian – of the receptive field is assumed, and the data from each voxel are used to fit the parameters of that model. The specific images that are used will depend on which part of visual cortex is the focus of the experiment. A counterphasing black and white checkerboard might be close to optimal for primary visual cortex, but the checkerboard would only weakly stimulate neurons in higher level visual regions. To stimulate most of visual cortex, other researchers rely on &lt;a href="https://kendrickkay.net/analyzePRF/">more varied displays&lt;/a>.&lt;/p>
&lt;div id="references" class="section level1 unnumbered">
&lt;h1>References&lt;/h1>
&lt;div id="refs" class="references csl-bib-body hanging-indent">
&lt;div id="ref-dumoulin2008" class="csl-entry">
Dumoulin, Serge O, and Brian A Wandell. 2008. &lt;span>“Population Receptive Field Estimates in Human Visual Cortex.”&lt;/span> &lt;em>Neuroimage&lt;/em> 39 (2): 647–60.
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="footnotes">
&lt;hr />
&lt;ol>
&lt;li id="fn1">&lt;p>e.g., if you want to understand how a neuron is tuned to color, it helps to know where to put the color&lt;a href="#fnref1" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn2">&lt;p> &lt;em>Voxels&lt;/em> are the elements that hold data in magnetic resonance imaging. A voxel in a 3D image is analogous to a pixel in a 2D image; a voxel is a pixel with a volume.&lt;a href="#fnref2" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>Serial Dependence</title><link>https://psadil.github.io/psadil/post/serial-dependence/</link><pubDate>Thu, 19 Sep 2019 00:00:00 +0000</pubDate><guid>https://psadil.github.io/psadil/post/serial-dependence/</guid><description>
&lt;script src="https://psadil.github.io/psadil/post/serial-dependence/index.en_files/header-attrs/header-attrs.js">&lt;/script>
&lt;p>Objects in the visual environment move suddenly and erratically, and visual perception must be sensitive to the changes that are important. But each saccade and head tilt change the image imprinted on the retina, and to perceive every tremor ignores the stability of the visual environment; a desk will still look like a desk in a few seconds. The visual system must therefore balance the ability to detect subtle changes in the environment against the efficiency afforded by accurate predictions.&lt;/p>
&lt;p>That the recent past influences current perception can be demonstrated easily. If you stare at Figure &lt;a href="#fig:tae">1&lt;/a>, you might observe that the Gabor has a bend immediately after changing orientations. The bend lasts for a moment, then straightens. But the bend is an illusion. While tracking Figure &lt;a href="#fig:tae">1&lt;/a>, the visual system allows for a momentary bias. Usefully, the bias is sensitive to experimental manipulation. Figure &lt;a href="#fig:tae2">2&lt;/a> shows the same Gabor with the same orientations, but the Gabor also moves. The movement largely eliminates the bending. The sensitivity of such biases to different experimental manipulations enables researchers to study how the visual system balances new information against the recent past.&lt;/p>
&lt;div class="figure">&lt;span id="fig:tae">&lt;/span>
&lt;img src="https://psadil.github.io/psadil/post/serial-dependence/index.en_files/figure-html/tae-1.gif" alt="A Gabor alternates between two orientations." />
&lt;p class="caption">
Figure 1: A Gabor alternates between two orientations.
&lt;/p>
&lt;/div>
&lt;div class="figure">&lt;span id="fig:tae2">&lt;/span>
&lt;img src="https://psadil.github.io/psadil/post/serial-dependence/index.en_files/figure-html/tae2-1.gif" alt="A Gabor alternates between two orientations, appearing in a different location with each orientation." />
&lt;p class="caption">
Figure 2: A Gabor alternates between two orientations, appearing in a different location with each orientation.
&lt;/p>
&lt;/div>
&lt;p>A closely effect is called &lt;em>serial dependence&lt;/em>. Serial dependence occurs when participants report the orientations of sequentially presented, tilted Gabors &lt;span class="citation">(&lt;a href="#ref-fischer2014" role="doc-biblioref">Fischer and Whitney 2014&lt;/a>)&lt;/span>. A visual mask to reduces the strong aftereffects present in Figures &lt;a href="#fig:tae">1&lt;/a> and &lt;a href="#fig:tae2">2&lt;/a> [Figure &lt;a href="#fig:gabor">3&lt;/a>]&lt;a href="#fn1" class="footnote-ref" id="fnref1">&lt;sup>1&lt;/sup>&lt;/a>. Even without the aftereffect, the perceptual-decision about one Gabor affects the perceptual-decision about the next; participants report orientations that consistently err toward the orientation of the most recently seen Gabor. Reports on the magnitude of the effect vary, but the error has an average maximum of less than a few degrees. However, serial dependence is affected by different manipulations than that the demonstration of Figures &lt;a href="#fig:tae">1&lt;/a> and &lt;a href="#fig:tae2">2&lt;/a>. For example, it appears insensitive to the location of the Gabors. This bias may therefore provide a unique way to study how current perception is not only biased by but toward the recent past.&lt;/p>
&lt;div class="figure">&lt;span id="fig:gabor">&lt;/span>
&lt;img src="https://psadil.github.io/psadil/post/serial-dependence/index.en_files/figure-html/gabor-1.gif" alt="Differently oriented Gabors presented with interspersed noise masks.." />
&lt;p class="caption">
Figure 3: Differently oriented Gabors presented with interspersed noise masks..
&lt;/p>
&lt;/div>
&lt;p>However, it remains unclear whether serial dependence is a bias of perceptual or post-perceptual processes. That is, does serial dependence alter participants’ perception of the Gabors, or does it alter how they report the orientation? The sequential timing of each trial – in which participants respond in a designated period after seeing the Gabor – does not imply that participants decide on an orientation only after they have finished perceiving the Gabor. For example, a participant can make decisions before the response period, and they can adopt a biased response strategy even before seeing the Gabor. Where and when to delineate between perception and decision, or whether they can be delineated, depends on assumptions about the relationship between perception and decisions. A tool like the &lt;a href="https://psadil.github.io/psadil/post/circular-diffusion-model-of-response-times/">circular diffusion model&lt;/a> can help make those assumptions explicit &lt;span class="citation">(&lt;a href="#ref-smith2016" role="doc-biblioref">Smith 2016&lt;/a>)&lt;/span>.&lt;/p>
&lt;div id="references" class="section level1 unnumbered">
&lt;h1>References&lt;/h1>
&lt;div id="refs" class="references csl-bib-body hanging-indent">
&lt;div id="ref-fischer2014" class="csl-entry">
Fischer, Jason, and David Whitney. 2014. &lt;span>“Serial Dependence in Visual Perception.”&lt;/span> &lt;em>Nature Neuroscience&lt;/em> 17 (5): 738.
&lt;/div>
&lt;div id="ref-smith2016" class="csl-entry">
Smith, Philip L. 2016. &lt;span>“Diffusion Theory of Decision Making in Continuous Report.”&lt;/span> &lt;em>Psychological Review&lt;/em> 123 (4): 425–51. &lt;a href="https://doi.org/10.1037/rev0000023">https://doi.org/10.1037/rev0000023&lt;/a>.
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="footnotes">
&lt;hr />
&lt;ol>
&lt;li id="fn1">&lt;p>The timing and spacing of this figure does not quite match a typical experiment. For example, participants take a few seconds to respond, so the amount of time between Gabors in this figure is too short.&lt;a href="#fnref1" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>Circular Diffusion Model of Response Times</title><link>https://psadil.github.io/psadil/post/circular-diffusion-model-of-response-times/</link><pubDate>Thu, 12 Sep 2019 00:00:00 +0000</pubDate><guid>https://psadil.github.io/psadil/post/circular-diffusion-model-of-response-times/</guid><description>
&lt;script src="https://psadil.github.io/psadil/post/circular-diffusion-model-of-response-times/index.en_files/header-attrs/header-attrs.js">&lt;/script>
&lt;p>Many cognitive experiments involve asking participants to answer questions that require circular responses (Figure &lt;a href="#fig:color">1&lt;/a>). What was the color of the shape you just saw? In which direction was the arrow pointing? How tilted was the bar? The answers required by these questions differ fundamentally from the more common, categorical responses required to questions. Was the color green or red? Was the arrow pointing left or right? Was the bar tilted more than 45 degrees from vertical, between 45-90, or more than 90 degrees? In the continuous case, the experimenter looses the ability to classify responses as either correct or incorrect, and an analysis must consider participants’ degree of inaccuracy, their relative error. Circularity adds the additional complication that a response can only be erroneous up to a point; if a person responds that a vertical bar is 3 degrees offset from vertical on trial one and 359 degrees on trial two, the analysis must acknowledge that the average is close to truth. Although many models exist that describe how a participant will respond when the choice is binary, models of these are much more limited.&lt;/p>
&lt;div class="figure">&lt;span id="fig:color">&lt;/span>
&lt;img src="https://psadil.github.io/psadil/post/circular-diffusion-model-of-response-times/index.en_files/figure-html/color-1.png" alt="Cognitive experiments often require participants to provide a circularly continuous response. A participant might be asked to study shapes of different colors, hold the colors of every shape in memory, and then report on the one shape's color." width="672" />
&lt;p class="caption">
Figure 1: Cognitive experiments often require participants to provide a circularly continuous response. A participant might be asked to study shapes of different colors, hold the colors of every shape in memory, and then report on the one shape’s color.
&lt;/p>
&lt;/div>
&lt;p>&lt;span class="citation">&lt;a href="#ref-smith2016" role="doc-biblioref">Smith&lt;/a> (&lt;a href="#ref-smith2016" role="doc-biblioref">2016&lt;/a>)&lt;/span> present a new model of how participants provide circularly continuous responses, called the circular diffusion model. It is a model of the decision-making process, analyzing both the numerical value participants provided and how long it took them to provide a response. The model extends the drift diffusion model of binary decisions &lt;span class="citation">(&lt;a href="#ref-ratcliff1978" role="doc-biblioref">Ratcliff 1978&lt;/a>)&lt;/span>. Like the drift diffusion model, the circular diffusion model casts perceptual decisions as a stochastic process of evidence accumulation to a threshold; evidence is accumulated over time, and when enough evidence has been reached the process terminates in a motor behavior. The model is not concerned with how evidence accumulates, just that it does. In a working memory experiment, evidence might accumulate through repeated probes of memory. In a perceptual-decision task, each saccade might provide a different amount of evidence. In both cases, evidence grows at an average rate, and when there is enough evidence for a decision that decision is made. The amount of time required to reach that threshold of evidence is the response time. The circular diffusion model, therefore, proposes that the responses of rapid decisions which require circularly continuous responses can be modeled as a particle drifting in two dimensions out towards a circular boundary.&lt;/p>
&lt;div class="figure">&lt;span id="fig:cdm">&lt;/span>
&lt;img src="https://psadil.github.io/psadil/post/circular-diffusion-model-of-response-times/index.en_files/figure-html/cdm-1.gif" alt="A diffusing particle models perceptual decisions. In this example, wherever the particle first crosses the circle corresponds to the response, and the amount of time required to reach the edge is their response time." />
&lt;p class="caption">
Figure 2: A diffusing particle models perceptual decisions. In this example, wherever the particle first crosses the circle corresponds to the response, and the amount of time required to reach the edge is their response time.
&lt;/p>
&lt;/div>
&lt;p>Using the circular diffusion model affords researchers the same advantages conferred by using the standard drift diffusion model: the decision-making process can be decomposed into parameters of the model, and those parameters have psychologically meaningful values. For example, a participant might respond quickly, but that could either occur because they accumulate evidence rapidly or because they set a low threshold for evidence. There are three key parameters in the model: 1) the average direction the particle drifts (towards what decision are participants mostly accumulating evidence?), 2) the average rate at which the particle drifts (how quickly do participants accumulate evidence?), and 3) the radius of the circular boundary (how conservative are participants?). Estimating these parameters for participants across different conditions of an experiment enables the researcher to “measure” each of these psychological constructs given participants’ behavior.&lt;/p>
&lt;div id="references" class="section level1 unnumbered">
&lt;h1>References&lt;/h1>
&lt;div id="refs" class="references csl-bib-body hanging-indent">
&lt;div id="ref-ratcliff1978" class="csl-entry">
Ratcliff, Roger. 1978. &lt;span>“A Theory of Memory Retrieval.”&lt;/span> &lt;em>Psychological Review&lt;/em> 85 (2): 59. &lt;a href="https://doi.org/10.1037/0033-295X.85.2.59">https://doi.org/10.1037/0033-295X.85.2.59&lt;/a>.
&lt;/div>
&lt;div id="ref-smith2016" class="csl-entry">
Smith, Philip L. 2016. &lt;span>“Diffusion Theory of Decision Making in Continuous Report.”&lt;/span> &lt;em>Psychological Review&lt;/em> 123 (4): 425–51. &lt;a href="https://doi.org/10.1037/rev0000023">https://doi.org/10.1037/rev0000023&lt;/a>.
&lt;/div>
&lt;/div>
&lt;/div></description></item><item><title>eyetracking with eyelink in psychtoolbox, now with oop</title><link>https://psadil.github.io/psadil/post/eyetracking-in-psychtoolbox-oop/</link><pubDate>Sat, 25 May 2019 00:00:00 +0000</pubDate><guid>https://psadil.github.io/psadil/post/eyetracking-in-psychtoolbox-oop/</guid><description>
&lt;script src="https://psadil.github.io/psadil/post/eyetracking-in-psychtoolbox-oop/index.en_files/header-attrs/header-attrs.js">&lt;/script>
&lt;p>I’ve started trying out &lt;a href="https://www.mathworks.com/discovery/object-oriented-programming.html">MATLAB’s OOP&lt;/a> after mounting suspicion that the way I’d been coding experiments basically involved making something that looked and behaved like an object–but did so in a convoluted and inefficient way. See this &lt;a href="https://psadil.github.io/psadil/post/eyetracking-init/">post on eyetracking with PTB&lt;/a> as proof.&lt;/p>
&lt;p>This post is brief, and is about as well thought out as a github gist/gitlab snippet.&lt;/p>
&lt;p>The two classes I’ll work with here is a Window class and a Tracker class. The window class has 3 methods. The first &lt;a href="https://www.mathworks.com/help/matlab/matlab_oop/class-constructor-methods.html">constructor&lt;/a> method exists just to create the object. The constructed object will have a few default properties of the class. The second method is open, which (can you guess?) calls the PTB functions to open an onscreen window. The open method is fancier than it needs to be for this post (note the PsychImaging configuration, and the optional debugLevel flag). The final window method is the &lt;a href="https://www.mathworks.com/help/matlab/matlab_oop/handle-class-destructors.html">desctructor&lt;/a>. The destructor method is one of the advantages of leaning on MATLAB’s OOP syntax. That method will get called whenever the Window object’s lifecycle has ended (which might happen from explicit deletion of the object, closing MATLAB, the object is no longer referenced in the call stack, etc).&lt;/p>
&lt;p>The second class is the Tracker class, which interfaces with Eyelink. The Window class is only present here because Eyelink needs an open window to run calibration. There are five Tracker methods, but they are either analogous to the Window objects methods (constructor, destructor) or were largely presented in the previous post.&lt;/p>
&lt;div id="window-object" class="section level2">
&lt;h2>Window Object&lt;/h2>
&lt;pre class="matlab">&lt;code>
classdef Window &amp;lt; handle
% Window handles opening and closing of screen
properties (Constant)
screenNumber = 0
% background color of screen
background = GrayIndex(Window.screenNumber)
end
properties
pointer
winRect
end
methods
function obj = Window()
end
function open(obj, skipsynctests, debuglevel)
PsychImaging(&amp;#39;PrepareConfiguration&amp;#39;);
PsychImaging(&amp;#39;AddTask&amp;#39;, &amp;#39;General&amp;#39;, &amp;#39;FloatingPoint16Bit&amp;#39;);
Screen(&amp;#39;Preference&amp;#39;, &amp;#39;SkipSyncTests&amp;#39;, skipsynctests);
switch debuglevel
% no debug. run as usual, without listening to keyboard input
% and also hiding the cursor
case 0
ListenChar(-1);
HideCursor;
[obj.pointer, obj.winRect] = ...
PsychImaging(&amp;#39;OpenWindow&amp;#39;, obj.screenNumber, obj.background);
% light debug: still open fullscreen window, but keep keyboard input
case 1
[obj.pointer, obj.winRect] = ...
PsychImaging(&amp;#39;OpenWindow&amp;#39;, obj.screenNumber, obj.background);
% full debug: only open transparent window
case 10
PsychDebugWindowConfiguration(0, .5)
[obj.pointer, obj.winRect] = ...
PsychImaging(&amp;#39;OpenWindow&amp;#39;, obj.screenNumber, obj.background);
end
% Turn on blendfunction for antialiasing of drawing dots
Screen(&amp;#39;BlendFunction&amp;#39;, obj.pointer, &amp;#39;GL_SRC_ALPHA&amp;#39;, &amp;#39;GL_ONE_MINUS_SRC_ALPHA&amp;#39;);
topPriorityLevel = MaxPriority(obj.pointer);
Priority(topPriorityLevel);
end
% will auto-close open windows and return keyboard control when
% this object is deleted
function delete(obj) %#ok&amp;lt;INUSD&amp;gt;
ListenChar(0);
Priority(0);
sca;
end
end
end
&lt;/code>&lt;/pre>
&lt;/div>
&lt;div id="tracker-object" class="section level2">
&lt;h2>Tracker Object&lt;/h2>
&lt;p>The tracker object will mostly do what it did in the &lt;a href="https://psadil.github.io/psadil/post/eyetracking-init/">previous post&lt;/a>. Same functionality, but the syntax is much cleaner than the heavy use of switch/case conditionals.&lt;/p>
&lt;pre class="matlab">&lt;code>classdef Tracker &amp;lt; handle
properties
% flag to be called in scripts which enable turning on or off the tracker
% in an experiment (e.g., when debug mode is on)
using_tracker logical = false
% name of the write. must follow eyelink conventions. alphanumeric only, no
% more than 8 characters
filename char = &amp;#39;&amp;#39;
% eyelink object structure. stores many relevant parameters
el
end
methods
function obj = Tracker(using_tracker, filename, window)
obj.using_tracker = using_tracker;
obj.filename = filename;
% run calibration for tracker (see method below)
calibrate(obj, window);
end
function calibrate(obj, window)
if obj.using_tracker
% Provide Eyelink with details about the graphics environment
% and perform some initializations. The information is returned
% in a structure that also contains useful defaults
% and control codes (e.g. tracker state bit and Eyelink key values).
obj.el = EyelinkInitDefaults(window.pointer);
if ~EyelinkInit(0, 1)
error(&amp;#39;\n Eyelink Init aborted \n&amp;#39;);
end
%Reduce FOV for calibration and validation. Helpful when the
% the stimulus is only in the center of the screen, or at places
% like the fMRI scanner at UMass where the eyes have a lot in front
% of them
Eyelink(&amp;#39;Command&amp;#39;,&amp;#39;calibration_area_proportion = 0.5 0.5&amp;#39;);
Eyelink(&amp;#39;Command&amp;#39;,&amp;#39;validation_area_proportion = 0.5 0.5&amp;#39;);
% open file to record data to
status = Eyelink(&amp;#39;Openfile&amp;#39;, obj.filename);
if status ~= 0
error(&amp;#39;\n Eyelink Init aborted \n&amp;#39;);
end
% Setting the proper recording resolution, proper calibration type,
% as well as the data file content;
Eyelink(&amp;#39;Command&amp;#39;,&amp;#39;screen_pixel_coords = %ld %ld %ld %ld&amp;#39;, 0, 0, window.winRect(3)-1, window.winRect(4)-1);
Eyelink(&amp;#39;message&amp;#39;, &amp;#39;DISPLAY_COORDS %ld %ld %ld %ld&amp;#39;, 0, 0, window.winRect(3)-1, window.winRect(4)-1);
% set calibration type to 5 point.
Eyelink(&amp;#39;Command&amp;#39;, &amp;#39;calibration_type = HV5&amp;#39;);
% set EDF file contents using the file_sample_data and
% file-event_filter commands
% set link data thtough link_sample_data and link_event_filter
Eyelink(&amp;#39;Command&amp;#39;, &amp;#39;file_event_filter = RIGHT,FIXATION,SACCADE,BLINK,MESSAGE,BUTTON,INPUT&amp;#39;);
Eyelink(&amp;#39;Command&amp;#39;, &amp;#39;link_event_filter = RIGHT,FIXATION,SACCADE,BLINK,MESSAGE,BUTTON,INPUT&amp;#39;);
% check the software version
% add &amp;quot;HTARGET&amp;quot; to record possible target data for EyeLink Remote
Eyelink(&amp;#39;command&amp;#39;, &amp;#39;file_sample_data = RIGHT,GAZE,HREF,GAZERES,AREA,HTARGET,STATUS,INPUT&amp;#39;);
Eyelink(&amp;#39;command&amp;#39;, &amp;#39;link_sample_data = RIGHT,GAZE,HREF,GAZERES,AREA,HTARGET,STATUS,INPUT&amp;#39;);
% make sure we&amp;#39;re still connected.
if Eyelink(&amp;#39;IsConnected&amp;#39;)~=1
error(&amp;#39;\n Eyelink Init aborted \n&amp;#39;);
end
% set sample rate in camera setup screen
Eyelink(&amp;#39;Command&amp;#39;, &amp;#39;sample_rate = %d&amp;#39;, 1000);
% opens up main calibration scheme
EyelinkDoTrackerSetup(obj.el);
end
end
function status = eyelink(obj, varargin)
% calls main Eyelink routines only when
% this tracker object property using_tracker==true.
status = [];
if obj.using_tracker
if nargin==2
% construct calls to eyelink that don&amp;#39;t output any
% status
if strcmp(varargin{1}, &amp;#39;StopRecording&amp;#39;) || ...
strcmp(varargin{1}, &amp;#39;Shutdown&amp;#39;) ||...
strcmp(varargin{1}, &amp;#39;SetOfflineMode&amp;#39;)
% magic happens here, where the variable argument input
% is expanded an repassed through to Eyelink()
Eyelink(varargin{:});
else
status = Eyelink(varargin{:});
end
% all calls to Eyelink that have more than two inputs (e.g., the
% name of a function with some parameters to that function) return
% some status
else
status = Eyelink(varargin{:});
end
end
end
% starts up the eyelink machine. call this once the start of each
% experiment. could modify function to also draw something special
% to the screen (e.g., a background image). this might be the kind
% of function to modify if you wanted to draw trial-by-trial material
% to the eyetracking computer
function startup(obj)
% Must be offline to draw to EyeLink screen
obj.eyelink(&amp;#39;SetOfflineMode&amp;#39;);
% clear tracker display and draw background img to host pc
obj.eyelink(&amp;#39;Command&amp;#39;, &amp;#39;clear_screen 0&amp;#39;);
% draw simple fixation cross as later reference
obj.eyelink(&amp;#39;command&amp;#39;, &amp;#39;draw_cross %d %d&amp;#39;, 1920/2, 1080/2);
% give image transfer time to finish
WaitSecs(0.1);
end
% destructor function will get called whenever tracker object is deleted (e.g.,
% this function is automatically called when MATLAB closes, meaning you can&amp;#39;t
% forget to close the file connection with the tracker computer).
function delete(obj)
% waitsecs occur because the filetransfer often takes a moment, and moving
% on too quickly will result in an error
% End of Experiment; close the file first
% close graphics window, close data file and shut down tracker
obj.eyelink(&amp;#39;StopRecording&amp;#39;);
WaitSecs(0.1); % Slack to let stop definitely happen
obj.eyelink(&amp;#39;SetOfflineMode&amp;#39;);
obj.eyelink(&amp;#39;CloseFile&amp;#39;);
WaitSecs(0.1);
obj.eyelink(&amp;#39;ReceiveFile&amp;#39;, obj.filename, fullfile(pwd,&amp;#39;events&amp;#39;), 1);
WaitSecs(0.2);
obj.eyelink(&amp;#39;Shutdown&amp;#39;);
end
end
end
&lt;/code>&lt;/pre>
&lt;/div>
&lt;div id="run-the-calibration-and-use-the-tracker" class="section level2">
&lt;h2>Run the calibration (and use the tracker)&lt;/h2>
&lt;p>Putting this together, the following script starts calibration, and outlines how this tracker could be used in an experiment.&lt;/p>
&lt;pre class="matlab">&lt;code>
%% input
% ------------------
skipsynctests = 2;
debuglevel = 0;
using_tracker = true;
%% setup
% ------------------
% boilerplate setup
PsychDefaultSetup(2);
% initialize window
window = Window();
% open that window
open(window, skipsynctests, debuglevel)
% Initialize tracker object
tracker = Tracker(using_tracker, &amp;#39;OOPDEMO.edf&amp;#39;, window);
% run calibration
tracker.startup();
% Let Eyelink know that the experiment starts now
tracker.eyelink(&amp;#39;message&amp;#39;, &amp;#39;SYNCTIME&amp;#39;);
%% Experiment/trial code
% ------------------
% note that we should not need to wait to start recording,
% given that the stimulus will always be drawn a bit later
% (determined by how often phase changes occur)
tracker.eyelink(&amp;#39;StartRecording&amp;#39;);
% trial/experiment happens here ...
tracker.eyelink(&amp;#39;StopRecording&amp;#39;);
% Wait moment to ensure that tracker is definitely finished with the last few samples
WaitSecs(0.001);
%% Cleanup
% ------------------
% closes connection to Eyelink system, saves file
delete(tracker);
% closes window, restores keyboard input
delete(window);
&lt;/code>&lt;/pre>
&lt;p>What’s nice about this syntax (as before) is that only very minimal changes are required you don’t want to call the Eyelink functions (e.g., if you’re testing on a computer that doesn’t have the Eyelink system connected, or you’re debugging other parts of the experiment). By changing just the input, the Eyelink functions won’t be called.&lt;/p>
&lt;pre class="matlab">&lt;code>
using_tracker = false;
% all the rest as above
% ...&lt;/code>&lt;/pre>
&lt;/div>
&lt;div id="summary" class="section level1">
&lt;h1>Summary&lt;/h1>
&lt;p>There’s not much to summarize because I haven’t explained much! Again, this post is largely just an attempt to revise what I now think is a poor implementation, presented in an earlier post.&lt;/p>
&lt;/div></description></item><item><title>A hierarchical Bayesian state trace analysis for assessing monotonicity while factoring out subject, item, and trial level dependencies</title><link>https://psadil.github.io/psadil/publication/sadil-2019-hierarchical/</link><pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate><guid>https://psadil.github.io/psadil/publication/sadil-2019-hierarchical/</guid><description/></item><item><title>A roadmap for understanding memory: Decomposing cognitive processes into operations and representations</title><link>https://psadil.github.io/psadil/publication/cowell-2019-roadmap/</link><pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate><guid>https://psadil.github.io/psadil/publication/cowell-2019-roadmap/</guid><description/></item><item><title>Connecting the dots without top-down knowledge: Evidence for rapidly-learned low-level associations that are independent of object identity.</title><link>https://psadil.github.io/psadil/publication/sadil-2019-connecting/</link><pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate><guid>https://psadil.github.io/psadil/publication/sadil-2019-connecting/</guid><description/></item><item><title>Basic Importance Sampling for Variance Reduction</title><link>https://psadil.github.io/psadil/post/importance-sampling/</link><pubDate>Sat, 10 Nov 2018 00:00:00 +0000</pubDate><guid>https://psadil.github.io/psadil/post/importance-sampling/</guid><description>
&lt;script src="https://psadil.github.io/psadil/post/importance-sampling/index.en_files/header-attrs/header-attrs.js">&lt;/script>
&lt;p>This post attempts to provide a brief intro to importance sampling, presenting a relatively simple situation in which importance sampling enables a speedup in Monte Carlo by reducing the variance of the estimate.&lt;/p>
&lt;p>A lot of the material will be an effort to remember material that was taught to be in a Stochastic Processes and Applications class. This post should be though of as my notes on the &lt;a href="http://people.math.umass.edu/~lr7q/m697u-spring2018/m697uhome.html">course notes&lt;/a>, with supplementation by &lt;a href="https://www.statlect.com/asymptotic-theory/importance-sampling" class="uri">https://www.statlect.com/asymptotic-theory/importance-sampling&lt;/a>. That is, it can safely be assumed that any content comes from those notes, excluding errors (which will be due to my translation)&lt;a href="#fn1" class="footnote-ref" id="fnref1">&lt;sup>1&lt;/sup>&lt;/a>.&lt;/p>
&lt;div id="what-is-importance-sampling" class="section level1">
&lt;h1>What is importance sampling?&lt;/h1>
&lt;p>Importance sampling comes up in the context of Monte Carlo, (MC) applications as a technique for reducing the variability in the MC estimator. As a reminder, MC can be used to estimate the expected value, &lt;span class="math inline">\(\mu\)&lt;/span>, of a random variable, &lt;span class="math inline">\(X\)&lt;/span>.&lt;/p>
&lt;p>&lt;span class="math display">\[
\mu = \mathbb{E}[h(x)] = \int h(x)p_X(x)\,dx
\]&lt;/span>&lt;/p>
&lt;p>The idea of Monte Carlo is that this expectation can be estimated by drawing &lt;span class="math inline">\(S\)&lt;/span> samples from the distribution &lt;span class="math inline">\(p_X\)&lt;/span>, where &lt;span class="math inline">\(X \sim p_X\)&lt;/span>&lt;/p>
&lt;p>&lt;span class="math display">\[
\hat{\mu} =\frac{1}{S}\sum_{s=1}^S h(x_s)
\]&lt;/span>&lt;/p>
&lt;p>where the subscript on &lt;span class="math inline">\(x\)&lt;/span> implies the &lt;span class="math inline">\(s^th\)&lt;/span> draw of &lt;span class="math inline">\(X\)&lt;/span>, and the hat over &lt;span class="math inline">\(\mu\)&lt;/span> indicates that the result is an estimate. Note that, since we’re starting from that equation, it is assumed that there is some way to produce random samples from the distribution &lt;span class="math inline">\(p_X\)&lt;/span>, and that the function, &lt;span class="math inline">\(h\)&lt;/span> is calculable for any &lt;span class="math inline">\(X\)&lt;/span>. Also, &lt;span class="math inline">\(h\)&lt;/span> might be something as simple as &lt;span class="math inline">\(h(x) = x\)&lt;/span> if the expectation should correspond to the mean of &lt;span class="math inline">\(x\)&lt;/span>].&lt;/p>
&lt;p>This is a powerful idea, though a general downside is that some &lt;span class="math inline">\(\mu\)&lt;/span> require many samples to obtain a reasonable estimate. The MC Standard Error (MCSE) provides an estimate of the approximation error, which is &lt;span class="math inline">\(\frac{1}{n} Var(h(X))\)&lt;/span>. This provides a metric by which to gauge different algorithms. In particular, if all else is equal, the estimator with lower variance will can obtain a given level of precision with a lower &lt;span class="math inline">\(S\)&lt;/span>.&lt;/p>
&lt;p>The basic idea with Importance Sampling (IS) is to draw samples from some alternative distribution, &lt;span class="math inline">\(p_Y\)&lt;/span>, which has the same support as &lt;span class="math inline">\(p_X\)&lt;/span>, then reweight those samples in accordance with the difference between &lt;span class="math inline">\(p_X\)&lt;/span> and &lt;span class="math inline">\(p_Y\)&lt;/span>.&lt;/p>
&lt;p>&lt;span class="math display">\[
\begin{aligned}
\mathbb{E}[h(x)] &amp;amp; = \int h(x)p_X(x) \,dx &amp;amp; \textrm{definition of expectation} \\
&amp;amp; = \int h(x)\frac{p_X(x)}{p_Y(x)}p_Y(x) \,dx &amp;amp; \textrm{multiplication by 1, assuming same support} \\
&amp;amp; \int h(y)\frac{p_X(y)}{p_Y(y)}p_Y(y) \,dy &amp;amp; \textrm{assuming same support} \\
&amp;amp; = \mathbb{E} \left[h(y)\frac{p_X(y)}{p_Y(y)} \right] &amp;amp; \textrm{our new importance sampling estimator}
\end{aligned}
\]&lt;/span>&lt;/p>
&lt;p>Recognize that there will often not be a single unique &lt;span class="math inline">\(p_Y\)&lt;/span>. The goal is to find a &lt;span class="math inline">\(p_Y\)&lt;/span> that results in lower MCSE. The MCSE for the importance sampling estimator is &lt;span class="math inline">\(\frac{1}{n}Var\left[h(y)\frac{p_X(y)}{p_Y(y)} \right]\)&lt;/span>. That will be used to gain an intuition for how to choose a useful &lt;span class="math inline">\(p_Y\)&lt;/span>.&lt;/p>
&lt;/div>
&lt;div id="why-does-importance-sampling-work" class="section level1">
&lt;h1>Why does importance sampling work?&lt;/h1>
&lt;p>One way to think about importance sampling is that, if we could sample from &lt;span class="math inline">\(h(y)\frac{p_X(y)}{p_Y(y)}\)&lt;/span> such that the result were constant, the variance would be 0, and so we’d only need a single sample. To see why, note that, for a constant &lt;span class="math inline">\(c\)&lt;/span>&lt;/p>
&lt;p>&lt;span class="math display">\[
\begin{aligned}
h(y)\frac{p_X(y)}{p_Y(y)} &amp;amp; = c \\
\implies p_Y(y)c &amp;amp; = h(y)p_X(y) \\
\implies p_Y(y) &amp;amp; \propto h(y)p_X(y) \\
\end{aligned}
\]&lt;/span>&lt;/p>
&lt;p>That is, &lt;span class="math inline">\(h(y)\frac{p_X(y)}{p_Y(y)}\)&lt;/span> will be constant whenever &lt;span class="math inline">\(p_Y(y)\)&lt;/span> is proportional to &lt;span class="math inline">\(h(y)p_X(x)\)&lt;/span>.&lt;/p>
&lt;p>&lt;span class="math display">\[
\begin{aligned}
p_Y(y) &amp;amp; = \frac{h(y)p_X(y)}{\int h(y)p_X(y)\,dy} \\
\implies p_Y(y) &amp;amp; = \frac{h(y)p_X(y)}{\mathbb{E}[h(X)]} \\
&amp;amp; = \frac{h(y)p_X(y)}{\mu} &amp;amp; \textrm {definition of }\mu
\end{aligned}
\]&lt;/span>&lt;/p>
&lt;p>Plugging this distribution into the IS estimator&lt;/p>
&lt;p>&lt;span class="math display">\[
\begin{aligned}
\frac{1}{S} \sum_{s=1}^S \frac{h(Y_s)p_X(Y_s)}{p_Y(Y_s)} &amp;amp; = \frac{1}{S} \sum_{s=1}^S \frac{h(Y_s)p_X(Y_s)}{\frac{h(Y_s)p_X(Y_s)}{\mathbb{E}[h(X_s)]}} \\
&amp;amp; = \frac{1}{S} S\mu \\
&amp;amp; = \mu
\end{aligned}
\]&lt;/span>&lt;/p>
&lt;p>So, regardless of &lt;span class="math inline">\(S\)&lt;/span>, the resulting estimator is always &lt;span class="math inline">\(\mu\)&lt;/span>.&lt;/p>
&lt;p>That’s almost useful, but this means that to get an optimal &lt;span class="math inline">\(p_Y\)&lt;/span> we need to know &lt;span class="math inline">\(\mathbb{E}[h(X)]\)&lt;/span>, which is by definition the &lt;span class="math inline">\(\mu\)&lt;/span> that we’re trying to estimate. Still, knowing what is optimal provides the intuition for why importance sampling works.&lt;/p>
&lt;p>There are two ideas going on here. First, the optimal &lt;span class="math inline">\(p_Y\)&lt;/span> is one which places higher density on regions where &lt;span class="math inline">\(h(X)\)&lt;/span> is high, as compared to &lt;span class="math inline">\(p_X\)&lt;/span>. Those “important” values are the ones that will determine the result of &lt;span class="math inline">\(h(x)\)&lt;/span>, so those are the ones that need to be altered the most (going from &lt;span class="math inline">\(p_X\)&lt;/span> to &lt;span class="math inline">\(p_Y\)&lt;/span>). Then, the second idea is that when averaging, we must account for the oversampling of these points – downweighting them by however much they were oversampled, which is the ratio &lt;span class="math inline">\(\frac{p_X(y)}{p_Y(y)}\)&lt;/span>.&lt;/p>
&lt;/div>
&lt;div id="using-is-to-reduce-variance" class="section level1">
&lt;h1>Using IS to reduce variance&lt;/h1>
&lt;p>Here’s an example of this working out. The value we’re trying to estimate will be, for &lt;span class="math inline">\(X \sim N(0,1)\)&lt;/span>&lt;/p>
&lt;p>&lt;span class="math display">\[
\mu = \int \phi(x-4)p_X(x)\,dx
\]&lt;/span>&lt;/p>
&lt;p>where &lt;span class="math inline">\(\phi\)&lt;/span> is the standard normal density function. This &lt;span class="math inline">\(h\)&lt;/span> is such that only values near 4 provide much contribution to the average.&lt;/p>
&lt;pre class="r">&lt;code>set.seed(1234)
hx &amp;lt;- function(x) {
return(dnorm(x - 4))
}
x &amp;lt;- seq(-5, 10, length.out = 1000)
plot(x, hx(x), type = &amp;quot;l&amp;quot;)
points(x, dnorm(x), col = &amp;quot;blue&amp;quot;, type = &amp;quot;l&amp;quot;)&lt;/code>&lt;/pre>
&lt;div class="figure">&lt;span id="fig:mismatch">&lt;/span>
&lt;img src="https://psadil.github.io/psadil/post/importance-sampling/index.en_files/figure-html/mismatch-1.png" alt="h(x) (black) has mostly 0 around where p_X (blue) has high density. The mismatch of these distributions will contribute to unnecessarily high variance of the estimator." width="672" />
&lt;p class="caption">
Figure 1: h(x) (black) has mostly 0 around where p_X (blue) has high density. The mismatch of these distributions will contribute to unnecessarily high variance of the estimator.
&lt;/p>
&lt;/div>
&lt;p>However, &lt;span class="math inline">\(X\)&lt;/span> will only rarely contain values that are near 4. Hence, the variance of the estimation will be relatively high. We can get an explicit formula for the variance.&lt;/p>
&lt;p>&lt;span class="math display">\[
Var(h(x)) = \mathbb{E}[h(x)^2] - \mathbb{E}[h(x)]^2
\]&lt;/span>&lt;/p>
&lt;p>A formula that involves calculating the expected value of this function&lt;/p>
&lt;p>&lt;span class="math display">\[
\begin{aligned}
\mathbb{E}[h(x)] &amp;amp; = \int_{-\infty}^{\infty} \left(\frac{\exp(- \frac{x^2}{2})}{\sqrt{2\pi}} \right)\left(\frac{\exp(- \frac{x^2}{2})}{\sqrt{2\pi}} \right) dx \\
&amp;amp; = \int_{-\infty}^{\infty} \frac{\exp (- x^2 + 4x - 8 )}{2\pi} dx \\
&amp;amp; = \frac{1}{2\pi} \int_{-\infty}^{\infty} \exp (- x^2 + 4x - 8 ) dx\\
&amp;amp; = \frac{1}{2\pi} \sqrt{\pi}\exp \left(\frac{4^2}{4}-8 \right) &amp;amp; \textrm{en.wikipedia.org/wiki/Gaussian_function} \\
&amp;amp; = \frac{1}{2 \exp(4) \sqrt{\pi}}
\end{aligned}
\]&lt;/span>&lt;/p>
&lt;p>Which we’ll save for now to use later&lt;/p>
&lt;pre class="r">&lt;code>mu &amp;lt;- 1/(2 * exp(4) * sqrt(pi))
mu&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] 0.005166746&lt;/code>&lt;/pre>
&lt;p>Returning to the variance calculation&lt;/p>
&lt;p>&lt;span class="math display">\[
\begin{aligned}
Var(h(x)) &amp;amp; = \left[ \int_{-\infty}^{\infty} \left(\frac{\exp(- \frac{(x-2)^2}{2})}{\sqrt{2\pi}} \right)^2 \left(\frac{\exp(- \frac{x^2}{2})}{\sqrt{2\pi}} \right) \,dx \right] - \mu^2 \\
&amp;amp; = \frac{1}{2\sqrt{2}\pi^{3/2}} \int_{-\infty}^{\infty} \exp\left(-\frac{3}{2}x^2+8x-16 \right) \,dx - \mu^2 \\
&amp;amp; = \frac{1}{2\sqrt{2}\pi^{3/2}} \sqrt{\frac{\pi}{3/2}}\exp \left(\frac{8^2}{6} -16 \right) \\
&amp;amp; = \frac{1}{2 \pi \sqrt{3} \exp(16/3)} - \mu^2
\end{aligned}
\]&lt;/span>&lt;/p>
&lt;pre class="r">&lt;code>1/(2 * pi * sqrt(3) * exp(16/3)) - mu^2&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] 0.0004169361&lt;/code>&lt;/pre>
&lt;div id="standard-mc-estimate-is-accurate-but-with-relatively-high-variance" class="section level3">
&lt;h3>Standard MC estimate is accurate, but with relatively high variance&lt;/h3>
&lt;p>Using an MC estimate,&lt;/p>
&lt;pre class="r">&lt;code>x &amp;lt;- rnorm(1e+06)
y &amp;lt;- hx(x)
var(y)&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] 0.0004270083&lt;/code>&lt;/pre>
&lt;p>Note also that the estimate (our target), is also accurate&lt;/p>
&lt;pre class="r">&lt;code>mean(y) - mu&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] 4.582938e-05&lt;/code>&lt;/pre>
&lt;/div>
&lt;div id="using-a-distribution-that-simply-matches-hx-is-also-not-so-great" class="section level3">
&lt;h3>Using a distribution that simply matches h(x) is also not-so-great&lt;/h3>
&lt;p>Now, let’s instead construct and IS estimator. One intuitive (but by no means optimal) attempt would be to use &lt;span class="math inline">\(Y \sim N(4,1)\)&lt;/span>, a distribution that matches with &lt;span class="math inline">\(h(x)\)&lt;/span> perfectly. Indeed, that will provide an accurate answer&lt;/p>
&lt;pre class="r">&lt;code>y &amp;lt;- rnorm(1e+06, mean = 4)
h &amp;lt;- hx(y)
IS &amp;lt;- h * dnorm(y, 0, 1)/dnorm(y, 4, 1)
mean(IS)&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] 0.005194268&lt;/code>&lt;/pre>
&lt;p>But, it turns out that the variance is about the same as before.&lt;/p>
&lt;pre class="r">&lt;code>var(IS)&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] 0.0004204215&lt;/code>&lt;/pre>
&lt;/div>
&lt;div id="the-proposal-distribution-needs-to-be-tuned-to-both-p_x-and-hx" class="section level3">
&lt;h3>The proposal distribution needs to be tuned to both p_X and h(x)&lt;/h3>
&lt;p>This is a somewhat subtle point of the derivation provided above. We &lt;em>don’t&lt;/em> just want a distribution that will be highest here &lt;span class="math inline">\(h(x)\)&lt;/span> is high. Instead, what we actually need is a distribution that will be highest when &lt;span class="math inline">\(h(x)p_X(x)\)&lt;/span> is high. That will be exactly where the two distributions intersect, at 2.&lt;/p>
&lt;pre class="r">&lt;code>x &amp;lt;- seq(-5, 10, length.out = 1000)
plot(x, hx(x), type = &amp;quot;l&amp;quot;)
points(x, dnorm(x), col = &amp;quot;blue&amp;quot;, type = &amp;quot;l&amp;quot;)
abline(v = 2)&lt;/code>&lt;/pre>
&lt;div class="figure">&lt;span id="fig:matching">&lt;/span>
&lt;img src="https://psadil.github.io/psadil/post/importance-sampling/index.en_files/figure-html/matching-1.png" alt="Same as above, but with line demonstrating intersection at x=2" width="672" />
&lt;p class="caption">
Figure 2: Same as above, but with line demonstrating intersection at x=2
&lt;/p>
&lt;/div>
&lt;pre class="r">&lt;code>y &amp;lt;- rnorm(1e+06, mean = 2)
h &amp;lt;- hx(y)
IS &amp;lt;- h * dnorm(y, 0)/dnorm(y, 2)
mean(IS)&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] 0.005165774&lt;/code>&lt;/pre>
&lt;pre class="r">&lt;code>var(IS)&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] 4.131799e-06&lt;/code>&lt;/pre>
&lt;p>The answer is still accurate, but the variance has been reduced by many factors. This means that a desired level of MCSE could be achieved with many fewer samples.&lt;/p>
&lt;p>One final demonstration, remember that &lt;span class="math inline">\(h(x)p_X(x)\)&lt;/span> describes a distribution. Hence it would be a mistake to try a &lt;span class="math inline">\(p_Y\)&lt;/span> that placed all of the density around that point of intersection. For example, let’s try &lt;span class="math inline">\(Y \sim N(2,0.1)\)&lt;/span>. Although that is centered on the region that contributes the largest values to the expectation, the largest values alone do not define the expectation; too much of the tails of &lt;span class="math inline">\(h(x)\)&lt;/span> are not included. Using this results is the worst variance.&lt;/p>
&lt;pre class="r">&lt;code>y &amp;lt;- rnorm(1e+06, mean = 2, 0.1)
h &amp;lt;- hx(y)
IS &amp;lt;- h * dnorm(y, 0)/dnorm(y, 2, 0.1)
mean(IS)&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] 0.002601838&lt;/code>&lt;/pre>
&lt;pre class="r">&lt;code>var(IS)&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] 0.006540712&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div id="other-references" class="section level1">
&lt;h1>Other References&lt;/h1>
&lt;ul>
&lt;li>&lt;a href="https://en.wikipedia.org/wiki/Gaussian_function#Integral_of_a_Gaussian_function">integral of Gaussian Function&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://people.math.umass.edu/~lr7q/m697u-spring2018/m697uhome.html">course notes&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.statlect.com/asymptotic-theory/importance-sampling">statlect&lt;/a>&lt;/li>
&lt;/ul>
&lt;/div>
&lt;div class="footnotes">
&lt;hr />
&lt;ol>
&lt;li id="fn1">&lt;p>this page is mostly a study page for upcoming comprehensive exams&lt;a href="#fnref1" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>Anonymizing MTurk WorkerIDs</title><link>https://psadil.github.io/psadil/post/anonymizing-mtruk-worker-ids/</link><pubDate>Tue, 07 Aug 2018 00:00:00 +0000</pubDate><guid>https://psadil.github.io/psadil/post/anonymizing-mtruk-worker-ids/</guid><description>
&lt;script src="https://psadil.github.io/psadil/post/anonymizing-mtruk-worker-ids/index.en_files/header-attrs/header-attrs.js">&lt;/script>
&lt;p>It may be the case that Amazon Mechanical Turk WorkerIDs are not anonymous. &lt;a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2228728">Lease et al., 2013&lt;/a> describe at length how personally identifying information may be exposed when a researcher shares WorkerIDs. It is unclear to me the extent to which Amazon constructs their WorkerIDs at present, given that one of their striking demonstrations did not apply to my WorkerID. That is, they describe simply googling the WorkerID and receiving a picture of the participant, along with their full name. My WorkerID turn up nothing. Though, I have only been a worker on MTurk for a short while, so maybe I’ve been lucky and my ID has just not yet been shared widely.&lt;/p>
&lt;p>Regardless, providing extra anonymity to participants isn’t too much trouble. This post serves as documentation for a brief script that takes a sqlite database produced by running an experiment in jspsych + psiturk and replaces all instances of the WorkerID with a more secure code.&lt;/p>
&lt;p>The script relies on five R libraries&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;a href="https://magrittr.tidyverse.org">magrittr&lt;/a>, for ease of writing&lt;/li>
&lt;li>&lt;a href="https://dplyr.tidyverse.org">dplyr&lt;/a>, through (dbplyr)[cran.r-project.org/web/packages/dbplyr/vignettes/dbplyr.html], serves as the way to interface with the sqlite database&lt;/li>
&lt;li>&lt;a href="https://github.com/jeroen/openssl#readme">openssl&lt;/a> constructs a more secure identifier for each participant that can still be used to cross reference them across studies&lt;/li>
&lt;li>&lt;a href="https://stringr.tidyverse.org">stringr&lt;/a> does the work of replacing instances of the WorkerID with the more secure code&lt;/li>
&lt;li>&lt;a href="https://github.com/docopt/docopt.R">docopt&lt;/a>, wraps up the Rscript such that it can be called from the command line (in an environment in which Rscript is the name of a function. i.e., may be Rscript.exe in Windows powershell)&lt;/li>
&lt;/ol>
&lt;pre class="r">&lt;code>#!/usr/bin/env Rscript
# Anonymize participants database NOTE: always overrides the file --outfile
library(docopt)
doc &amp;lt;- &amp;quot;Usage:
anonymize_db.R [-i DBNAME] KEY OUTFILE
anonymize_db.R -h
Options:
-i --infile DBNAME sqlite database filename from which to read [default: participants_raw.db]
-t --table TABLE name of table within database to anonymize [default: participants]
-h --help show this help text
Arguments:
KEY key to salt WorkerIDs for extra security
OUTFILE sqlite database filename to write&amp;quot;
opt &amp;lt;- docopt(doc)
library(magrittr)
library(dplyr)
library(stringr)
library(openssl)
db &amp;lt;- dplyr::src_sqlite(opt$infile) %&amp;gt;%
dplyr::tbl(opt$table) %&amp;gt;%
dplyr::collect() %&amp;gt;%
dplyr::mutate(uniqueid = stringr::str_replace(uniqueid, workerid, openssl::sha256(workerid,
key = opt$KEY)), datastring = dplyr::case_when(is.na(datastring) ~ datastring,
TRUE ~ stringr::str_replace_all(datastring, workerid, openssl::sha256(workerid,
key = opt$KEY))), workerid = openssl::sha256(workerid, key = opt$KEY))
message(paste0(&amp;quot;read raw database: &amp;quot;, opt$infile))
con &amp;lt;- DBI::dbConnect(RSQLite::SQLite(), opt$OUTFILE)
dplyr::copy_to(con, db, opt$table, temporary = FALSE, indexes = list(&amp;quot;uniqueid&amp;quot;),
overwrite = TRUE)
DBI::dbDisconnect(con)
message(paste0(&amp;quot;wrote anonymized database: &amp;quot;, opt$OUTFILE))
message(paste0(&amp;quot;Store your KEY securely if you want the same WorkerIDs to create the same HMACs!&amp;quot;))&lt;/code>&lt;/pre>
&lt;p>As stated in the initial string of this script, a typical call might be&lt;/p>
&lt;p>&lt;code>anonymize_db.R longandsecurelystoredsalt participants.db&lt;/code>&lt;/p>
&lt;p>which will read in the sqlite database &lt;code>participants_raw.db&lt;/code> (default for –infile), convert all instances of WorkerID into a hash-digest with the sha256 algorithm, and store the result in a new sqlite database called &lt;code>participants.db&lt;/code>.&lt;/p>
&lt;p>The general workflow would be to include in your .gitignore the raw database output by psiturk. That way, the raw database is never uploaded into any repository. Then, when you are ready to host the experiment, you pull your repository as usual. As an extra step, you will now need to separately move around your raw database such that when you run the next experiment psiturk will know which workers have already participated. After collecting data, retrieve the database and run this anonymization script on it. The newly created database can then be bundled with your repository.&lt;/p>
&lt;p>This is a bit of extra work (i.e., you must manually send the database, retrieve the database, then anonymize it). However, the whole point is to avoid making it easy to download something with potentially identifying information.&lt;/p>
&lt;div id="gotchas" class="section level1">
&lt;h1>Gotchas&lt;/h1>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;p>This function will overwrite any database of the same name as OUTFILE. Though, that’s often not an issue. If you’ve anonymized a database (call the result &lt;code>participants.db&lt;/code>), added new participants to the same raw database, and then anonymize the raw database again, those participants that were anonymized in the first round will be re-anonymized and included in the new result.&lt;/p>&lt;/li>
&lt;li>&lt;p>If you want this function to convert a given WorkerID into a consistent code, you’ll need to call it with the same value for KEY.&lt;/p>&lt;/li>
&lt;li>&lt;p>It would be more secure to use a salt of random length for each participant separately.&lt;/p>&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>Privacy Policy</title><link>https://psadil.github.io/psadil/privacy/</link><pubDate>Thu, 28 Jun 2018 00:00:00 +0100</pubDate><guid>https://psadil.github.io/psadil/privacy/</guid><description>&lt;p>&amp;hellip;&lt;/p></description></item><item><title>Terms</title><link>https://psadil.github.io/psadil/terms/</link><pubDate>Thu, 28 Jun 2018 00:00:00 +0100</pubDate><guid>https://psadil.github.io/psadil/terms/</guid><description>&lt;p>&amp;hellip;&lt;/p></description></item><item><title>eyetracking with eyelink in psychtoolbox</title><link>https://psadil.github.io/psadil/post/eyetracking-init/</link><pubDate>Tue, 05 Jun 2018 00:00:00 +0000</pubDate><guid>https://psadil.github.io/psadil/post/eyetracking-init/</guid><description>
&lt;script src="https://psadil.github.io/psadil/post/eyetracking-init/index.en_files/header-attrs/header-attrs.js">&lt;/script>
&lt;p>UPDATE: I now think that the examples I’ve presented here obscure the interface with Eyelink. Much cleaner to use MATLAB’s object oriented programming. This is covered in &lt;a href="https://psadil.github.io/psadil/post/eyetracking-in-psychtoolbox-oop/">another post&lt;/a>.&lt;/p>
&lt;p>This post is designed as minimal documentation for using the Eyelink software at the UMass Amherst &lt;a href="https://www.umass.edu/ials/hmrc">hMRC&lt;/a>. The goals are very modest&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>Provide sample &lt;a href="http://psychtoolbox.org">Psychtoolbox&lt;/a> (PTB) and MATLAB code for integrating eyelink&lt;/li>
&lt;li>Explain a few parameters that you might want to change in your experiment&lt;/li>
&lt;/ol>
&lt;p>The main audience includes members of the cMAP and CEMNL labs at UMass, but other users of the hMRC may also benefit. This post includes various lines of code throughout this post, but the full files can be downloaded from the links at the bottom. Many of those links are private and will only work if you are a member of one of those labs.&lt;/p>
&lt;p>NOTE: This post is not designed to be a full introduction to the Eyelink toolbox within PTB. I’m not qualified to give a detailed tutorial. These are just a few bits of code that I have found useful. But, my needs have so far been really simple (i.e., make a record of where the eyes were during each run so that runs can be discarded if fixations during that run deviate more than x degrees from the center of the screen). The main resource in this post is probably the collection of links in the next section.&lt;/p>
&lt;div id="background-links-installing-extra-software" class="section level1">
&lt;h1>Background links + installing extra software&lt;/h1>
&lt;p>You’ll need to download the Eyelink API provided by SR Research. To do that, register an account &lt;a href="https://www.sr-support.com">here&lt;/a>. Note that they moderate the accounts fairly heavily, so it may take 24 hrs+ for the registration to go though. Once you’re registered, you can download the developers kit API ( &lt;a href="https://www.sr-support.com/forum/downloads/eyelink-display-software/39-eyelink-developers-kit-for-windows-windows-display-software">Windows&lt;/a>, &lt;a href="https://www.sr-support.com/forum/downloads/eyelink-display-software/46-eyelink-developers-kit-for-linux-linux-display-software">Linux&lt;/a> ). You’ll need that kit to be able to call Eyelink functions from within matlab (otherwise you get an error about missing mex files whenever you search for help pages). Registering also gives access to a support forum.&lt;/p>
&lt;p>Before moving to the next session, it may make sense to look through their &lt;a href="https://www.sr-support.com/forum/downloads/manuals">manuals&lt;/a>. If you have access to our box folder, here’s a link to the relevant &lt;a href="https://umass.box.com/s/1nr9m302wqn5l2jd9kaf9guv8ngqa9wp">Eyelink II manual&lt;/a> and the &lt;a href="https://umass.box.com/s/n8ki3br7watw2niuangxxflj6ulpnk67q">Data Viewer&lt;/a>. The manuals are, well, manuals, but reading through them takes less time than their length might suggest. If you are not a member of our lab, you may be able to ask a member of the hMRC to share the manuals.&lt;/p>
&lt;p>Without a licensing key, the version of the data viewer that can be downloaded is more or less useless (but, &lt;a href="https://www.sr-support.com/forum/downloads/data-analysis/4557-eyelink-data-viewer?4434-EyeLink-Data-Viewer=">here it is&lt;/a>). Instead, for working with the data in R, see &lt;a href="https://github.com/jashubbard/edfR">edfR&lt;/a> and &lt;a href="https://github.com/jashubbard/itrackR">itrackR&lt;/a>. Note that these are only working on Mac and Linux. So, you may need to be working on the server to install / use those libraries. Alternatively, you can also read the edf files directly into matlab using &lt;a href="https://www.sr-support.com/forum/downloads/data-analysis/5446-edfmex-reading-edf-data-directly-into-matlab">EDFMEX&lt;/a>. However, I won’t be able to help much with using these packages, given that I only discovered them while writing this post.&lt;/p>
&lt;p>Kwan-Jin Jung wrote a technical note about the eyetracking system, &lt;a href="https://www.umass.edu/ials/sites/default/files/hmrc_tn_eye_monitoring_during_fmri_scan.pdf">see here&lt;/a>, and here’s the &lt;a href="https://www.sr-research.com/products/eyelink-1000-plus/#LongRangeMount">advertisement for our tracker&lt;/a>.&lt;/p>
&lt;/div>
&lt;div id="sec:init" class="section level1">
&lt;h1>Initializing Eyelink&lt;/h1>
&lt;p>This section walks through a function that initializes the eyelink system. The first step to interfacing with the Eyelink is to call the PTB command &lt;a href="https://web.archive.org/web/20171214112707/http://docs.psychtoolbox.org/EyelinkInitDefaults">&lt;code>EyelinkInitDefaults&lt;/code>&lt;/a>. This defines a struct with a number of default parameters, &lt;code>el&lt;/code> about how the eyetracker will operate. I generally don’t want all of those defaults, so the function below modifies them as needed. After the parameters in &lt;code>el&lt;/code> have been modified, this function calls &lt;a href="https://web.archive.org/web/20171214035622/http://docs.psychtoolbox.org/EyelinkUpdateDefaults">&lt;code>EyelinkUpdateDefaults(el)&lt;/code>&lt;/a> to indicate to inform the eyelink system that the parameters should change.&lt;/p>
&lt;p>The main other point of this function is to start the eyetracker calibration. That should be done at the start of each run.&lt;/p>
&lt;pre class="matlab">&lt;code>
function [el, exit_flag] = setupEyeTracker( tracker, window, constants )
% SET UP TRACKER CONFIGURATION. Main goal is to modify defaults set in EyelinkInitDefaults.
%{
REQUIRED INPUT:
tracker: string, either &amp;#39;none&amp;#39; or &amp;#39;T60&amp;#39;
window: struct containing at least the fields
window.background: background color (whatever was set during call to e.g., PsychImaging(&amp;#39;OpenWindow&amp;#39;, window.screenNumber, window.background))
window.white: numeric defining the color white for the open window (e.g., window.white = WhiteIndex(window.screenNumber);)
window.pointer: scalar pointing to main screen (e.g., [window.pointer, window.winRect] = PsychImaging(&amp;#39;OpenWindow&amp;#39;, ...
window.screenNumber,window.background);)
window.winRect; PsychRect defining size of main window (e.g., [window.pointer, window.winRect] = PsychImaging(&amp;#39;OpenWindow&amp;#39;, ...
window.screenNumber,window.background);)
constants: struct containing at least
constants.eyelink_data_fname: string defining eyetracking data to be saved. Cannot be longer than 8 characters (before file extention). File extension must be &amp;#39;.edf&amp;#39;. (e.g., constants.eyelink_data_fname = [&amp;#39;scan&amp;#39;, num2str(input.runnum, &amp;#39;%02d&amp;#39;), &amp;#39;.edf&amp;#39;];)
OUTPUT:
if tracker == &amp;#39;T60&amp;#39;
el: struct defining parameters that have been set up about the eyetracker (see EyelinkInitDefaults)
if tracker == &amp;#39;none&amp;#39;
el == []
exit_flag: string that can be used to check whether this function exited successfully
SIDE EFFECTS:
When tracker == &amp;#39;T60&amp;#39;, calibration is started
%}
%%
exit_flag = &amp;#39;OK&amp;#39;;
switch tracker
case &amp;#39;T60&amp;#39;
% Provide Eyelink with details about the graphics environment
% and perform some initializations. The information is returned
% in a structure that also contains useful defaults
% and control codes (e.g. tracker state bit and Eyelink key values).
el = EyelinkInitDefaults(window.pointer);
% overrride default gray background of eyelink, otherwise runs end
% up gray! also, probably best to calibrate with same colors of
% background / stimuli as participant will encounter
el.backgroundcolour = window.background;
el.foregroundcolour = window.white;
el.msgfontcolour = window.white;
el.imgtitlecolour = window.white;
el.calibrationtargetcolour=[window.white window.white window.white];
EyelinkUpdateDefaults(el);
if ~EyelinkInit(0, 1)
fprintf(&amp;#39;\n Eyelink Init aborted \n&amp;#39;);
exit_flag = &amp;#39;ESC&amp;#39;;
return;
end
%Reduce FOV
Eyelink(&amp;#39;command&amp;#39;,&amp;#39;calibration_area_proportion = 0.5 0.5&amp;#39;);
Eyelink(&amp;#39;command&amp;#39;,&amp;#39;validation_area_proportion = 0.48 0.48&amp;#39;);
% open file to record data to
i = Eyelink(&amp;#39;Openfile&amp;#39;, constants.eyelink_data_fname);
if i ~= 0
fprintf(&amp;#39;\n Cannot create EDF file \n&amp;#39;);
exit_flag = &amp;#39;ESC&amp;#39;;
return;
end
Eyelink(&amp;#39;command&amp;#39;, &amp;#39;add_file_preamble_text &amp;#39;&amp;#39;Recorded by NAME OF EXPERIMENT&amp;#39;&amp;#39;&amp;#39;);
% Setting the proper recording resolution, proper calibration type,
% as well as the data file content;
Eyelink(&amp;#39;command&amp;#39;,&amp;#39;screen_pixel_coords = %ld %ld %ld %ld&amp;#39;, 0, 0, window.winRect(3)-1, window.winRect(4)-1);
Eyelink(&amp;#39;message&amp;#39;, &amp;#39;DISPLAY_COORDS %ld %ld %ld %ld&amp;#39;, 0, 0, window.winRect(3)-1, window.winRect(4)-1);
% set calibration type.
Eyelink(&amp;#39;command&amp;#39;, &amp;#39;calibration_type = HV5&amp;#39;);
% set EDF file contents using the file_sample_data and
% file-event_filter commands
% set link data thtough link_sample_data and link_event_filter
Eyelink(&amp;#39;command&amp;#39;, &amp;#39;file_event_filter = LEFT,RIGHT,FIXATION,SACCADE,BLINK,MESSAGE,BUTTON,INPUT&amp;#39;);
Eyelink(&amp;#39;command&amp;#39;, &amp;#39;link_event_filter = LEFT,RIGHT,FIXATION,SACCADE,BLINK,MESSAGE,BUTTON,INPUT&amp;#39;);
% check the software version
% add &amp;quot;HTARGET&amp;quot; to record possible target data for EyeLink Remote
Eyelink(&amp;#39;command&amp;#39;, &amp;#39;file_sample_data = LEFT,RIGHT,GAZE,HREF,GAZERES,AREA,HTARGET,STATUS,INPUT&amp;#39;);
Eyelink(&amp;#39;command&amp;#39;, &amp;#39;link_sample_data = LEFT,RIGHT,GAZE,HREF,GAZERES,AREA,HTARGET,STATUS,INPUT&amp;#39;);
% make sure we&amp;#39;re still connected.
if Eyelink(&amp;#39;IsConnected&amp;#39;)~=1 &amp;amp;&amp;amp; input.dummymode == 0
exit_flag = &amp;#39;ESC&amp;#39;;
return;
end
% possible changes from EyelinkPictureCustomCalibration
% set sample rate in camera setup screen
Eyelink(&amp;#39;command&amp;#39;, &amp;#39;sample_rate = %d&amp;#39;, 1000);
% Will call the calibration routine
EyelinkDoTrackerSetup(el);
case &amp;#39;none&amp;#39;
el = [];
end
end
&lt;/code>&lt;/pre>
&lt;p>Here are a few parts of that function that you will probably want to adapt for your experiment.&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>The various color arguments&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>Eyelink changes the background color of whatever screen is open. So, these colors (e.g., &lt;code>el.backgroundcolour&lt;/code>) should match whatever background your stimuli will be displayed on.&lt;/li>
&lt;/ul>
&lt;ol start="2" style="list-style-type: decimal">
&lt;li>&lt;code>Eyelink('command','calibration_area_proportion = 0.5 0.5');&lt;/code> and &lt;code>Eyelink('command','validation_area_proportion = 0.48 0.48');&lt;/code>&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>The setup at the scanner has a hard time tracking eyes that are fixating near the edges of the screen. The issue is bad enough that it can be almost impossible to calibrate the tracker when the calibration dots appear on the edges. I only really use the eyetracker to have a record confirming that participants were more-or-less fixating during a run, so good calibration at the edges isn’t important to me. For this reason, I reduce the size of the calibration.&lt;/li>
&lt;/ul>
&lt;ol start="3" style="list-style-type: decimal">
&lt;li>Related to 2: &lt;code>Eyelink('command', 'calibration_type = HV5');&lt;/code>&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>This sets the calibration routine to only use 5 dots, rather than 9. Again, my needs are pretty simple and calibration can be challenging, so 5 seems good enough.&lt;/li>
&lt;/ul>
&lt;ol start="4" style="list-style-type: decimal">
&lt;li>Wrapping the function in a switch argument (e.g., &lt;code>tracker ==&lt;/code>)&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>See the next section for some of the logic in writing code with a switch statement or two that all depends on how an initial variable is set&lt;a href="#fn1" class="footnote-ref" id="fnref1">&lt;sup>1&lt;/sup>&lt;/a>.&lt;/li>
&lt;/ul>
&lt;/div>
&lt;div id="the-eyelink-functions" class="section level1">
&lt;h1>The Eyelink functions&lt;/h1>
&lt;p>In &lt;code>setupEyeTracker&lt;/code>, you may have noticed many calls that took the following format &lt;a href="https://web.archive.org/web/20171214045939/http://docs.psychtoolbox.org/Eyelink">&lt;code>Eyelink('dosomethingspecial');&lt;/code>&lt;/a>. Commands like these are PTB’s way of communicating with the Eyelink software.&lt;/p>
&lt;p>There are a few such functions that you’ll need to include to record any usable data. First, the function we defined above, &lt;code>setupEyeTracker&lt;/code>, called the function &lt;a href="https://web.archive.org/web/20171214112703/http://docs.psychtoolbox.org/EyelinkDoTrackerSetup">&lt;code>EyelinkDoTrackerSetup(el)&lt;/code>&lt;/a>. This is a function internal to PTB. It runs the calibration routine. So, you’ll want a call to &lt;code>[el, exitflag] = setupEyeTracker( input.tracker, window, constants );&lt;/code> somewhere early in your code. I rerun the calibration at the start of each experimental run.&lt;/p>
&lt;p>Next, the following commands make sure that you’ve turned on the eyetracker&lt;/p>
&lt;pre class="matlab">&lt;code>% Must be offline to draw to EyeLink screen
Eyelink(&amp;#39;Command&amp;#39;, &amp;#39;set_idle_mode&amp;#39;);
% clear tracker display
Eyelink(&amp;#39;Command&amp;#39;, &amp;#39;clear_screen 0&amp;#39;);
Eyelink(&amp;#39;StartRecording&amp;#39;);
% always wait a moment for recording to have definitely started
WaitSecs(0.1);&lt;/code>&lt;/pre>
&lt;p>Eyelink will save it’s files in a specialized format&lt;a href="#fn2" class="footnote-ref" id="fnref2">&lt;sup>2&lt;/sup>&lt;/a>. For that file, it’s useful to mark when the experiment has actually started. So, include a command like&lt;/p>
&lt;pre class="matlab">&lt;code>Eyelink(&amp;#39;message&amp;#39;, &amp;#39;SYNCTIME&amp;#39;);&lt;/code>&lt;/pre>
&lt;p>to mark the start. Since this will probably be run in the scanner, a sensible time to place that would be shortly after receiving the scanner trigger, but before the next flip.&lt;/p>
&lt;p>When you’re done with the experiment run &lt;a href="https://web.archive.org/web/20171214045939/http://docs.psychtoolbox.org/Eyelink">&lt;code>Eyelink('Command', 'set_idle_mode');&lt;/code>&lt;/a> before saving data. Here’s an example of a short routine to save the data. I’ve defined a variable &lt;code>constants.eyelink_data_fname&lt;/code> to be a string that ends in ‘.edf’. Note that the filename can be no longer than 8 characters and cannot contain any special characters (only digits and letters).&lt;/p>
&lt;pre class="matlab">&lt;code>% the Eyelink(&amp;#39;ReceiveFile&amp;#39;) function does not wait for the file
% transfer to complete so you must have the entire try loop
% surrounding the function to ensure complete transfer of the EDF.
try
fprintf(&amp;#39;Receiving data file &amp;#39;&amp;#39;%s&amp;#39;&amp;#39;\n&amp;#39;, constants.eyelink_data_fname );
status = eyetrackerFcn(&amp;#39;ReceiveFile&amp;#39;);
if status &amp;gt; 0
fprintf(&amp;#39;ReceiveFile status %d\n&amp;#39;, status);
end
if 2==exist(edfFile, &amp;#39;file&amp;#39;)
fprintf(&amp;#39;Data file &amp;#39;&amp;#39;%s&amp;#39;&amp;#39; can be found in &amp;#39;&amp;#39;%s&amp;#39;&amp;#39;\n&amp;#39;, constants.eyelink_data_fname, pwd );
end
catch
fprintf(&amp;#39;Problem receiving data file &amp;#39;&amp;#39;%s&amp;#39;&amp;#39;\n&amp;#39;, constants.eyelink_data_fname );
end
&lt;/code>&lt;/pre>
&lt;/div>
&lt;div id="sample-script" class="section level1">
&lt;h1>Sample script&lt;/h1>
&lt;p>Unfortunately, attempting to call these function from a computer that does not have Eyelink’s software installed will produce an error. This makes developing and testing an experimental script challenging, because if we litter our code with calls to &lt;a href="https://web.archive.org/web/20171214045939/http://docs.psychtoolbox.org/Eyelink">&lt;code>Eyelink(...)&lt;/code>&lt;/a>, then when we’re not at the scanner computer we need to comment out all of those lines. I have no faith that I’ll remember to uncomment all of these lines when I’m at the scanner each time, so when I’m writing code that calls these functions I place them in a wrapper. Credit goes to &lt;a href="https://people.umass.edu/whopper/">Will Hopper&lt;/a> for showing me this strategy when designing functions that receive input.&lt;/p>
&lt;p>The main idea is two wrap all calls to &lt;code>Eyelink(...)&lt;/code> with a function that starts like this&lt;/p>
&lt;pre class="matlab">&lt;code>
function eyelinkFcn = makeEyelinkFcn(handlerName)
valid_types = {&amp;#39;none&amp;#39;,&amp;#39;T60&amp;#39;};
assert(ismember(handlerName, valid_types),...
[&amp;#39;&amp;quot;handlerType&amp;quot; argument must be one of the following: &amp;#39; strjoin(valid_types,&amp;#39;, &amp;#39;)])
switch handlerName
case &amp;#39;T60&amp;#39;
eyelinkFcn = @T60;
case &amp;#39;none&amp;#39;
eyelinkFcn = @do_nothing;
end
% more code to follow
end
&lt;/code>&lt;/pre>
&lt;p>The outer function, &lt;code>makeEyelinkFcn&lt;/code> receives as input the variable &lt;code>handlerName&lt;/code>, which can be either &lt;code>none&lt;/code> or &lt;code>T60&lt;/code>. Depending on that variable, the output to eyelinkFcn is then a call to an anonymous function which implements the actual calls to Eyelink. When &lt;code>handlerName == 'T60'&lt;/code>, &lt;code>makeEyelinkFcn&lt;/code> returns a function that is going to try to call various &lt;code>Eyelink(...)&lt;/code> routines (shown below). But, when &lt;code>handlerName == 'none'&lt;/code> &lt;code>makeEyelinkFcn&lt;/code> will return a function that does nothing.&lt;/p>
&lt;p>This enables the writing of code that will call the eyelink functions when desired (e.g., when at the scanner), but calls to those functions can also be avoided when desired (by calling &lt;code>makeEyeLinkFcn('none')&lt;/code> instead of &lt;code>makeEyeLinkFcn('T60')&lt;/code>).&lt;/p>
&lt;pre class="matlab">&lt;code>
% ...
eyetrackerFcn = makeEyelinkFcn(input.tracker);
eyetrackerFcn(&amp;#39;message&amp;#39;, &amp;#39;SYNCTIME&amp;#39;);
% ...
&lt;/code>&lt;/pre>
&lt;p>So long as input.tracker is taking different values, there’s no need to comment or uncomment when I’m working on a computer that has or doesn’t have an eyelink hooked up&lt;a href="#fn3" class="footnote-ref" id="fnref3">&lt;sup>3&lt;/sup>&lt;/a>.&lt;/p>
&lt;p>The remainder of this script defines the local function &lt;code>T60&lt;/code>, which allows all of the necessary wrapping to the different &lt;code>Eyelink(...)&lt;/code> commands.&lt;/p>
&lt;pre class="matlab">&lt;code>
function eyelinkFcn = makeEyelinkFcn(handlerName)
valid_types = {&amp;#39;none&amp;#39;,&amp;#39;T60&amp;#39;};
assert(ismember(handlerName, valid_types),...
[&amp;#39;&amp;quot;handlerType&amp;quot; argument must be one of the following: &amp;#39; strjoin(valid_types,&amp;#39;, &amp;#39;)])
switch handlerName
case &amp;#39;T60&amp;#39;
eyelinkFcn = @T60;
case &amp;#39;none&amp;#39;
eyelinkFcn = @do_nothing;
end
function status = T60(varargin)
status = [];
switch varargin{1}
case &amp;#39;EyelinkDoDriftCorrection&amp;#39;
% Do a drift correction at the beginning of each trial
% Performing drift correction (checking) is optional for
% EyeLink 1000 eye trackers.
EyelinkDoDriftCorrection(varargin{2},[],[],0);
case &amp;#39;Command&amp;#39;
Eyelink(&amp;#39;Command&amp;#39;, varargin{2})
case &amp;#39;ImageTransfer&amp;#39;
%transfer image to host
transferimginfo = imfinfo(varargin{2});
[width, height] = Screen(&amp;#39;WindowSize&amp;#39;, 0);
% image file should be 24bit or 32bit b5itmap
% parameters of ImageTransfer:
% imagePath, xPosition, yPosition, width, height, trackerXPosition, trackerYPosition, xferoptions
transferStatus = Eyelink(&amp;#39;ImageTransfer&amp;#39;,transferimginfo.Filename,...
0, 0, transferimginfo.Width, transferimginfo.Height, ...
width/2-transferimginfo.Width/2 ,height/2-transferimginfo.Height/2, 1);
if transferStatus ~= 0
fprintf(&amp;#39;*****Image transfer Failed*****-------\n&amp;#39;);
end
case &amp;#39;StartRecording&amp;#39;
Eyelink(&amp;#39;StartRecording&amp;#39;);
case &amp;#39;Message&amp;#39;
if nargin == 2
Eyelink(&amp;#39;Message&amp;#39;, varargin{2});
elseif nargin == 3
Eyelink(&amp;#39;Message&amp;#39;, varargin{2}, varargin{3});
elseif nargin == 4
Eyelink(&amp;#39;Message&amp;#39;, varargin{2}, varargin{3}, varargin{4});
end
case &amp;#39;StopRecording&amp;#39;
Eyelink(&amp;#39;StopRecording&amp;#39;);
case &amp;#39;CloseFile&amp;#39;
Eyelink(&amp;#39;CloseFile&amp;#39;);
case &amp;#39;ReceiveFile&amp;#39;
Eyelink(&amp;#39;ReceiveFile&amp;#39;);
case &amp;#39;EyeAvailable&amp;#39;
status = Eyelink(&amp;#39;EyeAvailable&amp;#39;);
end
end
function do_nothing(varargin)
% do nothing with arguments
end
end
&lt;/code>&lt;/pre>
&lt;/div>
&lt;div id="extra-resourcess" class="section level1">
&lt;h1>Extra Resourcess&lt;/h1>
&lt;p>For examples of these methods in action, check out &lt;a href="https://github.com/psadil/VTF">an experiment on Voxel Tuning Functions&lt;/a>. In particular, see &lt;a href="https://github.com/psadil/VTF/blob/master/lib/PsychSetup/setupEyeTracker.m">setupEyeTracker&lt;/a>, &lt;a href="https://github.com/psadil/VTF/blob/master/lib/PsychEyelink/makeEyelinkFcn.m">makeEyelinkFcn&lt;/a>. That repository also has examples of using the value returned by &lt;code>makeEyelinkFcn&lt;/code> in &lt;a href="https://github.com/psadil/VTF/blob/master/lib/PsychTasks/runContrast.m">runContrast&lt;/a>. Note that the repository may change from time to time and might not match the code in this post exactly. To download the exact files defined above, see &lt;a href="https://psadil.github.io/psadil/files/matlab/setupEyeTracker.m">setupEyeTracker&lt;/a>, &lt;a href="https://psadil.github.io/psadil/files/matlab/makeEyelinkFcn.m">makeEyelinkFcn&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://link.springer.com/article/10.3758/BF03195489">Here’s the original publication&lt;/a> that introduced the Eyelink interface to PTB.&lt;/p>
&lt;p>Also, for inspiration about the cool experiments that can be run with Eyelink’s software, see the &lt;a href="https://github.com/kleinerm/Psychtoolbox-3/tree/master/Psychtoolbox/PsychHardware/EyelinkToolbox/EyelinkDemos">PTB Demos&lt;/a>. See a list of &lt;code>Eyelink&lt;/code> functions &lt;a href="http://psychtoolbox.org/docs/EyelinkToolbox">here&lt;/a>. You’ll need to look at this page if you want access to the help files for these commands on a computer without Eyelink installed.&lt;/p>
&lt;p>Finally, thanks to &lt;a href="https://www.umass.edu/pbs/people/ramiro-reyes">Ramiro&lt;/a> for sharing a PTB script that got me started with Eyelink.&lt;/p>
&lt;/div>
&lt;div class="footnotes">
&lt;hr />
&lt;ol>
&lt;li id="fn1">&lt;p>though, I’ve already broken some of the logic I outline in that section by having more than one function with a switch statement.&lt;a href="#fnref1" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn2">&lt;p>The options relating to saving data are for another post. It seems like you can do quite a lot with the Eyelink Data Viewer when various event tags have been set up properly (see &lt;a href="https://umass.box.com/s/n8ki3br7watw2niuangxxflj6ulpk67q">manual, on box&lt;/a> ), but my needs are so simple that I haven’t bothered digging too deeply.&lt;a href="#fnref2" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn3">&lt;p>Of course, a similar effect could be achieved by littering the experimental code with a bunch of &lt;code>if then else&lt;/code> statements. However, this method has the advantage of massively reducing the number of switch statements in the code. Fewer switch statements can be easier to follow and modify, because most of the effect of the &lt;code>input.tracker&lt;/code> variable can be localized to a single function (the definition of &lt;code>makeEyelinkFcn&lt;/code>)&lt;a href="#fnref3" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>A hierarchical Bayesian model for inferring neural tuning functions from voxel tuning functions</title><link>https://psadil.github.io/psadil/publication/sadil-2018-hierarchical/</link><pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate><guid>https://psadil.github.io/psadil/publication/sadil-2018-hierarchical/</guid><description/></item><item><title>A computational model of perceptual and mnemonic deficits in medial temporal lobe amnesia</title><link>https://psadil.github.io/psadil/publication/sadil-2017-computational/</link><pubDate>Thu, 27 Apr 2017 00:00:00 +0000</pubDate><guid>https://psadil.github.io/psadil/publication/sadil-2017-computational/</guid><description/></item><item><title>Hippocampal engagement during recall depends on memory content</title><link>https://psadil.github.io/psadil/publication/ross-2017-hippocampal/</link><pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate><guid>https://psadil.github.io/psadil/publication/ross-2017-hippocampal/</guid><description/></item></channel></rss>