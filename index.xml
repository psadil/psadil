<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>psadil</title><link>https://psadil.github.io/psadil/</link><atom:link href="https://psadil.github.io/psadil/index.xml" rel="self" type="application/rss+xml"/><description>psadil</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© 2021 Patrick Sadil</copyright><lastBuildDate>Fri, 07 May 2021 00:00:00 +0000</lastBuildDate><image><url>https://psadil.github.io/psadil/media/icon_hu3896c2ce465988ba1fc8077f9a6388c6_268630_512x512_fill_lanczos_center_2.png</url><title>psadil</title><link>https://psadil.github.io/psadil/</link></image><item><title>Spurious Serial Dependencies</title><link>https://psadil.github.io/psadil/post/2021-05-07-spurious-serial-dependencies/</link><pubDate>Fri, 07 May 2021 00:00:00 +0000</pubDate><guid>https://psadil.github.io/psadil/post/2021-05-07-spurious-serial-dependencies/</guid><description>
&lt;script src="https://psadil.github.io/psadil/post/2021-05-07-spurious-serial-dependencies/index.en_files/header-attrs/header-attrs.js">&lt;/script>
&lt;pre class="r">&lt;code>knitr::opts_chunk$set(echo = TRUE)
set.seed(1234)
library(dplyr)
library(ggplot2)
library(tidyr)&lt;/code>&lt;/pre>
&lt;p>I’m working on a project involving &lt;a href="https://psadil.github.io/psadil/post/serial-dependence/">serial dependence&lt;/a>. The project involves disentangling a dependence on the previous orientation from a dependence on the previous response. Unfortunately, there is a common way for a dependence on the previous response to be spurious, due to the oblique effect&lt;a href="#fn1" class="footnote-ref" id="fnref1">&lt;sup>1&lt;/sup>&lt;/a>. The first reference I’ve seen for this is a master’s thesis by &lt;span class="citation">&lt;a href="#ref-fritsche2016smooth" role="doc-biblioref">Fritsche&lt;/a> (&lt;a href="#ref-fritsche2016smooth" role="doc-biblioref">2016&lt;/a>)&lt;/span>. I didn’t follow that explanation, and so I’m using this post to explain how the oblique effect causes a spurious dependence on the previous response.&lt;/p>
&lt;p>First, let’s show that the confound is real. Data will be generated with an oblique effect, and there will be no dependence between trials – neither on the previous orientation nor the previous response. There will be no response variability, meaning that errors will only be caused by the oblique effect. Since the data are simulated without dependencies, any dependence that emerges will necessarily be spurious.&lt;/p>
&lt;pre class="r">&lt;code># helper functions for converting between angles and degrees
rad &amp;lt;- function(degree) degree * pi / 180
deg &amp;lt;- function(radian) radian * 180 / pi
# magnitude of oblique effect
oblique &amp;lt;- rad(-22.5)
d0 &amp;lt;- tibble(
orientation = runif(5000, 0, pi)) %&amp;gt;%
mutate(
trial = 1:n(),
oblique = oblique*sin(orientation*4),
response = rnorm(n(), orientation, 0) + oblique) &lt;/code>&lt;/pre>
&lt;p>To help generate the data, define a helper functions that calculates the signed, shortest angle between two angles (measured in radians).&lt;/p>
&lt;pre class="r">&lt;code>#&amp;#39; @param deg1 numeric degree
#&amp;#39; @param deg2 numeric degree
#&amp;#39;
#&amp;#39; @return
#&amp;#39; signed difference between inputs, wrapped to +-pi
#&amp;#39; output is shortest distance to the first input
#&amp;#39;
#&amp;#39; @examples
#&amp;#39; # pi/2 is 45 degrees clockwise from pi, so the output is pi/2
#&amp;#39; ang_diff(pi/2, pi)
#&amp;#39; # pi is 45 degrees counterclockwise from pi/2, so the output is -pi/2
#&amp;#39; ang_diff(pi, pi/2)
#&amp;#39; # notice the discontinuity when the shortest angle switches direction
#&amp;#39; ang_diff(pi/2 - .01, 0)
#&amp;#39; ang_diff(pi/2 + .01, 0)
ang_diff &amp;lt;- function(deg1, deg2){
stopifnot(length(deg1) == length(deg2))
diff &amp;lt;- ( deg1 - deg2 + pi/2 ) %% pi - pi/2
out &amp;lt;- dplyr::if_else(diff &amp;lt; -pi/2, diff + pi, diff)
return(out)
}&lt;/code>&lt;/pre>
&lt;p>Then use the function &lt;code>ang_diff&lt;/code> to calculate errors, and to calculate the relative orientation difference between the current trial and either the previous orientation or the previous response.&lt;/p>
&lt;pre class="r">&lt;code>d &amp;lt;- d0 %&amp;gt;%
mutate(
prev_response = lag(response),
prev_orientation = lag(orientation),
error = ang_diff(orientation, response),
orientation_diff = ang_diff(orientation, prev_orientation),
response_diff = ang_diff(orientation, prev_response)) %&amp;gt;%
filter(trial &amp;gt; 1) %&amp;gt;%
mutate(across(where(is.double), deg))&lt;/code>&lt;/pre>
&lt;p>Plot errors as a function of the current orientation to confirm that there is an oblique effect.&lt;/p>
&lt;pre class="r">&lt;code>d %&amp;gt;%
ggplot(aes(x=orientation, y=error)) +
geom_point() +
geom_smooth(
formula = y ~ sin(rad(x)*4),
method = &amp;quot;lm&amp;quot;,
se = FALSE) +
scale_y_continuous(
breaks = c(-20, -10, 0, 10, 20),
labels = c(&amp;quot;CCW&amp;quot;, -10, 0, 10, &amp;quot;CW&amp;quot;)) +
scale_x_continuous(
name = &amp;quot;orientation on current trial&amp;quot;,
labels = c(0, 90, 180),
breaks = c(0, 90, 180))&lt;/code>&lt;/pre>
&lt;div class="figure">&lt;span id="fig:oblique">&lt;/span>
&lt;img src="https://psadil.github.io/psadil/post/2021-05-07-spurious-serial-dependencies/index.en_files/figure-html/oblique-1.png" alt="The simulated data exhibit a clear oblique effect." width="672" />
&lt;p class="caption">
Figure 1: The simulated data exhibit a clear oblique effect.
&lt;/p>
&lt;/div>
&lt;p>Is there a dependence on either the previous response or previous orientation?&lt;/p>
&lt;pre class="r">&lt;code>d %&amp;gt;%
pivot_longer(
cols=c(orientation_diff, response_diff),
names_to = &amp;quot;covariate&amp;quot;,
names_pattern = &amp;quot;(orientation|response)&amp;quot;,
values_to = &amp;quot;x&amp;quot;) %&amp;gt;%
ggplot(aes(x=x, y=error)) +
geom_point() +
facet_wrap(~covariate) +
geom_smooth(
method = &amp;quot;gam&amp;quot;,
formula = y ~ s(x, bs = &amp;quot;cc&amp;quot;, k=9)) +
scale_x_continuous(
name = &amp;quot;relative orientation/response on previous trial&amp;quot;,
labels = c(-90, 0, 90),
breaks = c(-90, 0, 90))&lt;/code>&lt;/pre>
&lt;div class="figure">&lt;span id="fig:spurious">&lt;/span>
&lt;img src="https://psadil.github.io/psadil/post/2021-05-07-spurious-serial-dependencies/index.en_files/figure-html/spurious-1.png" alt="Errors do not depend on the previous orientation, but they do depend on the previous response. Since the data were generated without that dependence, it must be spurious." width="672" />
&lt;p class="caption">
Figure 2: Errors do not depend on the previous orientation, but they do depend on the previous response. Since the data were generated without that dependence, it must be spurious.
&lt;/p>
&lt;/div>
&lt;p>What’s going on? There are two key factors: first, the oblique effect operates on the previous trial to make some previous responses more likely than others, and second the oblique effect operates on the current trial to make certain previous responses more likely to have errors in a consistent direction. To be precise, I’ll use the following terminology.&lt;/p>
&lt;div id="terminology" class="section level2">
&lt;h2>Terminology&lt;/h2>
&lt;p>Trials will be indexed by natural numbers. The “current trial” will be referred to as trial &lt;span class="math inline">\(n\)&lt;/span>, and the “previous trial” as trial &lt;span class="math inline">\(n-1\)&lt;/span>. The orientation and responses on each trial will be thought of as sequences&lt;a href="#fn2" class="footnote-ref" id="fnref2">&lt;sup>2&lt;/sup>&lt;/a>. The variable &lt;span class="math inline">\(O_n\)&lt;/span> means the orientation on trial &lt;span class="math inline">\(n\)&lt;/span> (i.e., the current trial), whereas the variable &lt;span class="math inline">\(O_{n-1}\)&lt;/span> means the orientation on trial &lt;span class="math inline">\(n-1\)&lt;/span> (i.e., the previous trial). Similarly, the variable &lt;span class="math inline">\(R_n\)&lt;/span> means the response on trial &lt;span class="math inline">\(n\)&lt;/span>, whereas the variable &lt;span class="math inline">\(R_{n-1}\)&lt;/span> means the response on trial &lt;span class="math inline">\(n-1\)&lt;/span>.&lt;/p>
&lt;p>All angles (e.g., &lt;span class="math inline">\(O_n\)&lt;/span> and &lt;span class="math inline">\(R_n\)&lt;/span>) use the convention that &lt;span class="math inline">\(0^\circ\)&lt;/span> is horizontal, &lt;span class="math inline">\(45^\circ\)&lt;/span> is one quarter rotation counterclockwise from horizontal (e.g., at 1:30 on a clock), &lt;span class="math inline">\(90^\circ\)&lt;/span> is vertical, etc. However, differences between angles are reported such that a positive value implies a clockwise shift (i.e., moving forward on the clock) and a negative value implies a counterclockwise shift. For example, an error of &lt;span class="math inline">\(10^\circ\)&lt;/span> means that &lt;span class="math inline">\(R_n\)&lt;/span> is &lt;span class="math inline">\(10^\circ\)&lt;/span> clockwise from &lt;span class="math inline">\(O_n\)&lt;/span>. This means that we can determine an “attraction” effect based on whether the sign of the error on trial &lt;span class="math inline">\(n\)&lt;/span> matches the sign of the difference between &lt;span class="math inline">\(O_n\)&lt;/span> and either &lt;span class="math inline">\(O_{n-1}\)&lt;/span> or &lt;span class="math inline">\(R_{n-1}\)&lt;/span>. Conversely, a “repulsive” effect is when the error and differences have mismatched signs.&lt;/p>
&lt;/div>
&lt;div id="explanation" class="section level2">
&lt;h2>Explanation&lt;/h2>
&lt;p>First, consider a specific sequence of trials that could produce a spurious effect. To help with the explanation, the trials are colored based on the current trial.&lt;/p>
&lt;pre class="r">&lt;code>d %&amp;gt;%
pivot_longer(
cols=c(orientation_diff, response_diff),
names_to = &amp;quot;covariate&amp;quot;,
names_pattern = &amp;quot;(orientation|response)&amp;quot;,
values_to = &amp;quot;x&amp;quot;) %&amp;gt;%
ggplot(aes(x=x, y=error)) +
geom_point(aes(color=oblique)) +
scale_color_gradient2(low = scales::muted(&amp;quot;blue&amp;quot;), high = scales::muted(&amp;quot;red&amp;quot;)) +
facet_wrap(~covariate) +
geom_smooth(
method = &amp;quot;gam&amp;quot;,
formula = y ~ s(x, bs = &amp;quot;cc&amp;quot;, k=9)) +
scale_y_continuous(
breaks = c(-20, -10, 0, 10, 20),
labels = c(&amp;quot;CCW&amp;quot;, -10, 0, 10, &amp;quot;CW&amp;quot;)) +
scale_x_continuous(
name = &amp;quot;relative orientation/response on previous trial&amp;quot;,
labels = c(-90, 0, 90),
breaks = c(-90, 0, 90))&lt;/code>&lt;/pre>
&lt;div class="figure">&lt;span id="fig:spuriouscol">&lt;/span>
&lt;img src="https://psadil.github.io/psadil/post/2021-05-07-spurious-serial-dependencies/index.en_files/figure-html/spuriouscol-1.png" alt="Same figure as above, but colored based on the magnitude and direction of the oblique effect." width="672" />
&lt;p class="caption">
Figure 3: Same figure as above, but colored based on the magnitude and direction of the oblique effect.
&lt;/p>
&lt;/div>
&lt;p>When &lt;span class="math inline">\(O_{n-1}\)&lt;/span> is &lt;span class="math inline">\(0^\circ\)&lt;/span>, the oblique effect will have not caused an error. So, for &lt;span class="math inline">\(R_{n-1}\)&lt;/span> to be &lt;span class="math inline">\(22.5^\circ\)&lt;/span> clockwise to &lt;span class="math inline">\(O_n\)&lt;/span>, then &lt;span class="math inline">\(O_n\)&lt;/span> could be, itself &lt;span class="math inline">\(22.5^\circ\)&lt;/span>. But when &lt;span class="math inline">\(O_n\)&lt;/span> is &lt;span class="math inline">\(22.5^\circ\)&lt;/span>, the oblique effect will cause an error; &lt;span class="math inline">\(R_n\)&lt;/span> will be a clockwise error, in the same direction as &lt;span class="math inline">\(R_{n-1}\)&lt;/span>. Since &lt;span class="math inline">\(R_n\)&lt;/span> exhibits an error in the direction of &lt;span class="math inline">\(R_{n-1}\)&lt;/span>, it will look like &lt;span class="math inline">\(R_{n-1}\)&lt;/span> caused an attraction.&lt;/p>
&lt;p>More importantly, when the oblique effect acts on trial &lt;span class="math inline">\(n-1\)&lt;/span>, it will cause responses to collect along the cardinal axes. That is, regardless of the orientation on trial &lt;span class="math inline">\(n-1\)&lt;/span>, &lt;span class="math inline">\(R_{n-1}\)&lt;/span> will be close to either &lt;span class="math inline">\(0^\circ\)&lt;/span> or &lt;span class="math inline">\(90^\circ\)&lt;/span>. This means that, whenever &lt;span class="math inline">\(O_n\)&lt;/span> is close to &lt;span class="math inline">\(22.5^\circ\)&lt;/span>, the oblique effect’s influence on the previous response, &lt;span class="math inline">\(R_{n-1}\)&lt;/span>, makes it more likely that &lt;span class="math inline">\(R_{n-1}\)&lt;/span> will be approximately &lt;span class="math inline">\(22.5^\circ\)&lt;/span> clockwise from &lt;span class="math inline">\(O_n\)&lt;/span>, and then the oblique effect on trial &lt;span class="math inline">\(n\)&lt;/span> will further push the response toward &lt;span class="math inline">\(R_{n-1}\)&lt;/span>.&lt;/p>
&lt;p>We can see this play out empirically by looking at &lt;span class="math inline">\(O_n\)&lt;/span> as a function of &lt;span class="math inline">\(R_{n-1}\)&lt;/span>; when &lt;span class="math inline">\(R_{n-1}\)&lt;/span> is close to &lt;span class="math inline">\(22.5^\circ\)&lt;/span>, there is an over-representation of orientations for which the oblique effect will bias responses toward the previous response.&lt;/p>
&lt;pre class="r">&lt;code>d %&amp;gt;%
filter(between(response_diff, 21, 24)) %&amp;gt;%
ggplot(aes(x=orientation)) +
geom_histogram(bins=30) +
scale_x_continuous(
name = &amp;quot;orientation on current trial&amp;quot;,
labels = c(0, 90, 180),
breaks = c(0, 90, 180)) +
geom_vline(xintercept = c(22.5, 112.5), color=&amp;quot;blue&amp;quot;) &lt;/code>&lt;/pre>
&lt;div class="figure">&lt;span id="fig:dep">&lt;/span>
&lt;img src="https://psadil.github.io/psadil/post/2021-05-07-spurious-serial-dependencies/index.en_files/figure-html/dep-1.png" alt="When the previous orientation is 22.5 clockwise, the current orientation tends to be either 22.5 or 112.5, which is when the oblique effect causes maximal error." width="672" />
&lt;p class="caption">
Figure 4: When the previous orientation is 22.5 clockwise, the current orientation tends to be either 22.5 or 112.5, which is when the oblique effect causes maximal error.
&lt;/p>
&lt;/div>
&lt;p>We can think about this from the other direction, too; when &lt;span class="math inline">\(R_{n-1}\)&lt;/span> is &lt;span class="math inline">\(22.5^\circ\)&lt;/span> clockwise from &lt;span class="math inline">\(O_n\)&lt;/span>, it’s relatively difficult for &lt;span class="math inline">\(O_n\)&lt;/span> to be around &lt;span class="math inline">\(67.5^\circ\)&lt;/span>. For example, when &lt;span class="math inline">\(O_n=67.5\)&lt;/span>, the previous response could be &lt;span class="math inline">\(22.5^\circ\)&lt;/span> if &lt;span class="math inline">\(O_n=45^\circ\)&lt;/span>, but nearly no other orientation would work; when &lt;span class="math inline">\(O_n\)&lt;/span> is near but not exactly &lt;span class="math inline">\(45^\circ\)&lt;/span>, the oblique effect on trial &lt;span class="math inline">\(n-1\)&lt;/span> will push &lt;span class="math inline">\(R_{n-1}\)&lt;/span> away from &lt;span class="math inline">\(45^\circ\)&lt;/span>, away from a response that could be &lt;span class="math inline">\(22.5^\circ\)&lt;/span> clockwise to &lt;span class="math inline">\(O_n\)&lt;/span>. This is important because, if it is rare for trial &lt;span class="math inline">\(n\)&lt;/span> to have both &lt;span class="math inline">\(O_n=67.5\)&lt;/span> and &lt;span class="math inline">\(R_{n-1}\)&lt;/span> be &lt;span class="math inline">\(22.5^\circ\)&lt;/span> clockwise from &lt;span class="math inline">\(O_n\)&lt;/span>, then the oblique effect will be imbalanced.&lt;/p>
&lt;p>Together, this means that when the oblique effect on trial &lt;span class="math inline">\(n\)&lt;/span> causes a maximal clockwise error, the oblique effect on trial &lt;span class="math inline">\(n-1\)&lt;/span> makes it more likely that the previous response is also clockwise and less likely that it’s counterclockwise. The result is a spurious dependence on the previous response.&lt;/p>
&lt;p>We can see this play out more generally by looking at the current orientation as a function of the previous orientations and responses.&lt;/p>
&lt;pre class="r">&lt;code>d %&amp;gt;%
pivot_longer(
cols = c(orientation_diff, response_diff),
names_to = &amp;quot;covariate&amp;quot;,
names_pattern = &amp;quot;(response|orientation)&amp;quot;) %&amp;gt;%
ggplot(aes(x=orientation, y=value)) +
facet_wrap(~covariate) +
geom_point() +
coord_fixed() +
scale_y_continuous(
name = &amp;quot;relative orientation/response on previous trial&amp;quot;,
labels = c(-90, 0, 90),
breaks = c(-90, 0, 90)) +
scale_x_continuous(
name = &amp;quot;orientation on current trial&amp;quot;,
labels = c(0, 90, 180),
breaks = c(0, 90, 180))&lt;/code>&lt;/pre>
&lt;div class="figure">&lt;span id="fig:currbyprev">&lt;/span>
&lt;img src="https://psadil.github.io/psadil/post/2021-05-07-spurious-serial-dependencies/index.en_files/figure-html/currbyprev-1.png" alt="The current orientation is unrelated to the previous orientation, but there is a dependency on the previous response. " width="672" />
&lt;p class="caption">
Figure 5: The current orientation is unrelated to the previous orientation, but there is a dependency on the previous response.
&lt;/p>
&lt;/div>
&lt;p>As expected, there is no relationship between &lt;span class="math inline">\(O_n\)&lt;/span> and &lt;span class="math inline">\(O_{n-1}\)&lt;/span>, but there is a strong relationship between &lt;span class="math inline">\(O_n\)&lt;/span> and &lt;span class="math inline">\(R_{n-1}\)&lt;/span>. When &lt;span class="math inline">\(O_n \in (0,45)\)&lt;/span>, then it’s likely that &lt;span class="math inline">\(R_{n-1} \in (0,22.5)\)&lt;/span> (clockwise), or &lt;span class="math inline">\(R_{n-1} \in (-90, -67.5)\)&lt;/span> (counterclockwise). The figure below shows the same data, but now the data are colored according to how the oblique effect will cause errors on trial &lt;span class="math inline">\(n\)&lt;/span>.&lt;/p>
&lt;pre class="r">&lt;code>d %&amp;gt;%
na.omit() %&amp;gt;%
select(-response, -error) %&amp;gt;%
pivot_longer(
cols = c(orientation_diff, response_diff),
names_to = &amp;quot;covariate&amp;quot;,
names_pattern = &amp;quot;(response|orientation)&amp;quot;) %&amp;gt;%
ggplot(aes(x=orientation, y=value)) +
facet_wrap(~covariate) +
geom_point(aes(color=oblique)) +
scale_color_gradient2(low = scales::muted(&amp;quot;blue&amp;quot;), high = scales::muted(&amp;quot;red&amp;quot;)) +
coord_fixed() +
scale_y_continuous(
name = &amp;quot;relative orientation/response on previous trial&amp;quot;,
labels = c(-90, 0, 90),
breaks = c(-90, 0, 90)) +
scale_x_continuous(
name = &amp;quot;orientation on current trial&amp;quot;,
labels = c(0, 90, 180),
breaks = c(0, 90, 180))&lt;/code>&lt;/pre>
&lt;div class="figure">&lt;span id="fig:currbyprevcolor">&lt;/span>
&lt;img src="https://psadil.github.io/psadil/post/2021-05-07-spurious-serial-dependencies/index.en_files/figure-html/currbyprevcolor-1.png" alt="This is the same plot as above, but now the data have been colored according to how the oblique effect will cause errors. The point is that the errors caused by the oblique effect on trial $n$ are balanced when looking at the orientation on the previous trial (consider a horizontal slice), but no such balancing happens when looking at the response on the previous trial." width="672" />
&lt;p class="caption">
Figure 6: This is the same plot as above, but now the data have been colored according to how the oblique effect will cause errors. The point is that the errors caused by the oblique effect on trial &lt;span class="math inline">\(n\)&lt;/span> are balanced when looking at the orientation on the previous trial (consider a horizontal slice), but no such balancing happens when looking at the response on the previous trial.
&lt;/p>
&lt;/div>
&lt;p>Fortunately, this spurious bias isn’t too hard to adjust for&lt;a href="#fn3" class="footnote-ref" id="fnref3">&lt;sup>3&lt;/sup>&lt;/a>. But the point is that it would be a mistake to look at just a dependence on the previous orientation if there is an oblique effect; analyses must adjust for the oblique effect.&lt;/p>
&lt;p>I’m not sure if there is a similar issue with other domains (e.g., when participants discriminate tones, pain, faces, etc). Perhaps edge effects could cause a similar issue (e.g., if people are more likely to respond to the ends or middle of the scale)?&lt;/p>
&lt;div id="refs" class="references csl-bib-body hanging-indent">
&lt;div id="ref-fritsche2016smooth" class="csl-entry">
Fritsche, Matthias. 2016. &lt;span>“To Smooth or Not to Smooth: Investigating the Role of Serial Dependence in Stabilising Visual Perception.”&lt;/span> Master’s thesis, Radboud University.
&lt;/div>
&lt;div id="ref-wei2015bayesian" class="csl-entry">
Wei, Xue-Xin, and Alan A Stocker. 2015. &lt;span>“A Bayesian Observer Model Constrained by Efficient Coding Can Explain’anti-Bayesian’percepts.”&lt;/span> &lt;em>Nature Neuroscience&lt;/em> 18 (10): 1509.
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="footnotes">
&lt;hr />
&lt;ol>
&lt;li id="fn1">&lt;p>This effect occurs when participants are asked to report orientations. Participants are differently accurate across the range of orientations; they are maximally accurate when reporting &lt;span class="math inline">\(0^\circ\)&lt;/span>, &lt;span class="math inline">\(45^\circ\)&lt;/span>, &lt;span class="math inline">\(90^\circ\)&lt;/span>, and &lt;span class="math inline">\(135^\circ\)&lt;/span>, but minimally accurate at intermediate orientations (&lt;span class="math inline">\(22.5^\circ\)&lt;/span>, &lt;span class="math inline">\(67.5^\circ\)&lt;/span>, etc). The errors can either be clockwise or counterclockwise, depending on the experiment. For an overview, see &lt;span class="citation">&lt;a href="#ref-wei2015bayesian" role="doc-biblioref">Wei and Stocker&lt;/a> (&lt;a href="#ref-wei2015bayesian" role="doc-biblioref">2015&lt;/a>)&lt;/span>.&lt;a href="#fnref1" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn2">&lt;p>This won’t be used, but sequence of orientations could be written &lt;span class="math inline">\((O_n)_{n\in\mathbb{N}}\)&lt;/span>, and the sequence of responses &lt;span class="math inline">\((R_n)_{n\in\mathbb{N}}\)&lt;/span>. Selecting a particular trial involves dropping the parentheses; &lt;span class="math inline">\((O_n)_{n\in\mathbb{N}}\)&lt;/span> emphasizes the whole sequence, whereas &lt;span class="math inline">\(O_n\)&lt;/span> means take a particular (but arbitrary) element of the sequence. I am not a mathematician, and this post is a quick and dirty explanation mostly meant for later me, so don’t expect formality.&lt;a href="#fnref2" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn3">&lt;p>In a regression model of the errors, it would suffice to include a sinusoidal term.&lt;a href="#fnref3" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>NeuroModulation Modeling (NMM): Inferring the form of neuromodulation from fMRI tuning functions</title><link>https://psadil.github.io/psadil/publication/sadil-2021-nmm/</link><pubDate>Fri, 05 Mar 2021 14:14:56 -0500</pubDate><guid>https://psadil.github.io/psadil/publication/sadil-2021-nmm/</guid><description/></item><item><title>The Yin-yang of Serial Dependence Effects: Every Response is both an Attraction to the Prior Response and a Repulsion from the Prior Stimulus</title><link>https://psadil.github.io/psadil/publication/sadil-2021-serialdependence/</link><pubDate>Mon, 18 Jan 2021 14:35:30 -0500</pubDate><guid>https://psadil.github.io/psadil/publication/sadil-2021-serialdependence/</guid><description/></item><item><title>New England GAN</title><link>https://psadil.github.io/psadil/post/2021-01-02-gan-mass/</link><pubDate>Sun, 17 Jan 2021 00:00:00 +0000</pubDate><guid>https://psadil.github.io/psadil/post/2021-01-02-gan-mass/</guid><description>
&lt;script src="https://psadil.github.io/psadil/post/2021-01-02-gan-mass/index.en_files/header-attrs/header-attrs.js">&lt;/script>
&lt;p>A friend recently moved away from Western Mass, so I wanted to send them a gift to help remind them of the area. I also happened to want to learn a bit more about Generative Adversarial Networks (GANs).&lt;/p>
&lt;div id="goals" class="section level1">
&lt;h1>Goals&lt;/h1>
&lt;ul>
&lt;li>Briefly overview GANs&lt;/li>
&lt;li>Link to fun code for acquiring data from Google Street View&lt;/li>
&lt;li>Share a neat set of photographs&lt;/li>
&lt;/ul>
&lt;p>Note that this isn’t an ‘intro to GANs’. If that’s what you’re after, keep browsing.&lt;/p>
&lt;/div>
&lt;div id="overview" class="section level1">
&lt;h1>Overview&lt;/h1>
&lt;p>Computers have gotten very good at extracting information from images, particularly at identifying images’ contents. Such categorization is powerful, but it often requires access to labeled data – hundreds of thousands of pictures for which we can tell the computer: this is a cat, that is a dog, no that’s also a dog, yes that’s a dog but it’s also a husky. However, many applications remain where computer-aided categorization would be invaluable, but for which there isn’t sufficient labeled data. If an algorithm can learn to recognize the subtle features distinguishing &lt;a href="https://en.wikipedia.org/wiki/ImageNet">120 dog breeds&lt;/a>, it could probably learn visual features that help radiologists locate potential anomalies. But the guess-and-check strategy, despite being sufficient for many advanced computer vision algorithms, flounders when it has access to only a few hundred training examples. Computers have the potential to do some very clever things, but there is not always enough data to supervise their training.&lt;/p>
&lt;p>To mitigate a lack of data, one developing solution is a GAN. A common analogy for these networks envisions art forgery (&lt;a href="https://www.tensorflow.org/tutorials/generative/dcgan">e.g.&lt;/a>), a forger and a critic collaborating to learn about an artist. The forger paints fake works in the style of van Gough, while the critic distinguishes the fake from the real van Goughs. For the forger to succeed, it must paint the essences of van Gough: the reductionist features like the strokes and the yellows, and the holistic feelings of urgency and presence. For the critic to succeed, it must identify those essences, learning the sharp boundaries between longing and yearning. As the forgeries improve, the critic becomes more discerning, further inspiring the forger. Although the networks are taught the essences – the labels – explicitly, the two together learn about van Gough. And they’ll learn without supervision.&lt;/p>
&lt;p>After learning, the critic can be deployed for standard categorization tasks (e.g., aiding medical diagnoses). But the training also produces another useful machine, a machine that is capable of generating images. Predictably, there are challenges to training a generator that is capable of producing good quality, large, and diverse images. But I didn’t need the images to be stellar, so long as their content was clear (to a human). A lack of photorealism – imperfect training – could make the pictures more interesting&lt;a href="#fn1" class="footnote-ref" id="fnref1">&lt;sup>1&lt;/sup>&lt;/a>. To make a gift, I wanted a forger that could paint New England.&lt;/p>
&lt;/div>
&lt;div id="setup" class="section level1">
&lt;h1>Setup&lt;/h1>
&lt;p>I wanted the forger to generate images of New England, so I first needed a bunch of pictures of New England. I have photographed a few hundred pictures, but this wouldn’t be nearly enough&lt;a href="#fn2" class="footnote-ref" id="fnref2">&lt;sup>2&lt;/sup>&lt;/a>. Instead, I relied on a combination of Google’s &lt;a href="https://developers.google.com/maps/documentation/streetview/overview">Street View Static&lt;/a> and &lt;a href="https://developers.google.com/maps/documentation/directions/overview">Directions&lt;/a> APIs. The Street View API gives a picture associated with a location, and those locations were provided by the Directions API. &lt;a href="https://github.com/psadil/gan-mass">The repository&lt;/a> for the network has the details, but the result was that I could input an origin and a destination – meandering through a few waypoints – and download whatever the Street View Car recorded when it traveled along those directions&lt;a href="#fn3" class="footnote-ref" id="fnref3">&lt;sup>3&lt;/sup>&lt;/a>. In the end, I collected ~25,000 images.&lt;/p>
&lt;p>25k may feel like a lot of images. But skimming online suggested that &lt;a href="https://blogs.nvidia.com/blog/2020/12/07/neurips-research-limited-data-gan/">even 25k would not have been enough to adequately constrain the networks&lt;/a>. GANs may not require labeled examples, but they are still data-hungry. Given my relatively small dataset, I picked an adversarial architecture that incorporates a few extra tricks to glean information from smaller datasets: &lt;a href="https://github.com/NVlabs/stylegan2-ada">stylegan2-ada&lt;/a>&lt;a href="#fn4" class="footnote-ref" id="fnref4">&lt;sup>4&lt;/sup>&lt;/a>. To train the network, I used free credits on the Google’s cloud console.&lt;/p>
&lt;/div>
&lt;div id="curated-samples" class="section level1">
&lt;h1>Curated Samples&lt;/h1>
&lt;p>After one day of training&lt;a href="#fn5" class="footnote-ref" id="fnref5">&lt;sup>5&lt;/sup>&lt;/a>, the network started producing some useful images.&lt;/p>
&lt;div class="figure">
&lt;img src="print_5x7_256.png" alt="" />
&lt;p class="caption">These six images are fake, produced from a collaboration to learn about New England.&lt;/p>
&lt;/div>
&lt;p>I chose these six – and the seventh at the top – because they illustrate a few fun features of what the GAN learned. For example, the GAN learned, very early, that pictures of New England always have, in the bottom corners, the word “Google”&lt;a href="#fn6" class="footnote-ref" id="fnref6">&lt;sup>6&lt;/sup>&lt;/a>. That machine learning can produce realistic text surprises me (e.g., &lt;a href="https://www.facebook.com/botsofnewyork/photos/a.2028566864113743/2490502274586864/?type=3&amp;amp;theater">if the face is weird, how are all of the pixels in place to spell out a word&lt;/a>?!). I assume that text comes out clean because most lettering is tightly constrained. That is, when the forger paints something that could be categorized as lettering, the critic severely constrains those pixels; fuzzy letters betray forgery, and real photographs don’t have nonsense like UNS;QD*LKJ. So if the training images contain enough text that the generator starts producing letters, there is also enough text for the critic to learn what text is realistic.&lt;/p>
&lt;p>The forger had difficulty with buildings. I downloaded mostly images of the highways connecting cities. This means that there were enough cityscapes for the GAN to generate buildings, but relative to a road, it was much slower at learning the intricacies of a building. Of course, the roads are imperfect, too (the telephone pole in the upper middle ripples, the upper left has too many roads, the colors of the painted lines mismatch, etc). But unlike, say, a bad photoshop, these errors have a kind of global coherence that, subjectively, allows the images to seem not fake but instead surreal.&lt;/p>
&lt;/div>
&lt;div class="footnotes">
&lt;hr />
&lt;ol>
&lt;li id="fn1">&lt;p>If I wanted perfect pictures, I could have just used a camera.&lt;a href="#fnref1" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn2">&lt;p>The van Gough example is slightly misleading; in practice, van Gough didn’t paint enough pictures to train a GAN. Training a GAN from scratch doesn’t require labeled data, but it still requires many images. There are tricks that could help a GAN, but simply training his images would likely be insufficient.&lt;a href="#fnref2" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn3">&lt;p>Having not owned a car during graduate school, I found it funny that these networks learned about New England through its highways&lt;a href="#fnref3" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn4">&lt;p>But also, they provided a helpful docker image, functions to prep the data, and decent documentation. This is a good reminder about the benefits of polishing a repository.&lt;a href="#fnref4" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn5">&lt;p>After one day, the training error was still decreasing. But I was using a &lt;a href="https://cloud.google.com/compute/docs/instances/preemptible">preemptible virtual machine&lt;/a>, and so after 24 hours it was automatically shutdown.&lt;a href="#fnref5" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn6">&lt;p>I removed the text from the curated examples, but it can be seen in the preview image at the top.&lt;a href="#fnref6" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>Git as version control</title><link>https://psadil.github.io/psadil/post/2020-10-01-git-as-version-control/</link><pubDate>Thu, 01 Oct 2020 00:00:00 +0000</pubDate><guid>https://psadil.github.io/psadil/post/2020-10-01-git-as-version-control/</guid><description>
&lt;script src="https://psadil.github.io/psadil/post/2020-10-01-git-as-version-control/index.en_files/header-attrs/header-attrs.js">&lt;/script>
&lt;p>This is a first in a series of posts that will attempt to explain&lt;/p>
&lt;div id="the-problem-to-solve" class="section level1">
&lt;h1>The problem to solve&lt;/h1>
&lt;p>Jokes, &lt;a href="https://xkcd.com/1459/">1&lt;/a> and &lt;a href="http://phdcomics.com/comics/archive.php?comicid=1531">2&lt;/a>.&lt;/p>
&lt;p>With many systems of digital memory, the problem is rarely space; we can store loads of data indefinitely. Instead, the challenge is to develop a good system of indexing things, so that you can find what you’ve backed up. For this, it would be helpful to know about which commits are the most important.&lt;/p>
&lt;p>Git can be a system for “version control”. I don’t have much nuance to add to this phrase, other than to highlight that, in the long run, it will be helpful to spend time thinking about what is meant by a “version” for research. For myself, I find that a “version” means a particular output: each pilot of an experiment, the code right before a meeting, all of your analysis right before a poster, the analyses immediately before a publication. I find that a version tends to mean the exact state of a project as a certain time, something that I want to be easily accessible to a future me. Your own definition of a version will probably change as you go through graduate school.&lt;/p>
&lt;/div>
&lt;div id="assumptions" class="section level1">
&lt;h1>Assumptions&lt;/h1>
&lt;ul>
&lt;li>You have installed &lt;code>Git&lt;/code>.&lt;/li>
&lt;li>You know how to access a terminal that knows about &lt;code>Git&lt;/code>&lt;/li>
&lt;li>You’ve done the initial configuration steps&lt;/li>
&lt;/ul>
&lt;div class="marginnote">
&lt;p>&lt;a href="https://git-scm.com/book/en/v2/Getting-Started-Installing-Git">Git Pro Book&lt;/a>&lt;/p>
&lt;/div>
&lt;/div>
&lt;div id="initialize-a-repository-with-git-init" class="section level1">
&lt;h1>Initialize a repository with &lt;code>git init&lt;/code>&lt;/h1>
&lt;p>Make a new directory. Call it “demo”. Start &lt;code>Git&lt;/code> in that folder. The first step will be to ‘initialize’ &lt;code>Git&lt;/code>. Here, to initialize is the verb used for associating &lt;code>Git&lt;/code> with a particular folder.&lt;/p>
&lt;pre class="bash">&lt;code>$ git init&lt;/code>&lt;/pre>
&lt;p>New terminology! A “repository”, often shortened to “repo” is a particular folder that is under version control by &lt;code>Git&lt;/code>.&lt;/p>
&lt;/div>
&lt;div id="check-new-status-with-git-status" class="section level1">
&lt;h1>Check new status with &lt;code>git status&lt;/code>&lt;/h1>
&lt;p>There are many situations in which you will want to understand what’s new with a repository. The subcommand for this is &lt;code>staus&lt;/code>&lt;/p>
&lt;pre class="bash">&lt;code>$ git status&lt;/code>&lt;/pre>
&lt;p>Three new sets of words: “branch” (with “master”), “commit”, and “track”. Starting with the last of these, to track is the verb &lt;code>Git&lt;/code> uses to mean something whose version is controlled. So, this tip is saying that the next step of version control will involve directing &lt;code>Git&lt;/code> which files it should look for.&lt;/p>
&lt;/div>
&lt;div id="make-changes-to-the-repository" class="section level1">
&lt;h1>Make changes to the repository&lt;/h1>
&lt;p>We’ll work with a few simple scripts from &lt;a href="https://fishsciences.github.io/post/visualizing-fish-encounter-histories/" class="uri">https://fishsciences.github.io/post/visualizing-fish-encounter-histories/&lt;/a>.&lt;/p>
&lt;p>Add the file &lt;code>fishdata.csv&lt;/code> to your folder, and take a look at how &lt;code>Git&lt;/code> has responded.&lt;/p>
&lt;pre class="bash">&lt;code>$ git status&lt;/code>&lt;/pre>
&lt;p>Again, we see the same messages about branches and commits. Now we, also see a message about an “untracked” file. This is a file that is in the repository, but which is not yet under version control.&lt;/p>
&lt;/div>
&lt;div id="track-files-with-git-add" class="section level1">
&lt;h1>Track files with &lt;code>git add&lt;/code>&lt;/h1>
&lt;p>You must explicitly instruct &lt;code>Git&lt;/code> to track files. To track the &lt;code>fishdata.csv&lt;/code> file you can run the following&lt;/p>
&lt;pre class="bash">&lt;code>$ git add fishdata.csv&lt;/code>&lt;/pre>
&lt;p>Check out the new status&lt;/p>
&lt;pre class="bash">&lt;code>$ git status&lt;/code>&lt;/pre>
&lt;p>When you run &lt;code>git add&lt;/code>, you are telling git to track the changes made to a file. In this case, the changes are that there is a new file. &lt;code>Git&lt;/code> now knows that it should pay attention to this file (or changes that have been made to the file), but the changes are &lt;em>not&lt;/em> yet saved. Changes that have been added are also called changes that have been “staged”. There are many steps to this version control business.&lt;/p>
&lt;/div>
&lt;div id="save-a-version-with-git-commit" class="section level1">
&lt;h1>Save a version with &lt;code>git commit&lt;/code>&lt;/h1>
&lt;p>You save a version of your repository with the subcommand &lt;code>commit&lt;/code>. This will open up an editor, and you will be prompted to write a message. For now, you can type the message “Added first data file”.&lt;/p>
&lt;pre class="bash">&lt;code>$ git commit&lt;/code>&lt;/pre>
&lt;pre class="bash">&lt;code>$ git commit -m &amp;quot;Added first data file&amp;quot;&lt;/code>&lt;/pre>
&lt;p>When you commit changes, you create a snapshot of all of the tracked files, a snapshot that you will be able to access at a later date. The point of a commit message is to give those changes an explicit context, context to help explain why you’ve made changes. Ideally, commit messages will be informative, describing some specific goal that the changes have accomplished (see the above link for more details). There is an art to writing good commit messages (&lt;a href="https://chris.beams.io/posts/git-commit/">for a software engineer’s take, see here&lt;/a>). But not every commit message needs to be poetry. For example, when you’re done working for the day, the project might not have a particularly clear context, and yet you will still need to backup your code. Plenty of messages look like “stopped for the day”.&lt;/p>
&lt;/div>
&lt;div id="review-the-history-with-git-log" class="section level1">
&lt;h1>Review the history with &lt;code>git log&lt;/code>&lt;/h1>
&lt;p>To see a list of the commits you’ve made, use &lt;code>log&lt;/code>.&lt;/p>
&lt;pre class="bash">&lt;code>$ git log&lt;/code>&lt;/pre>
&lt;p>With each commit you make, &lt;code>Git&lt;/code> associates a long ID (e.g., &lt;code>f9390d95f5e369e12d6a65c8f5fa70b123cf8343&lt;/code>). You will not typically need to use that ID directly (more in the next session). The log also contains information about the author of the commit, their email (i.e., the things you setup with &lt;code>git config&lt;/code>), when the commit was made, and what message was written about the commit.&lt;/p>
&lt;p>For easier browsing, the logs can be condensed with the &lt;code>--oneline&lt;/code> flag.&lt;/p>
&lt;pre class="bash">&lt;code>$ git log --oneline&lt;/code>&lt;/pre>
&lt;p>Each commit describes a project during a certain moment in time. As a researcher, you probably won’t care about most moments (e.g., you probably won’t ever need to go back to the status of the code at the end of each day). It’s good that they are backed up, but you won’t ever need to return to those versions.&lt;/p>
&lt;p>You now have a system for backing up versions indefinitely. Rather than having version&lt;/p>
&lt;p>With &lt;code>Git&lt;/code>, as&lt;/p>
&lt;/div>
&lt;div id="mark-important-commits-with-git-tag" class="section level1">
&lt;h1>Mark important commits with &lt;code>git tag&lt;/code>&lt;/h1>
&lt;p>To mark a particular commit as important, you “tag” it. First, move the file “fish_figure.R” into your repository, stage it, then commit it. I’ve written the message “Added analyses of data”.&lt;/p>
&lt;pre class="bash">&lt;code>$ git add fish_figure.R
+ git commit&lt;/code>&lt;/pre>
&lt;p>You will now use a new subcommand to mark this as an important commit, &lt;code>tag&lt;/code>. Just like when you are committing something, you are given the opportunity to add a bit of context to the tag, and why it might be important. If this is the stage of the code right before a meeting with my adviser, my tag message might look like “before meeting; I’m going to talk about some cool plots”&lt;/p>
&lt;pre class="bash">&lt;code>$ git tag -a oct-1-2020&lt;/code>&lt;/pre>
&lt;p>&lt;code>-a&lt;/code> stands for “annotate”. The annotation is given by “oct-1-2020”. The annotation will be a method for finding that commit. I tag many projects before each meeting with an adviser, so many of my tags are dates.&lt;/p>
&lt;/div>
&lt;div id="to-view-previous-versions-of-the-code-use-git-checkout" class="section level1">
&lt;h1>To view previous versions of the code, use &lt;code>git checkout&lt;/code>&lt;/h1>
&lt;p>You’ve made two commits, and indicated that the second one is important. Now, you will see how to look at previous versions of your code.&lt;/p>
&lt;p>First, make some small change to &lt;code>fish_figure.R&lt;/code>. For example, you might add the following comment to the top of the script &lt;code># Here is a new comment at the top of the script.&lt;/code> After making the change, track those changes, and then commit. This time, write the commit message a bit differently.&lt;/p>
&lt;pre class="bash">&lt;code>$ git add fish_figure.R
+ git commit -m &amp;quot;Add line comment&amp;quot;&lt;/code>&lt;/pre>
&lt;p>Notice that the commit message is written slightly differently. Adding a single comment to &lt;code>fish_figure.R&lt;/code> was very minor, and so there is not a lot of context. By passing &lt;code>-m&lt;/code>, you can also type the message directly, without needing to open up the text editor.&lt;/p>
&lt;p>Take a look at the logs to see the current version in relation to previous commits&lt;/p>
&lt;pre class="bash">&lt;code>$ git log --oneline&lt;/code>&lt;/pre>
&lt;p>At this point, there are a total of 3 commits, the second one of which you’ve tagged. To visit the project at that moment in time, you use &lt;code>git checkout&lt;/code>, and you pass it the annotation you provided earlier.&lt;/p>
&lt;pre class="bash">&lt;code>$ git checkout oct-1-2020&lt;/code>&lt;/pre>
&lt;p>You can ignore most of the output on the console. But do go look at the file &lt;code>fish_figure.R&lt;/code>. You will see that the comment is not there. That is, you’ve visited the files exactly as they looked when you made a tag.&lt;/p>
&lt;p>To return to the current state of the files, run the following&lt;/p>
&lt;pre class="bash">&lt;code>$ git checkout master&lt;/code>&lt;/pre>
&lt;p>Confirm that the third change you committed is in the current working directory (e.g., the comment that you added should be back in the file).&lt;/p>
&lt;div id="side-note" class="section level2">
&lt;h2>Side note&lt;/h2>
&lt;p>You can access every commit based on the automatically generated ID. To see all of the commits and their IDs, again use the log command.&lt;/p>
&lt;pre class="bash">&lt;code>$ git log&lt;/code>&lt;/pre>
&lt;p>Your IDs will be different. But if I wanted to get to the commit tagged “oct-1-2020”, I could also type the following&lt;/p>
&lt;pre class="bash">&lt;code>$ git checkout dc160a659f7eb6503c78b3e28b659a91e7d0d89b&lt;/code>&lt;/pre>
&lt;p>Notice also that reference to ‘master’ again, which has not yet been explained. More on that next.&lt;/p>
&lt;/div>
&lt;/div>
&lt;div id="interim" class="section level1">
&lt;h1>Interim&lt;/h1>
&lt;p>At this point, you’ve started tracking files in a repository, and you’ve explicitly written out the context of all of those changes. The following is a bit esoteric, but I’m mentioning it as a way to start an explanation for what “branch” and “master” refers to, and to prepare you as you look at more resources.&lt;/p>
&lt;p>One way to view a “repository” is as a graph, in the &lt;a href="https://en.wikipedia.org/wiki/Graph_theory">graph theory sense&lt;/a>, with nodes and edges between those nodes.&lt;/p>
&lt;p>As you continually commit changes, you will add nodes to this graph, one node for each commit. Each node will have an edge, pointing some earlier commit. Some of those nodes will be easily (the tagged ones), but all are accessible.&lt;/p>
&lt;p>Notice also that this graph is one long chain, with each commit pointing to exactly one unique node. There are ways to add ‘branches’ to the graph, where you can maintain separate versions of your code simultaneously. With this branching, &lt;code>Git&lt;/code> becomes very powerful. The primary branch is often called the “master” branch. This is currently the default name on a new &lt;code>Git&lt;/code> repository (but see below). Similarly to how you can pass both IDs and a tag’s annotation to &lt;code>git checkout&lt;/code>, you can also pass the name of a branch. When you use the name of a branch, &lt;code>Git&lt;/code> checks out the most recent commit on that branch. So, earlier, when you ran &lt;code>git checkout master&lt;/code>, it’s like you told &lt;code>Git&lt;/code> to “convert all files to the most recent commit of the master branch”, which happened to be the most recent versions of the files. Branching gets complicated very quickly. For more information, see the references at the end of this document (especially &lt;a href="https://git-scm.com/book/en/v2/Git-Branching-Branches-in-a-Nutshell">the branching chapter&lt;/a> of the Git Pro book). As you learn more about &lt;code>Git&lt;/code>, branches may become important, and you will continue to encounter this graph metaphor.&lt;/p>
&lt;/div>
&lt;div id="extra" class="section level1">
&lt;h1>Extra&lt;/h1>
&lt;div id="gitignore" class="section level2">
&lt;h2>.gitignore&lt;/h2>
&lt;p>Sometimes, you will want to explicitly ignore files in the folder. These can be done by adding those files to a &lt;code>.gitignore&lt;/code> file. We talked about how a &lt;code>.gitignore&lt;/code> folder might contain the line &lt;code>hippa&lt;/code>, if you wanted to always make sure that &lt;code>Git&lt;/code> ignored a folder called “hippa”.&lt;/p>
&lt;p>Most coding languages generate temporary files that you don’t need to store. For this reasons, GitHub has a repository dedicated to commonly used &lt;code>.gitignore&lt;/code> files, &lt;a href="https://github.com/github/gitignore">here&lt;/a>. For example, &lt;a href="https://github.com/github/gitignore/blob/master/R.gitignore">here is a file for R&lt;/a>, and &lt;a href="https://github.com/github/gitignore/blob/master/Python.gitignore">here is one for python&lt;/a>. You could add the lines of those files to the &lt;code>.gitignore&lt;/code> file in your own projects. See how easy it is to share code with GitHub :).&lt;/p>
&lt;/div>
&lt;div id="larger-files" class="section level2">
&lt;h2>Larger Files&lt;/h2>
&lt;p>&lt;code>Git&lt;/code> is best at storing code, files that can be opened in a text editor. If you start storing larger files, binaries, things that end in ‘.pdf’, ‘.docx’, ‘.png’, etc, your repository will quickly become very large, and the different commands will start taking a long time to run. For this reason, you will often want to avoid adding such files to your repository (they can be inside your folder, but don’t call &lt;code>git add&lt;/code> on them). There are two points here. First, &lt;code>Git&lt;/code> isn’t for everything. For example, I organize my pdfs with Zotero, and backup those pdfs using box. Second, there are other tools for version control that integrate very smoothly with &lt;code>Git&lt;/code>. I like &lt;a href="https://git-lfs.github.com/">&lt;code>Git LFS&lt;/code>&lt;/a> (Large File Storage). For example, when an experiment involves images, I track the experimental code with &lt;code>Git&lt;/code> and the images with &lt;code>Git LFS&lt;/code>. If all of the images are stored as .png files in the repository folder, typing the following will put those under version control by &lt;code>Git LFS&lt;/code>.&lt;/p>
&lt;p>This allows me to keep a &lt;code>Git&lt;/code>-focused workflow, while still allowing the images to be connected with the experiment.&lt;/p>
&lt;p>The main competitor to LFS is &lt;a href="https://git-annex.branchable.com/">&lt;code>Git-annex&lt;/code>&lt;/a>. Git annex is much harder to use, and, as compared to LFS, Git-annex doesn’t play nicely with GitHub/GitLab. However, the fMRI community has built a tool on top of &lt;code>Git-annex&lt;/code>, called &lt;a href="https://www.datalad.org/">datalad&lt;/a>. &lt;code>datalad&lt;/code> promises to bring collaborative version control to neuroimaging.&lt;/p>
&lt;/div>
&lt;/div></description></item><item><title>scoring rules</title><link>https://psadil.github.io/psadil/post/2020-09-01-scoring-rules/</link><pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate><guid>https://psadil.github.io/psadil/post/2020-09-01-scoring-rules/</guid><description>
&lt;script src="https://psadil.github.io/psadil/post/2020-09-01-scoring-rules/index.en_files/header-attrs/header-attrs.js">&lt;/script>
&lt;p>Bayes’ Rule gives that &lt;span class="math inline">\(p(\theta|y) \propto p(\theta) p(y|\theta)\)&lt;/span>, that the posterior distribution, &lt;span class="math inline">\(p(\theta|y)\)&lt;/span> of the parameters, &lt;span class="math inline">\(\theta\)&lt;/span>, is proportional to a prior distribution on those parameters, &lt;span class="math inline">\(p(\theta)\)&lt;/span>, multiplied by the likelihood of the data, &lt;span class="math inline">\(p(y|\theta)\)&lt;/span>. The rule specifies a procedure for using observed data, &lt;span class="math inline">\(y\)&lt;/span>, to learn about the parameters, but it does not give a procedure for making decisions based on those parameters. In this dissertation, the statistical decisions are based primarily on the ability of different models to predict new data, &lt;span class="math inline">\(\tilde{y}\)&lt;/span>, as approximated by PSIS-LOO+. However, this approach is relatively rare in cognitive psychology. The notation closely follows that of &lt;span class="citation">[@vehtari_practical_2017]&lt;/span>.&lt;/p>
&lt;p>Currently in Psychology, the strategy is to rely on Bayes’ Factors. There are a few different ways to view Bayes’ Factors. I’m going to question whether Bayes’ Factors are actually a good way to choose a new model.&lt;/p>
&lt;p>There are a few different ways to motivate Bayes’ factors. One simple strategy comes from an application of Bayes’ rule. If we have a model, &lt;span class="math inline">\(M\)&lt;/span>, we can ask a question like: what is the probability of a model, given that we’ve observed data? This gives the relationship&lt;/p>
&lt;p>&lt;span class="math display">\[
p(M|y) = \frac{p(y|M)p(M)}{p(y)}
\]&lt;/span>&lt;/p>
&lt;p>This relationship is not, by itself, particularly helpful. There is approximately 0 chance that we’ve specified exactly the right model, so any calculation that allowed &lt;span class="math inline">\(p(M|y) &amp;gt; 0\)&lt;/span> should immediately be suspect. But if we have two models, &lt;span class="math inline">\(M_0\)&lt;/span> and &lt;span class="math inline">\(M_1\)&lt;/span>, it makes a bit of sense to talk about the relative probability of each model.&lt;/p>
&lt;p>&lt;span class="math display">\[
\frac{p(M_1|y)}{p(M_0|y)} = \underbrace{\frac{p(y|M_1)}{p(y|M_0)}}_{\text{Bayes Factor}} \frac{p(M_1)}{p(M_0)}
\]&lt;/span>&lt;/p>
&lt;p>Assuming that the prior probability associated with each model are equal, then this ratio of posterior is driven entirely by the ratio of likelihoods of the data under each of the different models. However, it is again questionable whether this relationship practical makes sense to calculate. As before, I have no faith that the models I’ve specified are literally the truth, so for me &lt;span class="math inline">\(p(M_0)\)&lt;/span> is 0. Just because these are the two models I’m currently willing to entertain does not mean that I think they are actually true (after all, &lt;em>all&lt;/em> models are wrong…).&lt;/p>
&lt;p>But let’s assume that we’re talking about some small world, where all statements are qualified with the assumption that one of these two models are true.&lt;/p>
&lt;p>Now we step into a tricky part of language. It is common to talk about a model’s ‘predictions’ in reference to how well the model can capture the already observed data. This language leads some authors to talk about Bayes’ factor as a measure of the (relative) ‘predictive accuracy’ of a model.&lt;/p>
&lt;p>By the chain rule, the implied predictions can also be viewed sequentially [waiting on book to see whether this is relevant]&lt;/p>
&lt;p>&lt;span class="math display">\[
p(y|M) = p(y_1|M)p(y_2|M,y_1)\ldots p(y_n | M,y_1,y_2,\ldots,y_{n-1})
\]&lt;/span>&lt;/p>
&lt;p>:A distribution indicating what new data are likely under a posterior distribution. &lt;span class="math inline">\(p(\tilde{y}_i | y) = \int p(\tilde{y}_i|\theta)p(\theta | y) d \theta\)&lt;/span>. Note that the distribution is for a single new observation, &lt;span class="math inline">\(\tilde{y}_i\)&lt;/span>.&lt;/p>
&lt;p>: Expected log, pointwise predictive density. This is an expectation of the log score of the posterior predictive distribution for each observation, summed across all observations in a dataset: $_{i=1}^{n} p_t(_i)(_i | y ) d _i $. The distribution &lt;span class="math inline">\(p_t(\tilde{y}_i)\)&lt;/span> is the true (unknown) data-generating distribution for observation &lt;span class="math inline">\(\tilde{y}_i\)&lt;/span> . Roughly, the ELPD measures the predictive ability of a model by first scoring how likely a model “thinks” a particular observation will be (through the log of the posterior predictive distribution), then weighting that score by the probability of observing that observation (through the true, data generating distribution), and finally combining these weighted scores across all possible values of each observation and all observations in a dataset (the integration and summation).&lt;/p></description></item><item><title>serial dependence experiment demo, online</title><link>https://psadil.github.io/psadil/post/serial-dependence-experiment-demo-online/</link><pubDate>Thu, 25 Jun 2020 00:00:00 +0000</pubDate><guid>https://psadil.github.io/psadil/post/serial-dependence-experiment-demo-online/</guid><description>
&lt;script src="https://psadil.github.io/psadil/post/serial-dependence-experiment-demo-online/index_files/header-attrs/header-attrs.js">&lt;/script>
&lt;p>A breadcrumb trail of post &lt;a href="https://psadil.github.io/psadil/post/derivative-of-gaussian-for-serial-dependence/">topics&lt;/a> &lt;a href="https://psadil.github.io/psadil/post/serial-dependence-reflects-a-preference-for-low-variability/">suggests&lt;/a> that I have been working on an experiment to explore &lt;a href="https://psadil.github.io/psadil/post/serial-dependence/">serial dependence&lt;/a>. When I explain these kinds of experiments to people outside of cognitive psychology, I find it hard to explain the experiment’s simplicity. The tasks are basic.&lt;/p>
&lt;p>Today, I am happy to report that I now have an online demonstration of the task! The demo can be reached &lt;a href="https://morning-earth-15290.herokuapp.com/serialdependence">here&lt;/a>. The experiment will start with a few pages of (poorly formatted) instructions. After that, you’ll do 11 trials.&lt;/p></description></item><item><title>counterbalanced continuous designs with eulerian walks</title><link>https://psadil.github.io/psadil/post/counterbalanced-continuous-designs-with-eulerian-walks/</link><pubDate>Sat, 14 Mar 2020 00:00:00 +0000</pubDate><guid>https://psadil.github.io/psadil/post/counterbalanced-continuous-designs-with-eulerian-walks/</guid><description>
&lt;script src="https://psadil.github.io/psadil/post/counterbalanced-continuous-designs-with-eulerian-walks/index.en_files/header-attrs/header-attrs.js">&lt;/script>
&lt;script src="https://psadil.github.io/psadil/post/counterbalanced-continuous-designs-with-eulerian-walks/index.en_files/htmlwidgets/htmlwidgets.js">&lt;/script>
&lt;script src="https://psadil.github.io/psadil/post/counterbalanced-continuous-designs-with-eulerian-walks/index.en_files/viz/viz.js">&lt;/script>
&lt;link href="https://psadil.github.io/psadil/post/counterbalanced-continuous-designs-with-eulerian-walks/index.en_files/DiagrammeR-styles/styles.css" rel="stylesheet" />
&lt;script src="https://psadil.github.io/psadil/post/counterbalanced-continuous-designs-with-eulerian-walks/index.en_files/grViz-binding/grViz.js">&lt;/script>
&lt;p>Many experiments require counterbalancing sequences of trials. For example, I’m currently running an experiment on &lt;a href="https://psadil.github.io/psadil/post/serial-dependence/">serial dependence&lt;/a>&lt;a href="#fn1" class="footnote-ref" id="fnref1">&lt;sup>1&lt;/sup>&lt;/a>. In my experiment, participants report the orientation of a grating&lt;a href="#fn2" class="footnote-ref" id="fnref2">&lt;sup>2&lt;/sup>&lt;/a> stimulus on each trial. The serial dependence effect is how their responses on one trial depend on either the orientation of the previous trial or their response on that trial. To tease apart the effects of prior stimuli from prior responses, I’m manipulating the visual contrast of the gratings ( &lt;a href="https://en.wikipedia.org/wiki/Contrast_(vision)#Michelson_contrast">Michelson contrast&lt;/a> ). There are three levels of contrast: high, low, and zero (at zero contrast, there is no grating stimulus). This experiment will only need a few of the eight possible pairs of contrasts, and I’d like a sequence of trials that does not have any filler trials. So I need a flexible way to generate sequences of contrast.&lt;/p>
&lt;p>It turns out that this problem can be formulated as constructing an &lt;a href="https://en.wikipedia.org/wiki/Eulerian_path">Eulerian, directed cycle&lt;/a>. There are likely other ways &lt;a href="#fn3" class="footnote-ref" id="fnref3">&lt;sup>3&lt;/sup>&lt;/a>, but I think this is a neat approach. I won’t talk much about why any of this works, primarily because I don’t feel qualified to do so. However, the post includes a script that implements the algorithm, and checks that it has worked. So, hopefully it’ll be useful to at least a future me. But before discussing an Eulerian circuit, let’s talk about formulating the stimulus conditions as a graph.&lt;/p>
&lt;div id="trials-can-be-represented-with-a-graph" class="section level1">
&lt;h1>Trials can be represented with a graph&lt;/h1>
&lt;p>All potential sequences of trials will be represented as a graph. The graphs nodes will correspond to conditions, and edges between the nodes will correspond to allowable transitions. To represent these graphs, I’ll use the &lt;a href="http://visualizers.co/diagrammer/">&lt;code>DiagrammeR&lt;/code> package&lt;/a>.&lt;/p>
&lt;pre class="r">&lt;code># library(DiagrammeR)
library(magrittr)
# library(dplyr)&lt;/code>&lt;/pre>
&lt;p>In the graph of my experiment, there will be three nodes for each of the three conditions (Figure &lt;a href="#fig:nodes">1&lt;/a>).&lt;/p>
&lt;pre class="r">&lt;code>nodes &amp;lt;- DiagrammeR::create_node_df(
n = 3,
label = c(&amp;quot;zero&amp;quot;,&amp;quot;low&amp;quot;,&amp;quot;high&amp;quot;))
DiagrammeR::create_graph(nodes_df = nodes) %&amp;gt;%
DiagrammeR::render_graph(layout = &amp;quot;tree&amp;quot;)&lt;/code>&lt;/pre>
&lt;div class="figure">&lt;span id="fig:nodes">&lt;/span>
&lt;div id="htmlwidget-1" style="width:672px;height:480px;" class="grViz html-widget">&lt;/div>
&lt;script type="application/json" data-for="htmlwidget-1">{"x":{"diagram":"digraph {\n\ngraph [layout = \"neato\",\n outputorder = \"edgesfirst\",\n bgcolor = \"white\"]\n\nnode [fontname = \"Helvetica\",\n fontsize = \"10\",\n shape = \"circle\",\n fixedsize = \"true\",\n width = \"0.5\",\n style = \"filled\",\n fillcolor = \"aliceblue\",\n color = \"gray70\",\n fontcolor = \"gray50\"]\n\nedge [fontname = \"Helvetica\",\n fontsize = \"8\",\n len = \"1.5\",\n color = \"gray80\",\n arrowsize = \"0.5\"]\n\n \"1\" [label = \"zero\", fillcolor = \"#F0F8FF\", fontcolor = \"#000000\", pos = \"0,1!\"] \n \"2\" [label = \"low\", fillcolor = \"#F0F8FF\", fontcolor = \"#000000\", pos = \"1,1!\"] \n \"3\" [label = \"high\", fillcolor = \"#F0F8FF\", fontcolor = \"#000000\", pos = \"2,1!\"] \n}","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}&lt;/script>
&lt;p class="caption">
Figure 1: Node represent experimental conditions.
&lt;/p>
&lt;/div>
&lt;p>In my experiment, I want trials to go from low to high, zero to high, or high to high, high to low, and high to zero (Figure &lt;a href="#fig:edges">2&lt;/a>). Including only these five types of transitions means excluding a few of the possible edges that could be in the graph. For example, I do not want any zero contrast trials to follow any other zero contrast trials, nor do I want a low contrast trial to follow a zero contrast trial.&lt;/p>
&lt;pre class="r">&lt;code>edges &amp;lt;- DiagrammeR::create_edge_df(
from = c(1,2,3,3,3),
to = c(3,3,3,1,2))
DiagrammeR::create_graph(
nodes_df = nodes,
edges_df = edges) %&amp;gt;%
DiagrammeR::render_graph(
layout = &amp;quot;tree&amp;quot;)&lt;/code>&lt;/pre>
&lt;div class="figure">&lt;span id="fig:edges">&lt;/span>
&lt;div id="htmlwidget-2" style="width:672px;height:480px;" class="grViz html-widget">&lt;/div>
&lt;script type="application/json" data-for="htmlwidget-2">{"x":{"diagram":"digraph {\n\ngraph [layout = \"neato\",\n outputorder = \"edgesfirst\",\n bgcolor = \"white\"]\n\nnode [fontname = \"Helvetica\",\n fontsize = \"10\",\n shape = \"circle\",\n fixedsize = \"true\",\n width = \"0.5\",\n style = \"filled\",\n fillcolor = \"aliceblue\",\n color = \"gray70\",\n fontcolor = \"gray50\"]\n\nedge [fontname = \"Helvetica\",\n fontsize = \"8\",\n len = \"1.5\",\n color = \"gray80\",\n arrowsize = \"0.5\"]\n\n \"1\" [label = \"zero\", fillcolor = \"#F0F8FF\", fontcolor = \"#000000\", pos = \"0,2!\"] \n \"2\" [label = \"low\", fillcolor = \"#F0F8FF\", fontcolor = \"#000000\", pos = \"1,2!\"] \n \"3\" [label = \"high\", fillcolor = \"#F0F8FF\", fontcolor = \"#000000\", pos = \"0.5,1!\"] \n \"1\"->\"3\" \n \"2\"->\"3\" \n \"3\"->\"3\" \n \"3\"->\"1\" \n \"3\"->\"2\" \n}","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}&lt;/script>
&lt;p class="caption">
Figure 2: Directed edges between nodes represent allowable transitions.
&lt;/p>
&lt;/div>
&lt;p>Constructing a sequence of trials will correspond to walking along the edges, from node to node. That walk will be Eulerian if each edge is be visited exactly once. With so few edges, it’s easy enough to visualize an Eulerian walk through the edges. One possible Eulerian walk (a cycle&lt;a href="#fn4" class="footnote-ref" id="fnref4">&lt;sup>4&lt;/sup>&lt;/a>, even) is shown in Figure &lt;a href="#fig:smallwalk">3&lt;/a>.&lt;/p>
&lt;pre class="r">&lt;code>edges_labelled &amp;lt;- DiagrammeR::create_edge_df(
from = c(3,2,3,3,1),
to = c(2,3,3,1,3),
label = as.character(1:5))
DiagrammeR::create_graph(
nodes_df = nodes,
edges_df = edges_labelled) %&amp;gt;%
DiagrammeR::render_graph(
layout = &amp;quot;tree&amp;quot;)&lt;/code>&lt;/pre>
&lt;div class="figure">&lt;span id="fig:smallwalk">&lt;/span>
&lt;div id="htmlwidget-3" style="width:672px;height:480px;" class="grViz html-widget">&lt;/div>
&lt;script type="application/json" data-for="htmlwidget-3">{"x":{"diagram":"digraph {\n\ngraph [layout = \"neato\",\n outputorder = \"edgesfirst\",\n bgcolor = \"white\"]\n\nnode [fontname = \"Helvetica\",\n fontsize = \"10\",\n shape = \"circle\",\n fixedsize = \"true\",\n width = \"0.5\",\n style = \"filled\",\n fillcolor = \"aliceblue\",\n color = \"gray70\",\n fontcolor = \"gray50\"]\n\nedge [fontname = \"Helvetica\",\n fontsize = \"8\",\n len = \"1.5\",\n color = \"gray80\",\n arrowsize = \"0.5\"]\n\n \"1\" [label = \"zero\", fillcolor = \"#F0F8FF\", fontcolor = \"#000000\", pos = \"0,2!\"] \n \"2\" [label = \"low\", fillcolor = \"#F0F8FF\", fontcolor = \"#000000\", pos = \"1,2!\"] \n \"3\" [label = \"high\", fillcolor = \"#F0F8FF\", fontcolor = \"#000000\", pos = \"0.5,1!\"] \n\"3\"->\"2\" [label = \"1\"] \n\"2\"->\"3\" [label = \"2\"] \n\"3\"->\"3\" [label = \"3\"] \n\"3\"->\"1\" [label = \"4\"] \n\"1\"->\"3\" [label = \"5\"] \n}","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}&lt;/script>
&lt;p class="caption">
Figure 3: The numbers trace an Eulerian cycle on this graph.
&lt;/p>
&lt;/div>
&lt;p>The cycle in Figure &lt;a href="#fig:smallwalk">3&lt;/a> implies a workable sequence of six trials, but the use of this Eulerian conceptualization will be how it automates creating much longer sequences. For example, to achieve 21 trials the edges could be replicated four times. Figure &lt;a href="#fig:messy">4&lt;/a> shows the graph with replicated edges, and already it looks too messy to traverse by sight. A real experiment will involve hundreds of trials, meaning that we’d like a way to automatically traverse an Eulerian circuit.&lt;/p>
&lt;pre class="r">&lt;code>edges_messy &amp;lt;- DiagrammeR::create_edge_df(
from = rep(c(3,2,3,3,1), each=4),
to = rep(c(2,3,3,1,3), each=4))
DiagrammeR::create_graph(
nodes_df = nodes,
edges_df = edges_messy) %&amp;gt;%
DiagrammeR::render_graph(layout = &amp;quot;tree&amp;quot;)&lt;/code>&lt;/pre>
&lt;div class="figure">&lt;span id="fig:messy">&lt;/span>
&lt;div id="htmlwidget-4" style="width:672px;height:480px;" class="grViz html-widget">&lt;/div>
&lt;script type="application/json" data-for="htmlwidget-4">{"x":{"diagram":"digraph {\n\ngraph [layout = \"neato\",\n outputorder = \"edgesfirst\",\n bgcolor = \"white\"]\n\nnode [fontname = \"Helvetica\",\n fontsize = \"10\",\n shape = \"circle\",\n fixedsize = \"true\",\n width = \"0.5\",\n style = \"filled\",\n fillcolor = \"aliceblue\",\n color = \"gray70\",\n fontcolor = \"gray50\"]\n\nedge [fontname = \"Helvetica\",\n fontsize = \"8\",\n len = \"1.5\",\n color = \"gray80\",\n arrowsize = \"0.5\"]\n\n \"1\" [label = \"zero\", fillcolor = \"#F0F8FF\", fontcolor = \"#000000\", pos = \"0,2!\"] \n \"2\" [label = \"low\", fillcolor = \"#F0F8FF\", fontcolor = \"#000000\", pos = \"1,2!\"] \n \"3\" [label = \"high\", fillcolor = \"#F0F8FF\", fontcolor = \"#000000\", pos = \"0.5,1!\"] \n \"3\"->\"2\" \n \"3\"->\"2\" \n \"3\"->\"2\" \n \"3\"->\"2\" \n \"2\"->\"3\" \n \"2\"->\"3\" \n \"2\"->\"3\" \n \"2\"->\"3\" \n \"3\"->\"3\" \n \"3\"->\"3\" \n \"3\"->\"3\" \n \"3\"->\"3\" \n \"3\"->\"1\" \n \"3\"->\"1\" \n \"3\"->\"1\" \n \"3\"->\"1\" \n \"1\"->\"3\" \n \"1\"->\"3\" \n \"1\"->\"3\" \n \"1\"->\"3\" \n}","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}&lt;/script>
&lt;p class="caption">
Figure 4: Replicating edges quickly complicates the graph.
&lt;/p>
&lt;/div>
&lt;/div>
&lt;div id="hierholzers-algorithm-automates-eulerian-cycles" class="section level1">
&lt;h1>Hierholzer’s algorithm automates Eulerian cycles&lt;/h1>
&lt;p>Fortunately, there exists and algorithm for making Eulerian cycles that is both simple to implement and quick to run. First, here is a helper function to replicate edges, &lt;code>replicate_edges&lt;/code>.&lt;/p>
&lt;pre class="r">&lt;code>#&amp;#39; replicate_edges
#&amp;#39;
#&amp;#39; @param edge_df output of DiagrammeR::create_edge_df (will only need columns `to` and `from`)
#&amp;#39; @param n_reps integer number of times that the edges should be replicated
#&amp;#39;
#&amp;#39; @return replicated edge dataframe
replicate_edges &amp;lt;- function(edge_df, n_reps){
replicate(n_reps, edge_df, simplify = FALSE) %&amp;gt;%
dplyr::bind_rows() %&amp;gt;%
dplyr::mutate(id = 1:dplyr::n())
}&lt;/code>&lt;/pre>
&lt;p>The next function will generate the Eulerian circuit, &lt;code>walk_circuit&lt;/code>. It will take in an edge dataframe (possibly replicated) and output a vector containing the nodes listed in the order that they were reached. Again, I won’t spend too long explaining why this works. But the basic idea is to traverse the edges, deleting edges as you walk along them. You’ll eventually reach a dead-end. If there are still more edges, then backtrack until you can travel along an edge that will result in a different dead-end. Save a list of the nodes that were traveled while backtracking, and these nodes will contain the circuit.&lt;/p>
&lt;pre class="r">&lt;code>#&amp;#39; walk_circuit
#&amp;#39;
#&amp;#39; @param edge_df edge dataframes
#&amp;#39; @param curr_v vertex at which to start the circuit
#&amp;#39;
#&amp;#39; @return vector consisting of Eulerian circuit along edges
#&amp;#39;
#&amp;#39; @details modified python script from https://gregorulm.com/finding-an-eulerian-path/.
walk_circuit &amp;lt;- function(edge_df, curr_v){
# helpful to have the edges stored by node
adj &amp;lt;- edge_df %&amp;gt;%
dplyr::group_split(from)
# vector to store final circuit
circuit &amp;lt;- c()
# Maintain a stack to keep vertices
# start from given node
curr_path &amp;lt;- curr_v
while (length(curr_path)){
# If there&amp;#39;s a remaining edge
if (nrow(adj[[curr_v]])){
# Push the vertex
curr_path &amp;lt;- c(curr_path,curr_v)
# Find the next vertex using an edge
next_v_ind &amp;lt;- sample.int(nrow(adj[[curr_v]]), size=1)
next_v &amp;lt;- adj[[curr_v]]$to[next_v_ind]
# and remove that edge
adj[[curr_v]] &amp;lt;- adj[[curr_v]][-next_v_ind,]
# Move to next vertex
curr_v &amp;lt;- next_v
} else{ # back-track to find remaining circuit
circuit &amp;lt;- c(circuit, curr_v)
# Back-tracking
curr_v &amp;lt;- tail(curr_path, n = 1)
curr_path &amp;lt;- head(curr_path, n = -1)
}
}
return(rev(circuit))
}&lt;/code>&lt;/pre>
&lt;p>Now replicate the edges twice and go for and Eulerian tour.&lt;/p>
&lt;pre class="r">&lt;code>edges_twice &amp;lt;- replicate_edges(edges, 2)
walk_circuit(edges_twice, 3)&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] 3 1 3 2 3 1 3 3 2 3 3&lt;/code>&lt;/pre>
&lt;p>This sequence is small enough that it’s feasible to verify the Eulerian property by hand, but it’ll be nice to have automate the checking. That is the purpose of this next function, &lt;code>check_blocking&lt;/code>.&lt;/p>
&lt;pre class="r">&lt;code>#&amp;#39; check_blocking
#&amp;#39;
#&amp;#39; @param circuit output of walk_circuit
#&amp;#39; @param nodes nodes_df, output of DiagrammeR::create_node_df. Used to label which nodes were visited during the walk
#&amp;#39;
#&amp;#39; @return tbl containing the counts of each transition type contained in the circuit.
#&amp;#39; If all went well, the counts should be equal
check_blocking &amp;lt;- function(circuit, nodes){
tibble::tibble(contrast = circuit, .name_repair = &amp;quot;check_unique&amp;quot;) %&amp;gt;%
dplyr::mutate(
trial = 1:dplyr::n(),
contrast = nodes$label[contrast],
last_contrast = dplyr::lag(contrast)) %&amp;gt;%
dplyr::filter(trial &amp;gt; 1) %&amp;gt;%
dplyr::group_by(contrast, last_contrast) %&amp;gt;%
dplyr::summarise(n = dplyr::n(), .groups = &amp;quot;drop&amp;quot;)
}&lt;/code>&lt;/pre>
&lt;p>Now, generate a sequence of 101 trials,&lt;/p>
&lt;pre class="r">&lt;code>edges_large &amp;lt;- replicate_edges(edges, n_reps = 20)
circuit &amp;lt;- walk_circuit(edges_large, 3)
circuit&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] 3 2 3 3 1 3 2 3 2 3 3 1 3 1 3 3 1 3 3 2 3 1 3 2 3 2 3 1 3 3 1 3 3 2 3 3 3
## [38] 1 3 1 3 1 3 3 2 3 2 3 3 2 3 2 3 1 3 1 3 3 2 3 1 3 1 3 3 2 3 1 3 3 3 3 3 2
## [75] 3 3 1 3 2 3 3 2 3 1 3 1 3 3 1 3 2 3 2 3 1 3 3 2 3 2 3&lt;/code>&lt;/pre>
&lt;p>and check that each transition happened equally often&lt;/p>
&lt;pre class="r">&lt;code>check_blocking(circuit, nodes) %&amp;gt;%
knitr::kable()&lt;/code>&lt;/pre>
&lt;table>
&lt;thead>
&lt;tr class="header">
&lt;th align="left">contrast&lt;/th>
&lt;th align="left">last_contrast&lt;/th>
&lt;th align="right">n&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr class="odd">
&lt;td align="left">high&lt;/td>
&lt;td align="left">high&lt;/td>
&lt;td align="right">20&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="left">high&lt;/td>
&lt;td align="left">low&lt;/td>
&lt;td align="right">20&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td align="left">high&lt;/td>
&lt;td align="left">zero&lt;/td>
&lt;td align="right">20&lt;/td>
&lt;/tr>
&lt;tr class="even">
&lt;td align="left">low&lt;/td>
&lt;td align="left">high&lt;/td>
&lt;td align="right">20&lt;/td>
&lt;/tr>
&lt;tr class="odd">
&lt;td align="left">zero&lt;/td>
&lt;td align="left">high&lt;/td>
&lt;td align="right">20&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;/div>
&lt;div class="footnotes">
&lt;hr />
&lt;ol>
&lt;li id="fn1">&lt;p>Well, I was running. Out of precaution for COVID-19, it currently seems like a bad idea to try to collect more participants. And UMass is closed for the rest of the semester.&lt;a href="#fnref1" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn2">&lt;p>adjacent black and white lines cropped to a circle, where the transitions between luminance follows a sinusoid&lt;a href="#fnref2" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn3">&lt;p>In this particular case, a simpler solution would be to assign each pair of contrasts a number. For example,&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>high -&amp;gt; high&lt;/li>
&lt;li>low -&amp;gt; high&lt;/li>
&lt;li>zero -&amp;gt; high&lt;/li>
&lt;/ol>
&lt;p>An appropriate sequence could be generated by simply permuting the numbers. For example 2, 3, 1, 3, 2 In that case, the sequence of trials would be &lt;code>low high zero high high high zero high low high&lt;/code>. This works because the second trial of each of the transitions are &lt;code>high&lt;/code>. But what if you also wanted a few &lt;code>low-&amp;gt;low&lt;/code> and &lt;code>zero-&amp;gt;zero&lt;/code> transitions, but wanted neither &lt;code>low-&amp;gt;zero&lt;/code> nor &lt;code>zero-&amp;gt;low&lt;/code>? By simply permuting the number codes, a &lt;code>zero-&amp;gt;zero&lt;/code> transition could appear right after a &lt;code>low-&amp;gt;low&lt;/code> transition, but to do that would require a filler &lt;code>low-zero&lt;/code>.&lt;a href="#fnref3" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn4">&lt;p>a cycle or circuit is a walk that starts and ends at the same node&lt;a href="#fnref4" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>thoughts on eye movement</title><link>https://psadil.github.io/psadil/post/thoughts-on-eye-movement/</link><pubDate>Thu, 21 Nov 2019 00:00:00 +0000</pubDate><guid>https://psadil.github.io/psadil/post/thoughts-on-eye-movement/</guid><description>
&lt;script src="https://psadil.github.io/psadil/post/thoughts-on-eye-movement/index.en_files/header-attrs/header-attrs.js">&lt;/script>
&lt;p>Visual perception research has produced many illusions. Stare at a waterfall for a minute, then look away and the whole world appears in motion, traveling upwards &lt;span class="citation">(&lt;a href="#ref-addams1834" role="doc-biblioref">Addams 1834&lt;/a>)&lt;/span>. Given proper lighting, black paper can appear white, but placing a white piece of paper nearby colors the first dark gray &lt;span class="citation">(&lt;a href="#ref-gelb1929" role="doc-biblioref">Gelb 1929&lt;/a>; as cited by &lt;a href="#ref-cataliotti1995" role="doc-biblioref">Cataliotti and Gilchrist 1995&lt;/a>)&lt;/span>. Inspecting two sets of black lines – horizontal lines that obscure a solid red field like a picket fence, along with vertical lines that obscure a solid green field – causes the black lines alone to induce a perception of color, an illusory shading that can last for days &lt;span class="citation">(&lt;a href="#ref-mccollough1965" role="doc-biblioref">McCollough 1965&lt;/a>)&lt;/span>. Such illusions reveal the intricacies of visual perception, kludges and all&lt;a href="#fn1" class="footnote-ref" id="fnref1">&lt;sup>1&lt;/sup>&lt;/a>. The pizzazz of the illusions affords visual perception a kind of scientific rigor; the effects are obviously real and reproducible, so to satisfyingly explain how visual perception works &lt;em>so well&lt;/em> also requires explaining how atypical visual environments can so often dupe vision.&lt;/p>
&lt;p>However, research on visual perception inevitably strays from fascinating and easily demonstrable illusions. Of course, even without the glamour of classic visual illusions an effect can still be a reliable and valid object of research. But as the effect becomes more subtle, observing the effect requires increasingly complex analyses. Unfortunately, the most complex analyses, when misapplied, can also transmute noise into something that appears noteworthy&lt;a href="#fn2" class="footnote-ref" id="fnref2">&lt;sup>2&lt;/sup>&lt;/a>. So when a subtle effect relies entirely on a complex analysis, the effect becomes suspect. To highlight the obviousness of an effect, it can help to visualize and revisualize the data.&lt;/p>
&lt;p>A few weeks ago, &lt;a href="https://psadil.github.io/psadil/post/serial-dependence/">I posted&lt;/a> about an effect that warrants revisualizing, the apparent stability of visual perception. I discussed this stability as one of the reasons that perception exhibits serial dependence &lt;span class="citation">(&lt;a href="#ref-fischer2014" role="doc-biblioref">Fischer and Whitney 2014&lt;/a>)&lt;/span>, which is the tendency of visual perception to be slightly erroneous, resembling not an accurate reproduction of the input it receives but a mixture of current input and input from the past. This dependence may occur during perception to refine the inherently erratic input provided by the retina. I attempt to demonstrate the need for refinement with Figures 1-3. Figure &lt;a href="#fig:density">1&lt;/a> shows a stimulus from an ongoing experiment. In the experiment, the participant was required to simply hold their gaze still. The figure is a heatmap, showing that this participant successfully fixated on a small region of the stimulus. However, the heatmap obscures how fixating on a “small” region implies ample movement. Figure &lt;a href="#fig:centric1">2&lt;/a> recapitulates, in real time, how the gaze wandered during fixation. But then Figure &lt;a href="#fig:centric1">2&lt;/a> obscures what that wandering means for the visual system; whenever the eye moves, the retina receives (and so must then output) a different image. With Figure &lt;a href="#fig:final">3&lt;/a>, I attempt to visualize what these eye movements mean for the retinal image. In Figure &lt;a href="#fig:final">3&lt;/a>, the movements of the gaze are transferred to the stimulus, revealing how the retina receives a twitching stimulus. The effect to explain here is why fixating at the dot in Figure &lt;a href="#fig:density">1&lt;/a> – given that the eyes move as shown in Figure &lt;a href="#fig:centric1">2&lt;/a> – does not elicit the jumpy movie depicted in Figure &lt;a href="#fig:final">3&lt;/a>, but instead elicits the stable image of Figure &lt;a href="#fig:density">1&lt;/a>.&lt;/p>
&lt;div class="figure">&lt;span id="fig:density">&lt;/span>
&lt;img src="https://psadil.github.io/psadil/post/thoughts-on-eye-movement/index.en_files/figure-html/density-1.png" alt="Example stimulus, with heatmap of eye positions during one trial overlayed. Participants viewed such grating stimuli, each for five seconds. They were instructed to fixate on the central magenta dot. The blob to the left of the dot indicates where this participant looked during the trial; the brightest regions held their gaze for the most time." width="672" />
&lt;p class="caption">
Figure 1: Example stimulus, with heatmap of eye positions during one trial overlayed. Participants viewed such grating stimuli, each for five seconds. They were instructed to fixate on the central magenta dot. The blob to the left of the dot indicates where this participant looked during the trial; the brightest regions held their gaze for the most time.
&lt;/p>
&lt;/div>
&lt;div class="figure">&lt;span id="fig:centric1">&lt;/span>
&lt;img src="https://psadil.github.io/psadil/post/thoughts-on-eye-movement/index.en_files/figure-html/centric1-1.gif" alt="Time course of fixations from Figure 1. The blue dot shows where the participant looked at each moment. Fixations were recorded at 1000 Hz, but this video has been downsampled to 10 Hz." />
&lt;p class="caption">
Figure 2: Time course of fixations from Figure 1. The blue dot shows where the participant looked at each moment. Fixations were recorded at 1000 Hz, but this video has been downsampled to 10 Hz.
&lt;/p>
&lt;/div>
&lt;div class="figure">&lt;span id="fig:final">&lt;/span>
&lt;img src="https://psadil.github.io/psadil/post/thoughts-on-eye-movement/index.en_files/figure-html/final-1.gif" alt="Approximate example of the retinal image from Figure 2. While the gaze travels throughout the visual environment, that environment is largely stable. Yet the travelling gaze constantly alters the image imprinted on the retina." />
&lt;p class="caption">
Figure 3: Approximate example of the retinal image from Figure 2. While the gaze travels throughout the visual environment, that environment is largely stable. Yet the travelling gaze constantly alters the image imprinted on the retina.
&lt;/p>
&lt;/div>
&lt;div id="references" class="section level1 unnumbered">
&lt;h1>References&lt;/h1>
&lt;div id="refs" class="references csl-bib-body hanging-indent">
&lt;div id="ref-addams1834" class="csl-entry">
Addams, Roberts. 1834. &lt;span>“An Account of a Peculiar Optical Phenomenon Seen After Having Looked at a Moving Body.”&lt;/span> &lt;em>The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science&lt;/em> 5 (29): 373–74.
&lt;/div>
&lt;div id="ref-cataliotti1995" class="csl-entry">
Cataliotti, Joseph, and Alan Gilchrist. 1995. &lt;span>“Local and Global Processes in Surface Lightness Perception.”&lt;/span> &lt;em>Perception &amp;amp; Psychophysics&lt;/em> 57 (2): 125–35.
&lt;/div>
&lt;div id="ref-fischer2014" class="csl-entry">
Fischer, Jason, and David Whitney. 2014. &lt;span>“Serial Dependence in Visual Perception.”&lt;/span> &lt;em>Nature Neuroscience&lt;/em> 17 (5): 738.
&lt;/div>
&lt;div id="ref-gelb1929" class="csl-entry">
Gelb, Adhémar. 1929. &lt;span>“Die "Farbenkonstanz" Der Sehdinge.”&lt;/span> In &lt;em>Handbuch Der Normalen Und Pathologischen Physiologie&lt;/em>, 594–678. Springer-Verlag.
&lt;/div>
&lt;div id="ref-mccollough1965" class="csl-entry">
McCollough, Celeste. 1965. &lt;span>“Color Adaptation of Edge-Detectors in the Human Visual System.”&lt;/span> &lt;em>Science&lt;/em> 149 (3688): 1115–16.
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="footnotes">
&lt;hr />
&lt;ol>
&lt;li id="fn1">&lt;p>To personally experience the illusions mentioned here – and many others – explore &lt;a href="https://michaelbach.de/ot/">Michael Bach’s website&lt;/a>.&lt;a href="#fnref1" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn2">&lt;p>Full disclosure: I write this as someone who has written &lt;a href="https://psadil.github.io/psadil/publication/sadil-2019-hierarchical/">a paper&lt;/a> on a novel development of an already obscure analysis, a development that I needed to support the claims in &lt;a href="https://psadil.github.io/psadil/publication/sadil-2019-connecting/">another paper&lt;/a>. I am a kettle.&lt;a href="#fnref2" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>Half of a parameter</title><link>https://psadil.github.io/psadil/post/half-of-a-parameter/</link><pubDate>Fri, 15 Nov 2019 00:00:00 +0000</pubDate><guid>https://psadil.github.io/psadil/post/half-of-a-parameter/</guid><description>
&lt;script src="https://psadil.github.io/psadil/post/half-of-a-parameter/index_files/header-attrs/header-attrs.js">&lt;/script>
&lt;p>Science produces models that provide parsimonious descriptions of the world. In cognitive psychology, models regularly compete to explain a few phenomena. But models can survive experiment after experiment, both because of the difficulty of capturing participant’s nuanced behavior, and because models often make highly overlapping predictions. In these cases, a model succeeds through its relative parsimony.&lt;/p>
&lt;p>In cognitive psychology, measures of information criteria, specifically Akaike’s and the Bayesian information criteria &lt;span class="citation">(&lt;a href="#ref-schwarz1978" role="doc-biblioref">Schwarz 1978&lt;/a>; &lt;a href="#ref-akaike1998" role="doc-biblioref">Akaike 1973&lt;/a>)&lt;/span>, determine a winning model. These criteria measure complexity by tallying the number of parameters in a model. Long maths and specific assumptions justify the claim that a model with more parameters is more complex than a model with fewer parameters, and so does our intuition that a model is complex if it has many moving parts. Unfortunately, the assumptions fail in common situations, such as when the models are fit in a Bayesian rather than frequentist setting. An alphabet soup of other information criteria exists (in addition to Akaike’s the Bayesian criteria, there is the DIC, WAIC, KIC, NIC, TIC, etc), and these other criteria assign complexity more complexly. These criteria are sensitive not only to the number of parameters in a model but also to the varied roles that a parameter can have. They assign the complexity of a model based on the model’s number of ‘effective parameters.’&lt;/p>
&lt;p>For intuition on why tallying the number of parameters is an insufficient measure complexity, consider two models of response time. Both models assume that response times are distributed according to a normal distribution. In this simple example, the variability of the distributions are known, and so the models have only a single free parameter, which is the average response time. In one model, that average can be any number, a value from negative to positive infinity. This is the kind of model implicitly assumed when we conduct a t-test on the averages of response times. Of course the model is a simplification of response times, but this model also has the glaring flaw that it allows the average response time to be negative; a participant cannot respond to stimulation before the stimulus appears. The second model addresses this flaw by adding the constraint that the average response time cannot be negative. Although the second model is more constrained, the models have the same number of parameters. The second model can only account for half of the patterns of data as the first; the second model is twice as parsimonious as the first &lt;span class="citation">(&lt;a href="#ref-gelman2014" role="doc-biblioref">Gelman, Hwang, and Vehtari 2014&lt;/a>)&lt;/span>. To adjudicate between these model requires a measure that is sensitive to complexity but does not simply tally the number of parameters in each model.&lt;/p>
&lt;div id="references" class="section level1 unnumbered">
&lt;h1>References&lt;/h1>
&lt;div id="refs" class="references csl-bib-body hanging-indent">
&lt;div id="ref-akaike1998" class="csl-entry">
Akaike, Hirotogu. 1973. &lt;span>“Information Theory and an Extension of the Maximum Likelihood Principle.”&lt;/span> In &lt;em>Proceedings of the Second International Symposium on Information Theory&lt;/em>, 267–81.
&lt;/div>
&lt;div id="ref-gelman2014" class="csl-entry">
Gelman, Andrew, Jessica Hwang, and Aki Vehtari. 2014. &lt;span>“Understanding Predictive Information Criteria for &lt;span>Bayesian&lt;/span> Models.”&lt;/span> &lt;em>Statistics and Computing&lt;/em> 24 (6): 997–1016. &lt;a href="https://doi.org/10.1007/s11222-013-9416-2">https://doi.org/10.1007/s11222-013-9416-2&lt;/a>.
&lt;/div>
&lt;div id="ref-schwarz1978" class="csl-entry">
Schwarz, Gideon. 1978. &lt;span>“Estimating the Dimension of a Model.”&lt;/span> &lt;em>The Annals of Statistics&lt;/em> 6 (2): 461–64. &lt;a href="https://doi.org/10.1214/aos/1176344136">https://doi.org/10.1214/aos/1176344136&lt;/a>.
&lt;/div>
&lt;/div>
&lt;/div></description></item><item><title>Staircases for Thresholds, Part 2</title><link>https://psadil.github.io/psadil/post/staircases-for-thresholds-part2/</link><pubDate>Fri, 08 Nov 2019 00:00:00 +0000</pubDate><guid>https://psadil.github.io/psadil/post/staircases-for-thresholds-part2/</guid><description>
&lt;script src="https://psadil.github.io/psadil/post/staircases-for-thresholds-part2/index_files/header-attrs/header-attrs.js">&lt;/script>
&lt;!--Incantation to add equation numbers
https://stackoverflow.com/questions/35026405/auto-number-equations-in-r-markdown-documents-in-rstudio-->
&lt;script type="text/x-mathjax-config">
MathJax.Hub.Config({
TeX: {
equationNumbers: {
autoNumber: "ams"
}
}
});
&lt;/script>
&lt;p>In last week’s post, I discussed how some experiments in cognitive psychology require researchers to pick a differently intense stimulus for each participant. In particular, I discussed a procedure for picking an intensity that elicits positive responses on approximately half of trials, the staircase procedure. In the staircase procedure, the researcher increases the intensity after every positive response and decreases the intensity after every negative response. If the participant completes another set of trials in which the intensity is fixed to the average of the intensities that were used during the staircase, the participant will provide positive responses approximately half of the time. But a researcher may want participants to give positive responses with a different proportion. A different proportion of responses can be achieved by transforming the staircase &lt;span class="citation">(&lt;a href="#ref-levitt1997" role="doc-biblioref">Levitt 1971&lt;/a>)&lt;/span>.&lt;/p>
&lt;p>The original staircase produced an intensity that elicited half positive responses by balancing the proportion of positive and negative responses. Intuitively, to elicit a higher proportion of positive responses, the transformed staircase must tilt this balance by converging on a more intense stimulus. Any staircase affects responses by changing the stimulus intensity as a function of how participants behave. The staircase that changes the intensity after every response is called a one-up, one-down staircase; one negative response causes the intensity to go up, one positive response causes the intensity to go down. A transformed staircase can make a positive response more likely by making intensity decreases less likely. The names of transformed staircases are analogous to the one-up, one-down label: a one-up, two-down staircase increases the stimulus after any negative response and decreases the intensity after two positive responses; a two-up, three-down staircase increases the intensity after two negative responses and only decreases the intensity after three positive responses; and so on. Altering when the staircase increments the intensity alters the intensity at which the staircase converges.&lt;/p>
&lt;p>We can use algebra to calculate the proportion of positive responses elicited by the intensity converged on by a staircase. As an example, consider a staircase that increases the intensity after a single positive response but decreases the intensity after two negative responses, a one-up, two-down staircase. Since the one-up, two-down regime results in fewer decreases than the one-up, one-down staircase, we should expect that the proportion of positive responses will be higher than half. For the calculation, note that there are three possible sequences of responses that result in an intensity change. Two of these sequences cause an increase, either a single negative or a positive followed by a negative. For the algebra later, let &lt;span class="math inline">\(p(x|i)\)&lt;/span> be the probability of obtaining a positive response at stimulus intensity, &lt;span class="math inline">\(i\)&lt;/span>. Our goal is to solve for this probability. Participants can only provide either positive or negative responses, so the probability of obtaining a negative response to that stimulus is &lt;span class="math inline">\(1-p(x|i)\)&lt;/span>. The probability of increasing the stimulus intensity away from intensity &lt;span class="math inline">\(i\)&lt;/span>, &lt;span class="math inline">\(p(\text{up|i})\)&lt;/span> is the sum of the probabilities for the two sequences:&lt;/p>
&lt;p>&lt;span class="math display">\[
\begin{equation}
p(\text{up|i}) = (1-p(x|i)) + p(x|i)(1-p(x|i))
\end{equation}
\]&lt;/span>&lt;/p>
&lt;p>There is only a single way in which the staircase decreases intensity: the participant must provide two positive responses in a row. The probability of the intensity decreasing away from &lt;span class="math inline">\(i\)&lt;/span>, &lt;span class="math inline">\(p(\text{down|i})\)&lt;/span> is equal to the probability of two positive responses to that stimulus, or&lt;/p>
&lt;p>&lt;span class="math display">\[
\begin{equation}
p(\text{down|i}) = p(x|i)^2
\end{equation}
\]&lt;/span>&lt;/p>
&lt;p>Combining equations (1) and (2) will give a relationship that determines the proportion of positive responses participants will tend to provide under this staircase. To see how to combine these equations, remember that the one-up, one-down staircase converged on an intensity for which the proportion of positive and negative responses were equal. This equality occurred because at that intensity, the probability of an up and down step were equally likely. So, to determine at which intensity the one-up, two-down staircase converges, we must determine the probability of a positive response that will make an up and down step likely in the one-up, two-down staircase. That is, we set the right hand sides of equations (1) and (2) to be equal, and solve for &lt;span class="math inline">\(p(x|i)\)&lt;/span>&lt;a href="#fn1" class="footnote-ref" id="fnref1">&lt;sup>1&lt;/sup>&lt;/a>.&lt;/p>
&lt;p>&lt;span class="math display">\[
\begin{equation}
\begin{aligned}
p(x|i)(1-p(x|i)) + (1-p(x|i)) &amp;amp; = p(x|i)^2 \\
\implies p(x|i) - p(x|i)^2 + 1-p(x|i) &amp;amp; = p(x|i)^2 \\
\implies -2 p(x|i)^2 &amp;amp; = -1 \\
\implies p(x|i)&amp;amp; = \frac{1}{\sqrt{2}} \\
&amp;amp; \approx 0.707
\end{aligned}
\end{equation}
\]&lt;/span>&lt;/p>
&lt;p>Equation (3) shows that a one-up, two-down staircase will converge on a stimulus intensity that elicits approximately 70% positive responses (Figure &lt;a href="#fig:staircase">1&lt;/a>). As one way to see that this solution makes sense, relate this solution back to the probabilities of making either an up or down step. By equation (2), this solution implies that at this intensity, that an up step occurs with a 50% probability. As desired, any sequence that elicits a transition has an equal chance of being one that elicits either an up or down step.&lt;/p>
&lt;p>The original staircase procedure capitalized on the idea that the stimulus intensity which elicits half positive responses can be estimated by starting from an arbitrary intensity, changing the intensity on every trial based on whether a participant responded positively or negatively, and then retroactively looking at which intensities were shown. The transformed staircase enables estimation of an intensity that elicits different behavior. Similar algebra to that outlined in this post can be used to determine the proportion of positive responses elicited by other staircases. Unfortunately, most proportions will not have an easy staircase regime. Moreover, complex staircases will only change the stimulus intensity infrequently, requiring more trials to estimate the converged upon intensity stably. However, the proportions reachable by simple staircase are often good enough; rare is the experiment that requires, not 70.7% but 73% positive responses. And the staircase procedure did not require any knowledge of the exact shape of the psychometric function, just that there was a psychometric function. The simplicity of the transformed staircase makes it an attractive way to pick an intensity.&lt;/p>
&lt;div class="figure">&lt;span id="fig:staircase">&lt;/span>
&lt;img src="https://psadil.github.io/psadil/post/staircases-for-thresholds-part2/index_files/figure-html/staircase-1.png" alt="A sequence of trials with stimulus intensity governed by a one-up, two-down staircase. With this staircase, the intensity increases after a single negative response but decreases only after two positive responses. After enough trials, the average of the stimulus intensities shown to participants will elicit approximately 70% positive responses (dashed line). The intensity resulting from a one-up, one-down staircase is shown for comparison (solid line)." width="672" />
&lt;p class="caption">
Figure 1: A sequence of trials with stimulus intensity governed by a one-up, two-down staircase. With this staircase, the intensity increases after a single negative response but decreases only after two positive responses. After enough trials, the average of the stimulus intensities shown to participants will elicit approximately 70% positive responses (dashed line). The intensity resulting from a one-up, one-down staircase is shown for comparison (solid line).
&lt;/p>
&lt;/div>
&lt;div id="references" class="section level1 unnumbered">
&lt;h1>References&lt;/h1>
&lt;div id="refs" class="references csl-bib-body hanging-indent">
&lt;div id="ref-levitt1997" class="csl-entry">
Levitt, H. 1971. &lt;span>“Transformed up-down Methods in Psychoacoustics.”&lt;/span> &lt;em>The Journal of the Acoustical Society of America&lt;/em> 49 (2B): 467–77.
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="footnotes">
&lt;hr />
&lt;ol>
&lt;li id="fn1">&lt;p>The equation will have two solutions, but one of those solutions will be negative. A negative value is not an actual solution, because we are dealing with probabilities and so there is an additional constraint that &lt;span class="math inline">\(0 \leq p(x|i) \leq 1\)&lt;/span>&lt;a href="#fnref1" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>Staircases for Thresholds</title><link>https://psadil.github.io/psadil/post/staircases-for-thresholds/</link><pubDate>Wed, 30 Oct 2019 00:00:00 +0000</pubDate><guid>https://psadil.github.io/psadil/post/staircases-for-thresholds/</guid><description>
&lt;script src="https://psadil.github.io/psadil/post/staircases-for-thresholds/index_files/header-attrs/header-attrs.js">&lt;/script>
&lt;p>Performing any experiment on cognition requires deciding which stimuli to use. Some experiments require participants to make many errors, requiring the stimuli to be challenging. In other experiments, participants must respond accurately, requiring stimuli that are easy but not so easy that participants lose attention. Moreover, participants behave idiosyncratically, so to avoid wasting either the researchers’ or participants’ time the stimuli ought to be tailored to each participant. To decide which stimuli to use, researchers can rely on a psychometric function (Figure &lt;a href="#fig:psychometric">1&lt;/a>). These functions describe a relationship between the intensity of a stimulus and how a participant responds to that stimulus, when responses can be classified as either a positive or negative. Precisely what is meant by ‘intensity’ and ‘positive or negative’ depends on the experimental task, but they roughly correspond to the amount of stimulation on each trial and how difficult it is to notice that stimulation. In a task in which participants must detect a pure tone that is occasionally presented over white noise, the intensity could be the volume of the tone and participants’ responses are positive when they detect the tone. With a psychometric function, deciding on a stimulus translates to picking the proportion of trials that should receive positive responses – picking the desired difficulty – and then using the intensity that elicits that behavior. This replaces the task of picking stimuli with inferring participants’ psychometric functions.&lt;/p>
&lt;div class="figure">&lt;span id="fig:psychometric">&lt;/span>
&lt;img src="https://psadil.github.io/psadil/post/staircases-for-thresholds/index_files/figure-html/psychometric-1.png" alt="A psychometric curve with threshold intensity. Psychometric curves relate the intensity of stimulation to the perception of stimulation, or the proportion positive responses. There are many parameterizations of these functions, but they are typically sigmoidal. The intensity at which, on average, half of responses are positive is often of interest. This intensity is called the threshold." width="672" />
&lt;p class="caption">
Figure 1: A psychometric curve with threshold intensity. Psychometric curves relate the intensity of stimulation to the perception of stimulation, or the proportion positive responses. There are many parameterizations of these functions, but they are typically sigmoidal. The intensity at which, on average, half of responses are positive is often of interest. This intensity is called the threshold.
&lt;/p>
&lt;/div>
&lt;p>To infer psychometric functions, standard procedures exist, though these procedures have varied efficiency. In particular, when only a single stimulus intensity is required, it would be inefficient to estimate the entire function. Consider a researcher attempting to elicit half positive responses, behavior elicited by the so called threshold stimulus intensity. A simple procedure to infer the psychometric function involves presenting a wide range of stimulus intensities, fitting the function to the data, and using the estimated function to infer the threshold. Each datum increases the precision of the estimate, but some data will be more useful than others. Intensities close to the tails of the function will pin down the function at those tails, but functions with different thresholds can behave similarly in their tails (Figure &lt;a href="#fig:psychometric2">2&lt;/a>). The threshold is most tightly constrained by responses to stimuli at the threshold &lt;span class="citation">(&lt;a href="#ref-levitt1997" role="doc-biblioref">Levitt 1971&lt;/a>)&lt;/span>. Therefore, an ideal procedure to estimate the threshold intensity would involve repeatedly presenting the threshold intensity. The ideal procedure is unfeasible, since if the threshold were known there would be no need for inference. But although the exact threshold intensity cannot be presented on every trial, certain procedures enable most trials to approximate the ideal.&lt;/p>
&lt;div class="figure">&lt;span id="fig:psychometric2">&lt;/span>
&lt;img src="https://psadil.github.io/psadil/post/staircases-for-thresholds/index_files/figure-html/psychometric2-1.png" alt="Psychometric functions with different thresholds. Although behavior at the tails of these functions are similar, they have different thresholds." width="672" />
&lt;p class="caption">
Figure 2: Psychometric functions with different thresholds. Although behavior at the tails of these functions are similar, they have different thresholds.
&lt;/p>
&lt;/div>
&lt;p>One type of procedure that locates the threshold intensity, both simply and efficiently, is called a staircase. A staircase procedure changes the stimulus intensity on every trial based on how participants respond. A staircase that locates the threshold increases stimulus intensity after a participant makes a negative response and decreases the intensity after a participant responds positively. Even when the first stimulus has an intensity far from the threshold (Figure &lt;a href="#fig:staircase">3&lt;/a>), the staircase brings the intensity to the threshold, a convergence that is ensured by the psychometric function. For example, when intensity is lower than the threshold, a participant tends to make negative responses. After a negative response, the contrast is increased. With an increased contrast, the participant will be more likely to make a positive response. If the intensity is still lower then then threshold, the participant will likely provide another negative response, causing the intensity increase further. After enough trials with intensity too low, the intensity will be pushed towards the threshold. If the intensity strays from the threshold, the same dynamics push the threshold back to threshold. The staircase forces the intensity to remain close to the threshold.&lt;/p>
&lt;div class="figure">&lt;span id="fig:staircase">&lt;/span>
&lt;img src="https://psadil.github.io/psadil/post/staircases-for-thresholds/index_files/figure-html/staircase-1.png" alt="Example sequence of trials in which stimulus intensity is controlled by a staircase procedure. After each negative response, the intensity increases, and after each positive response the intensity decreases. Although the initial intensity was much lower than the threshold, the staircase brings the intensity close to threshold and then keeps it there." width="672" />
&lt;p class="caption">
Figure 3: Example sequence of trials in which stimulus intensity is controlled by a staircase procedure. After each negative response, the intensity increases, and after each positive response the intensity decreases. Although the initial intensity was much lower than the threshold, the staircase brings the intensity close to threshold and then keeps it there.
&lt;/p>
&lt;/div>
&lt;div id="references" class="section level1 unnumbered">
&lt;h1>References&lt;/h1>
&lt;div id="refs" class="references csl-bib-body hanging-indent">
&lt;div id="ref-levitt1997" class="csl-entry">
Levitt, H. 1971. &lt;span>“Transformed up-down Methods in Psychoacoustics.”&lt;/span> &lt;em>The Journal of the Acoustical Society of America&lt;/em> 49 (2B): 467–77.
&lt;/div>
&lt;/div>
&lt;/div></description></item><item><title>Forward encoding model</title><link>https://psadil.github.io/psadil/post/forward-encoding-model/</link><pubDate>Thu, 24 Oct 2019 00:00:00 +0000</pubDate><guid>https://psadil.github.io/psadil/post/forward-encoding-model/</guid><description>
&lt;script src="https://psadil.github.io/psadil/post/forward-encoding-model/index_files/header-attrs/header-attrs.js">&lt;/script>
&lt;p>Functional magnetic resonance imaging records brain activity with spatially distinct voxels, but this segmentation will be misaligned with a brain’s meaningful boundaries. The segmentation results in some voxels recording activity from different types of tissue – types that are both neural an non-neural – but even voxels that exclusively sample gray matter can span functionally distinct cortex. For example, a 3T scanner allows voxels in the range of 1.5-3 mm&lt;span class="math inline">\(^3\)&lt;/span>, but orientation columns have an average width of 0.8 mm &lt;span class="citation">(&lt;a href="#ref-yacoub2008" role="doc-biblioref">Yacoub, Harel, and Uğurbil 2008&lt;/a>)&lt;/span>. Studying orientation columns with such low resolution requires statistical tools.&lt;/p>
&lt;p>One statistical tool models voxel activity as a linear combination of the activity of a small number of neural channels &lt;span class="citation">(&lt;a href="#ref-brouwer2009" role="doc-biblioref">Brouwer and Heeger 2009&lt;/a>; &lt;a href="#ref-kay2008" role="doc-biblioref">Kay et al. 2008&lt;/a>)&lt;/span>. These models are called forward models, describing how the channel activity transforms into voxel activity. In early sensory cortex, the channels are analogous to cortical columns. In later cortex, the channels are more abstract dimensions of a representational space. Developing a forward model requires assuming not only how many channels contribute of a voxel’s activity, but also the tuning properties of those channels. With these assumptions, regression allows inferring the contribution of each channel to each voxel’s activity. Let &lt;span class="math inline">\(N\)&lt;/span> be the number of observations for each voxel, &lt;span class="math inline">\(M\)&lt;/span> be the number of voxels, and &lt;span class="math inline">\(K\)&lt;/span> be the number of channels within a voxel. The forward model specifies that the data (&lt;span class="math inline">\(B\)&lt;/span>, &lt;span class="math inline">\(M \times N\)&lt;/span>) result from a weighted combination of the assumed channels responses (&lt;span class="math inline">\(C\)&lt;/span>, &lt;span class="math inline">\(K \times N\)&lt;/span>), where the weights (&lt;span class="math inline">\(W\)&lt;/span>, &lt;span class="math inline">\(M \times K\)&lt;/span>) are unknown.&lt;/p>
&lt;p>&lt;span class="math display">\[
B = WC
\]&lt;/span>&lt;/p>
&lt;p>Taking the pseudoinverse of the channel matrix and multiplying the result by the data gives an estimate of the weight matrix:&lt;/p>
&lt;p>&lt;span class="math display">\[
\widehat{W} = BC^T(CC^T)^{-1}
\]&lt;/span>&lt;/p>
&lt;p>Assumptions about &lt;span class="math inline">\(C\)&lt;/span> are assumptions about how the channels encode stimuli. Different encoding schemes can be instantiated with different &lt;span class="math inline">\(C\)&lt;/span>, and any method for comparing linear models could be used to compare the schemes.&lt;/p>
&lt;p>The forward encoding model enables comparison of static encoding schemes, but neural encoding schemes are dynamic. Attentional fluctuations, perceptual learning, and stimulation history all modulate neural tuning functions &lt;span class="citation">(&lt;a href="#ref-mcadams1999" role="doc-biblioref">McAdams and Maunsell 1999&lt;/a>; &lt;a href="#ref-reynolds2000" role="doc-biblioref">Reynolds, Pasternak, and Desimone 2000&lt;/a>; &lt;a href="#ref-siegel2015" role="doc-biblioref">Siegel, Buschman, and Miller 2015&lt;/a>; &lt;a href="#ref-yang2004" role="doc-biblioref">Yang and Maunsell 2004&lt;/a>)&lt;/span>. To explore modulations with functional magnetic resonance imaging, some researchers have inverted the encoding model &lt;span class="citation">(&lt;a href="#ref-garcia2013" role="doc-biblioref">Garcia, Srinivasan, and Serences 2013&lt;/a>; &lt;a href="#ref-rahmati2018" role="doc-biblioref">Rahmati, Saber, and Curtis 2018&lt;/a>; &lt;a href="#ref-saproo2014" role="doc-biblioref">Saproo and Serences 2014&lt;/a>; &lt;a href="#ref-scolari2012" role="doc-biblioref">Scolari, Byers, and Serences 2012&lt;/a>; &lt;a href="#ref-sprague2013" role="doc-biblioref">Sprague and Serences 2013&lt;/a>; &lt;a href="#ref-vo2017" role="doc-biblioref">Vo, Sprague, and Serences 2017&lt;/a>)&lt;/span>. The inversion is a variation of cross validation. The method estimates the weight matrix with only some of the data (e.g., all data excluding a single run). The held out data, &lt;span class="math inline">\(B_H\)&lt;/span>, contains observations from all experimental condition across which the tuning functions might vary. The encoding model is inverted by multiplying the pseudoinverse of the weight matrix with the held out data to estimate a new channel response matrix.&lt;/p>
&lt;p>&lt;span class="math display">\[
\widehat{C} = \widehat{W}^T(\widehat{W}\widehat{W}^T)^{-1}B_H
\]&lt;/span>&lt;/p>
&lt;p>The new channel response matrix estimates how the channels respond in each experimental condition.&lt;/p>
&lt;p>Although validation studies demonstrated that the inverted encoding model enables inferences that recapitulate some modulations observed with electrophysiology &lt;span class="citation">(&lt;a href="#ref-sprague2018" role="doc-biblioref">Sprague et al. 2018&lt;/a>; &lt;a href="#ref-sprague2015" role="doc-biblioref">Sprague, Saproo, and Serences 2015&lt;/a>)&lt;/span>, the inversion also misleads inferences about certain fundamental modulations &lt;span class="citation">(&lt;a href="#ref-gardner2019" role="doc-biblioref">Gardner and Liu 2019&lt;/a>; &lt;a href="#ref-liu2018" role="doc-biblioref">Liu, Cable, and Gardner 2018&lt;/a>)&lt;/span>. In particular, increasing the contrast of an orientation increases the gain of neurons tuned to orientation without altering their tuning bandwidth &lt;span class="citation">(&lt;a href="#ref-alitto2004" role="doc-biblioref">Alitto and Usrey 2004&lt;/a>; &lt;a href="#ref-sclar1982" role="doc-biblioref">Sclar and Freeman 1982&lt;/a>; &lt;a href="#ref-skottun1987" role="doc-biblioref">Skottun et al. 1987&lt;/a>)&lt;/span>, but the inverted encoding model (incorrectly) suggests that higher contrast decreases bandwidth &lt;span class="citation">(&lt;a href="#ref-liu2018" role="doc-biblioref">Liu, Cable, and Gardner 2018&lt;/a>)&lt;/span>. Inferences are misled because the estimated channel responses are constrained by the initial assumptions about &lt;span class="math inline">\(C\)&lt;/span> &lt;span class="citation">(&lt;a href="#ref-gardner2019" role="doc-biblioref">Gardner and Liu 2019&lt;/a>)&lt;/span>. Using the encoding model to study modulations requires a way to estimate the contribution of each channel without assuming a fixed channel response function.&lt;/p>
&lt;div id="references" class="section level1 unnumbered">
&lt;h1>References&lt;/h1>
&lt;div id="refs" class="references csl-bib-body hanging-indent">
&lt;div id="ref-alitto2004" class="csl-entry">
Alitto, Henry J, and W Martin Usrey. 2004. &lt;span>“Influence of Contrast on Orientation and Temporal Frequency Tuning in Ferret Primary Visual Cortex.”&lt;/span> &lt;em>Journal of Neurophysiology&lt;/em> 91 (6): 2797–2808.
&lt;/div>
&lt;div id="ref-brouwer2009" class="csl-entry">
Brouwer, Gijs Joost, and David J Heeger. 2009. &lt;span>“Decoding and Reconstructing Color from Responses in Human Visual Cortex.”&lt;/span> &lt;em>Journal of Neuroscience&lt;/em> 29 (44): 13992–4003.
&lt;/div>
&lt;div id="ref-garcia2013" class="csl-entry">
Garcia, Javier O, Ramesh Srinivasan, and John T Serences. 2013. &lt;span>“Near-Real-Time Feature-Selective Modulations in Human Cortex.”&lt;/span> &lt;em>Current Biology&lt;/em> 23 (6): 515–22.
&lt;/div>
&lt;div id="ref-gardner2019" class="csl-entry">
Gardner, Justin L, and Taosheng Liu. 2019. &lt;span>“Inverted Encoding Models Reconstruct an Arbitrary Model Response, Not the Stimulus.”&lt;/span> &lt;em>eNeuro&lt;/em> 6 (2).
&lt;/div>
&lt;div id="ref-kay2008" class="csl-entry">
Kay, Kendrick N, Thomas Naselaris, Ryan J Prenger, and Jack L Gallant. 2008. &lt;span>“Identifying Natural Images from Human Brain Activity.”&lt;/span> &lt;em>Nature&lt;/em> 452 (7185): 352.
&lt;/div>
&lt;div id="ref-liu2018" class="csl-entry">
Liu, Taosheng, Dylan Cable, and Justin L Gardner. 2018. &lt;span>“Inverted Encoding Models of Human Population Response Conflate Noise and Neural Tuning Width.”&lt;/span> &lt;em>Journal of Neuroscience&lt;/em> 38 (2): 398–408.
&lt;/div>
&lt;div id="ref-mcadams1999" class="csl-entry">
McAdams, Carrie J, and John HR Maunsell. 1999. &lt;span>“Effects of Attention on Orientation-Tuning Functions of Single Neurons in Macaque Cortical Area V4.”&lt;/span> &lt;em>Journal of Neuroscience&lt;/em> 19 (1): 431–41.
&lt;/div>
&lt;div id="ref-rahmati2018" class="csl-entry">
Rahmati, Masih, Golbarg T Saber, and Clayton E Curtis. 2018. &lt;span>“Population Dynamics of Early Visual Cortex During Working Memory.”&lt;/span> &lt;em>Journal of Cognitive Neuroscience&lt;/em> 30 (2): 219–33.
&lt;/div>
&lt;div id="ref-reynolds2000" class="csl-entry">
Reynolds, John H, Tatiana Pasternak, and Robert Desimone. 2000. &lt;span>“Attention Increases Sensitivity of V4 Neurons.”&lt;/span> &lt;em>Neuron&lt;/em> 26 (3): 703–14.
&lt;/div>
&lt;div id="ref-saproo2014" class="csl-entry">
Saproo, Sameer, and John T Serences. 2014. &lt;span>“Attention Improves Transfer of Motion Information Between V1 and MT.”&lt;/span> &lt;em>Journal of Neuroscience&lt;/em> 34 (10): 3586–96.
&lt;/div>
&lt;div id="ref-sclar1982" class="csl-entry">
Sclar, G, and RD Freeman. 1982. &lt;span>“Orientation Selectivity in the Cat’s Striate Cortex Is Invariant with Stimulus Contrast.”&lt;/span> &lt;em>Experimental Brain Research&lt;/em> 46 (3): 457–61.
&lt;/div>
&lt;div id="ref-scolari2012" class="csl-entry">
Scolari, Miranda, Anna Byers, and John T Serences. 2012. &lt;span>“Optimal Deployment of Attentional Gain During Fine Discriminations.”&lt;/span> &lt;em>Journal of Neuroscience&lt;/em> 32 (22): 7723–33.
&lt;/div>
&lt;div id="ref-siegel2015" class="csl-entry">
Siegel, Markus, Timothy J Buschman, and Earl K Miller. 2015. &lt;span>“Cortical Information Flow During Flexible Sensorimotor Decisions.”&lt;/span> &lt;em>Science&lt;/em> 348 (6241): 1352–55.
&lt;/div>
&lt;div id="ref-skottun1987" class="csl-entry">
Skottun, Bernt C, Arthur Bradley, Gary Sclar, Izumi Ohzawa, and Ralph D Freeman. 1987. &lt;span>“The Effects of Contrast on Visual Orientation and Spatial Frequency Discrimination: A Comparison of Single Cells and Behavior.”&lt;/span> &lt;em>Journal of Neurophysiology&lt;/em> 57 (3): 773–86.
&lt;/div>
&lt;div id="ref-sprague2018" class="csl-entry">
Sprague, Thomas C, Kirsten CS Adam, Joshua J Foster, Masih Rahmati, David W Sutterer, and Vy A Vo. 2018. &lt;span>“Inverted Encoding Models Assay Population-Level Stimulus Representations, Not Single-Unit Neural Tuning.”&lt;/span> &lt;em>eNeuro&lt;/em> 5 (3).
&lt;/div>
&lt;div id="ref-sprague2015" class="csl-entry">
Sprague, Thomas C, Sameer Saproo, and John T Serences. 2015. &lt;span>“Visual Attention Mitigates Information Loss in Small-and Large-Scale Neural Codes.”&lt;/span> &lt;em>Trends in Cognitive Sciences&lt;/em> 19 (4): 215–26.
&lt;/div>
&lt;div id="ref-sprague2013" class="csl-entry">
Sprague, Thomas C, and John T Serences. 2013. &lt;span>“Attention Modulates Spatial Priority Maps in the Human Occipital, Parietal and Frontal Cortices.”&lt;/span> &lt;em>Nature Neuroscience&lt;/em> 16 (12): 1879.
&lt;/div>
&lt;div id="ref-vo2017" class="csl-entry">
Vo, Vy A, Thomas C Sprague, and John T Serences. 2017. &lt;span>“Spatial Tuning Shifts Increase the Discriminability and Fidelity of Population Codes in Visual Cortex.”&lt;/span> &lt;em>Journal of Neuroscience&lt;/em> 37 (12): 3386–3401.
&lt;/div>
&lt;div id="ref-yacoub2008" class="csl-entry">
Yacoub, Essa, Noam Harel, and Kâmil Uğurbil. 2008. &lt;span>“High-Field fMRI Unveils Orientation Columns in Humans.”&lt;/span> &lt;em>Proceedings of the National Academy of Sciences&lt;/em> 105 (30): 10607–12.
&lt;/div>
&lt;div id="ref-yang2004" class="csl-entry">
Yang, Tianming, and John HR Maunsell. 2004. &lt;span>“The Effect of Perceptual Learning on Neuronal Responses in Monkey Visual Area V4.”&lt;/span> &lt;em>Journal of Neuroscience&lt;/em> 24 (7): 1617–26.
&lt;/div>
&lt;/div>
&lt;/div></description></item><item><title>Modulations to tuning functions can bias evidence accumulation</title><link>https://psadil.github.io/psadil/post/neural-modulation-for-serial-dependence/</link><pubDate>Fri, 18 Oct 2019 00:00:00 +0000</pubDate><guid>https://psadil.github.io/psadil/post/neural-modulation-for-serial-dependence/</guid><description>
&lt;script src="https://psadil.github.io/psadil/post/neural-modulation-for-serial-dependence/index_files/header-attrs/header-attrs.js">&lt;/script>
&lt;p>Perceptual decisions can be deconstructed with evidence accumulation models. These models formalize expectations about how participants behave, when that behavior involves repeatedly sampling information towards until surpassing a necessary threshold of information. At a cognitive level, the different models instantiate the components differently, but at a neural level the models rely on common mechanisms. To accumulate evidence the models assume two distinct populations of neurons. One population responds to available information. This population can be thought of as a sensory population, such that each neuron in the population represents one of the available options. The second population listens to the first, transforming the sensory activity into evidence for each decision and accumulating the evidence through time. This second population can be called an integrating population. While the location of the sensory population depends on the information that needs to be represented, the location of the integrating population depends on the required behavior. If participants must make decisions about orientations, the sensory population might be striatal neurons tuned to different orientations. If participants make decisions with saccades, the integrating population might be in the frontal eye fields. Understanding how these two populations reveals different ways that decisions can be biased.&lt;/p>
&lt;div class="figure">&lt;span id="fig:readout">&lt;/span>
&lt;img src="https://psadil.github.io/psadil/post/neural-modulation-for-serial-dependence/index_files/figure-html/readout-1.png" alt="Sensory channels evenly represent orientations. The curves from the sensory population represent the average activity of each neuron to a given orientation. Although the curves show average activity, at any given moment the actual activity of each neuron may be higher or lower. The integrating curve reflects the average evidence that the integrating population will record. The flatness of the integrating curve reflects an unbiased representation. Only eight neurons from the sensory population are shown to avoid overcrowding." width="672" />
&lt;p class="caption">
Figure 1: Sensory channels evenly represent orientations. The curves from the sensory population represent the average activity of each neuron to a given orientation. Although the curves show average activity, at any given moment the actual activity of each neuron may be higher or lower. The integrating curve reflects the average evidence that the integrating population will record. The flatness of the integrating curve reflects an unbiased representation. Only eight neurons from the sensory population are shown to avoid overcrowding.
&lt;/p>
&lt;/div>
&lt;p>For the integrating population to accumulate evidence, it must transform the activity of the sensory population into a meaningful signal. Figure &lt;a href="#fig:readout">1&lt;/a> depicts that transformation when participants must report orientations. Each neuron in the sensory population responds most strongly to a specific orientation, but all of them are active whenever an orientation is present. The function describing how a sensory neuron respond to different orientations is called the neuron’s tuning function. The integrating population will respond according to some other function of that sensory activity. One simple integrating function associates each neuron with its preferred orientation; the integrating population then tallies evidence based on whichever neuron is most active. This function requires the sensory population to represent each orientation with at least one neuron. If there are enough sensory neurons, the integrating population will be able to accumulate evidence for each orientation without bias.&lt;/p>
&lt;div class="figure">&lt;span id="fig:modulations">&lt;/span>
&lt;img src="https://psadil.github.io/psadil/post/neural-modulation-for-serial-dependence/index_files/figure-html/modulations-1.png" alt="Modulations to activity of the sensory population will provide the integrating population a biased representation of the orientation. Unlike in Figure 1, the sensory population in both panels provides the integrating population with an uneven representation of orientation." width="672" />
&lt;p class="caption">
Figure 2: Modulations to activity of the sensory population will provide the integrating population a biased representation of the orientation. Unlike in Figure 1, the sensory population in both panels provides the integrating population with an uneven representation of orientation.
&lt;/p>
&lt;/div>
&lt;p>The tuning characteristics of sensory neurons are variable, and this variability causes biases to emerge in the evidence accumulation process (Figure &lt;a href="#fig:modulations">2&lt;/a>). One common alteration is an increased gain, whereby the tuning function is multiplied by some value. When the gains of tuning functions are altered heterogeneously, a neuron may have a higher activity even when the orientation it is responsible for is not present. The neurons with the highest gain will bias the evidence gathered by the integrating population. Alternatively, tuning functions might shift, along with the orientation each neuron signals. The shift causes the sensory population to over-represent of some orientations and leave others underrepresented. These alterations can provide advantages in certain circumstances. For example, a heterogeneously increased gain will be useful when some orientations are known to be more likely than others, and a shift will be useful when different orientations require differently precise responses. But to accumulate evidence without bias, the sensory population must restore more uniform tuning.&lt;/p></description></item><item><title>serial dependence reflects a preference for low variability</title><link>https://psadil.github.io/psadil/post/serial-dependence-reflects-a-preference-for-low-variability/</link><pubDate>Fri, 11 Oct 2019 00:00:00 +0000</pubDate><guid>https://psadil.github.io/psadil/post/serial-dependence-reflects-a-preference-for-low-variability/</guid><description>
&lt;script src="https://psadil.github.io/psadil/post/serial-dependence-reflects-a-preference-for-low-variability/index_files/header-attrs/header-attrs.js">&lt;/script>
&lt;p>One framework for understanding perception casts it as inference: just as a statistician uncovers noisy data to uncover patterns, an organism perceives when it converts sensations into guesses about its environment. The framework not concrete enough to be called a theory of perception, since it is not clear what data could falsify it&lt;a href="#fn1" class="footnote-ref" id="fnref1">&lt;sup>1&lt;/sup>&lt;/a>. But the framework can remind perceptual researchers about the many strategies available for modeling the world. Do perceiving organisms employ similar strategies?&lt;/p>
&lt;p>One property that distinguishes many statistical strategies is a tradeoff between bias and variability Consider this tradeoff with an example. A statistician must estimate the average height of college-level soccer players. The true average could be uncovered by measuring the height of every player at every college–the statistician would not need inference. But the statistician is constrained by limited resources. They can only measure the players from a single college, though they may measure the heights of any student at the college. The statistician must now decide between an unbiased but variable or biased but precise strategy. Measuring only the soccer players gives an unbiased estimate, but with so few players the team’s average may be far from the true average. The statistician cannot be confident that the single team resembles all teams. Alternatively, the statistician may supplement their estimate with the heights of players from another, related sport. Since ultimate frisbee players may have similar heights to soccer players, incorporating their heights into the estimate may counteract any anomalously sized soccer players. However, incorporating even a single player from another sport biases the estimate, in the sense that the average height of all soccer players will not equal the average height of all soccer players and the one ultimate player&lt;a href="#fn2" class="footnote-ref" id="fnref2">&lt;sup>2&lt;/sup>&lt;/a>. One strategy – measure only soccer players – would give the true answer if there were enough resources, but the other strategy – measure everyone that is similar to a soccer player – may approximate the truth well with limited resources.&lt;/p>
&lt;p>The bias-variability tradeoff gives a functional interpretation to the perceptual effect called serial dependence &lt;span class="citation">(&lt;a href="#ref-fischer2014" role="doc-biblioref">Fischer and Whitney 2014&lt;/a>; &lt;a href="#ref-cicchini2018" role="doc-biblioref">Cicchini, Mikellidou, and Burr 2018&lt;/a>)&lt;/span>. Serial dependence occurs when participants judge perceptual stimuli across many trials, and their judgments on one trial depend on their immediately preceding judgment. Like the constrained statistician, participants may not process each stimulus completely: participants only see stimuli for brief durations, their judgments are made after the stimuli are masked, and their attention fluctuates throughout the hundreds of trials. The bias is often attractive, meaning that participants’ judgments reflect a blending of the stimuli on the current and previous trials. Serial dependence may reflect a strategy – not necessarily intentional – that reduces variability across judgments by combining information. Although the strategy biases the judgments, it may help each individual estimate approach truth.&lt;/p>
&lt;div id="references" class="section level1 unnumbered">
&lt;h1>References&lt;/h1>
&lt;div id="refs" class="references csl-bib-body hanging-indent">
&lt;div id="ref-cicchini2018" class="csl-entry">
Cicchini, G. M., K. Mikellidou, and D. C Burr. 2018. &lt;span>“The Functional Role of Serial Dependence.”&lt;/span> &lt;em>Proceedings of the Royal Society B&lt;/em> 285 (1890): 20181722.
&lt;/div>
&lt;div id="ref-fischer2014" class="csl-entry">
Fischer, Jason, and David Whitney. 2014. &lt;span>“Serial Dependence in Visual Perception.”&lt;/span> &lt;em>Nature Neuroscience&lt;/em> 17 (5): 738.
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="footnotes">
&lt;hr />
&lt;ol>
&lt;li id="fn1">&lt;p>A perceptual system does need to be limited to the statistical tools that have already been developed, so even a demonstration that organisms don’t employ any known statistical tool would not rule out the framework.&lt;a href="#fnref1" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn2">&lt;p>Assume that the ultimate player is not as tall as the average soccer player&lt;a href="#fnref2" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>derivative of gaussian for serial dependence</title><link>https://psadil.github.io/psadil/post/derivative-of-gaussian-for-serial-dependence/</link><pubDate>Thu, 03 Oct 2019 00:00:00 +0000</pubDate><guid>https://psadil.github.io/psadil/post/derivative-of-gaussian-for-serial-dependence/</guid><description>
&lt;script src="https://psadil.github.io/psadil/post/derivative-of-gaussian-for-serial-dependence/index_files/header-attrs/header-attrs.js">&lt;/script>
&lt;p>Cognitive experiments can require participants to complete hundreds of trials, but completing so many trials invariably alters participants’ behavior. Their behavior late in the experiment can depend on their behavior early in the experiment. Although such dependence can be an experimental confound,
the dependence itself can provide clues about cognition. One simple kind of dependence occurs through learning; hundreds of trials provides participants ample practice. A more subtle dependence can emerge between sequential trials, an effect called serial dependence. Theoretical interpretations of serial dependence vary, and some of that variability may relate to how the dependence is measured. In this post, I review a statistical method commonly used to analyze serial dependence and discuss one way that method can fail.&lt;/p>
&lt;p>I will focus on the analysis of an orientation judgment task, in which participants simply see an oriented bar on each trial, remember the bar’s orientation for a short period, and then report the orientation. Participants’ responses on one trial can depend on the orientation they saw in the previous trial. The dependence follows a Gaussian’s derivative function. Figure &lt;a href="#fig:dog0">1&lt;/a>A shows a Gaussian function with its derivative, and Figure &lt;a href="#fig:dog0">1&lt;/a>B shows the derivative modeling a range of different serial dependence patterns. The derivative captures three key features of the data. First, different changes in orientation between trials result in serial dependencies of different magnitude. The responsiveness of dependence is captured by the width of the derivative. Second, serial dependence can have a different magnitude. The magnitude is captured by the amplitude of the derivative. Finally, responses on the current trial can either be attracted towards or repulsed away from the orientation of the previous trial. The direction of the effect is captured with the sign of the amplitude. The direction of the effect–and the experimental manipulations that change that direction–are often critical to different theoretical interpretations of serial dependence.&lt;/p>
&lt;div class="figure">&lt;span id="fig:dog0">&lt;/span>
&lt;img src="https://psadil.github.io/psadil/post/derivative-of-gaussian-for-serial-dependence/index_files/figure-html/dog0-1.png" alt="A) A Gaussian function and its derivative. B) The derivative captures how errors on the current trial can depend on how the relationship between the orientation seen in the current and previous trials. Positive values on the horizontal axis signify a clockwise difference and negative values a counterclockwise difference. Likewise, positive errors signify responses on the current trial which were clockwise to the true orientation, and negative errors are counterclockwise. When errors are in the same direction as the difference in orientations, the error is said to be attractive. Otherwise, the error is repulsive. Whether errors are attractive or repulsive is given by the sign of the derivative's amplitude." width="672" />
&lt;p class="caption">
Figure 1: A) A Gaussian function and its derivative. B) The derivative captures how errors on the current trial can depend on how the relationship between the orientation seen in the current and previous trials. Positive values on the horizontal axis signify a clockwise difference and negative values a counterclockwise difference. Likewise, positive errors signify responses on the current trial which were clockwise to the true orientation, and negative errors are counterclockwise. When errors are in the same direction as the difference in orientations, the error is said to be attractive. Otherwise, the error is repulsive. Whether errors are attractive or repulsive is given by the sign of the derivative’s amplitude.
&lt;/p>
&lt;/div>
&lt;p>Although the Gaussian’s derivative adequately models the serial dependence between trials with similar orientations (less than 45 degree differences), the derivative fits poorly the dependencies following large changes. When sequential trials have a large orientation difference, the sign of the dependence often changes; small orientation differences can elicit an attractive dependence even while large differences are repulsive. These sign flips are called the peripheral bumps, and they are not captured by the Gaussian’s derivative. If the bumps are large enough, they can interpretations about the sign to of dependencies following small changes can be inverted (Figure &lt;a href="#fig:bumps">2&lt;/a>). Unfortunately, noticing the peripheral bumps can be hard with sparse data. But even with sparse data, the width of the best-fitting derivative can help identify bumps. If the best-fitting derivative is abnormally wide (with peaks larger than approximately 35 degrees), then the derivative is tracking dependencies wider than it should. In that circumstance, it may be best to focus analyses on only the trials with smaller orientation differences.&lt;/p>
&lt;div class="figure">&lt;span id="fig:bumps">&lt;/span>
&lt;img src="https://psadil.github.io/psadil/post/derivative-of-gaussian-for-serial-dependence/index_files/figure-html/bumps-1.png" alt="Misfits of the Gaussian's derivative. The dots give hypothetical data. The data were generated with a function whose average is traced by the dashed line. The data were fit with a derivative of Guassian function, and the best-fitting derivative is shown with a solid line. The derivative does not match the data-generating function." width="672" />
&lt;p class="caption">
Figure 2: Misfits of the Gaussian’s derivative. The dots give hypothetical data. The data were generated with a function whose average is traced by the dashed line. The data were fit with a derivative of Guassian function, and the best-fitting derivative is shown with a solid line. The derivative does not match the data-generating function.
&lt;/p>
&lt;/div></description></item><item><title>an overview of population receptive field mapping</title><link>https://psadil.github.io/psadil/post/population-receptive-field-mapping/</link><pubDate>Fri, 27 Sep 2019 00:00:00 +0000</pubDate><guid>https://psadil.github.io/psadil/post/population-receptive-field-mapping/</guid><description>
&lt;script src="https://psadil.github.io/psadil/post/population-receptive-field-mapping/index.en_files/header-attrs/header-attrs.js">&lt;/script>
&lt;p>Perceiving the world requires representing the world in neural tissue. A neuron is &lt;em>tuned&lt;/em> to perceivable information when different values of that information cause the neuron to fire at a different rate. For example, most visual neurons are tuned to spatial location. The spatial tuning could be measured by placing a recording electrode in a neuron in a macaque’s visual cortex while the macaque fixated on the center of a computer monitor and a picture moved across that monitor. The electrode would report higher activity only when the picture was in certain parts of the macaque’s visual field. The function relating the position of the picture to the neuron’s activity is the tuning function. Such functions often resembles a bivariate Gaussian (Figure &lt;a href="#fig:prf">1&lt;/a>). To study these tuning functions is to study how these neurons represent the world.&lt;/p>
&lt;div class="figure">&lt;span id="fig:prf">&lt;/span>
&lt;img src="https://psadil.github.io/psadil/post/population-receptive-field-mapping/index.en_files/figure-html/prf-1.png" alt="The bivariate Gaussian represents a neuron's receptive field. The neuron will be most responsive to information that overlaps with the bright yellow regions. Since this the upper left portion of the box is slightly encompassed by the receptive field, the neuron' might fire slightly more rapidly as compared to its baseline rate." width="672" />
&lt;p class="caption">
Figure 1: The bivariate Gaussian represents a neuron’s receptive field. The neuron will be most responsive to information that overlaps with the bright yellow regions. Since this the upper left portion of the box is slightly encompassed by the receptive field, the neuron’ might fire slightly more rapidly as compared to its baseline rate.
&lt;/p>
&lt;/div>
&lt;p>Sensory neurons are tuned to many other features such as orientation, color, pitch, direction of motion. Most neurons tuned to one visual feature are also tuned to spatial location, so understanding a neuron’s spatial can facilitate understanding its other sensitivities&lt;a href="#fn1" class="footnote-ref" id="fnref1">&lt;sup>1&lt;/sup>&lt;/a>. The tuning functions of neurons even have a special name, their receptive field. However, it is often unfeasible to record from individual neurons in humans, and instead only non-invasive neuroimaging methods are available. But these non-invasive methods have low spatial resolution. Even the relatively well spatially resolved technique of functional magnetic resonance imaging reflects the aggregated activity of 10e5 - 10e6 neurons.&lt;/p>
&lt;p>Fortunately, the &lt;em>retinotopic&lt;/em> arrangement of visual neurons facilitates relating the spatial tuning of a voxel&lt;a href="#fn2" class="footnote-ref" id="fnref2">&lt;sup>2&lt;/sup>&lt;/a> to the receptive field of individual neurons. A retinotopic arrangement means that neighboring neurons are tuned to neighboring locations in the visual field; for example, neurons tuned to things in the fovea cluster together and neurons tuned to peripheral locations surround that cluster. This retinotopic arrangement implies that all neurons sampled by a voxel represent nearby regions in the visual environment. Referring to the neurons in a voxel as a population, the receptive field of a voxel is called a population receptive field. Studying population receptive fields alone cannot reveal how individual neurons contribute to a coherent perceptual experience, but studying them can reveal how the populations respond as a group.&lt;/p>
&lt;p>To chart out all of the mountainous population receptive fields in visual cortex is called population receptive field mapping &lt;span class="citation">(&lt;a href="#ref-dumoulin2008" role="doc-biblioref">Dumoulin and Wandell 2008&lt;/a>)&lt;/span>. The receptive fields can be mapped by recording the activity of each voxel while a human participant is shown some visually salient movie. A mathematical model – such as a bivariate Gaussian – of the receptive field is assumed, and the data from each voxel are used to fit the parameters of that model. The specific images that are used will depend on which part of visual cortex is the focus of the experiment. A counterphasing black and white checkerboard might be close to optimal for primary visual cortex, but the checkerboard would only weakly stimulate neurons in higher level visual regions. To stimulate most of visual cortex, other researchers rely on &lt;a href="https://kendrickkay.net/analyzePRF/">more varied displays&lt;/a>.&lt;/p>
&lt;div id="references" class="section level1 unnumbered">
&lt;h1>References&lt;/h1>
&lt;div id="refs" class="references csl-bib-body hanging-indent">
&lt;div id="ref-dumoulin2008" class="csl-entry">
Dumoulin, Serge O, and Brian A Wandell. 2008. &lt;span>“Population Receptive Field Estimates in Human Visual Cortex.”&lt;/span> &lt;em>Neuroimage&lt;/em> 39 (2): 647–60.
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="footnotes">
&lt;hr />
&lt;ol>
&lt;li id="fn1">&lt;p>e.g., if you want to understand how a neuron is tuned to color, it helps to know where to put the color&lt;a href="#fnref1" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn2">&lt;p> &lt;em>Voxels&lt;/em> are the elements that hold data in magnetic resonance imaging. A voxel in a 3D image is analogous to a pixel in a 2D image; a voxel is a pixel with a volume.&lt;a href="#fnref2" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>Serial Dependence</title><link>https://psadil.github.io/psadil/post/serial-dependence/</link><pubDate>Thu, 19 Sep 2019 00:00:00 +0000</pubDate><guid>https://psadil.github.io/psadil/post/serial-dependence/</guid><description>
&lt;script src="https://psadil.github.io/psadil/post/serial-dependence/index.en_files/header-attrs/header-attrs.js">&lt;/script>
&lt;p>Objects in the visual environment move suddenly and erratically, and visual perception must be sensitive to the changes that are important. But each saccade and head tilt change the image imprinted on the retina, and to perceive every tremor ignores the stability of the visual environment; a desk will still look like a desk in a few seconds. The visual system must therefore balance the ability to detect subtle changes in the environment against the efficiency afforded by accurate predictions.&lt;/p>
&lt;p>That the recent past influences current perception can be demonstrated easily. If you stare at Figure &lt;a href="#fig:tae">1&lt;/a>, you might observe that the Gabor has a bend immediately after changing orientations. The bend lasts for a moment, then straightens. But the bend is an illusion. While tracking Figure &lt;a href="#fig:tae">1&lt;/a>, the visual system allows for a momentary bias. Usefully, the bias is sensitive to experimental manipulation. Figure &lt;a href="#fig:tae2">2&lt;/a> shows the same Gabor with the same orientations, but the Gabor also moves. The movement largely eliminates the bending. The sensitivity of such biases to different experimental manipulations enables researchers to study how the visual system balances new information against the recent past.&lt;/p>
&lt;div class="figure">&lt;span id="fig:tae">&lt;/span>
&lt;img src="https://psadil.github.io/psadil/post/serial-dependence/index.en_files/figure-html/tae-1.gif" alt="A Gabor alternates between two orientations." />
&lt;p class="caption">
Figure 1: A Gabor alternates between two orientations.
&lt;/p>
&lt;/div>
&lt;div class="figure">&lt;span id="fig:tae2">&lt;/span>
&lt;img src="https://psadil.github.io/psadil/post/serial-dependence/index.en_files/figure-html/tae2-1.gif" alt="A Gabor alternates between two orientations, appearing in a different location with each orientation." />
&lt;p class="caption">
Figure 2: A Gabor alternates between two orientations, appearing in a different location with each orientation.
&lt;/p>
&lt;/div>
&lt;p>A closely effect is called &lt;em>serial dependence&lt;/em>. Serial dependence occurs when participants report the orientations of sequentially presented, tilted Gabors &lt;span class="citation">(&lt;a href="#ref-fischer2014" role="doc-biblioref">Fischer and Whitney 2014&lt;/a>)&lt;/span>. A visual mask to reduces the strong aftereffects present in Figures &lt;a href="#fig:tae">1&lt;/a> and &lt;a href="#fig:tae2">2&lt;/a> [Figure &lt;a href="#fig:gabor">3&lt;/a>]&lt;a href="#fn1" class="footnote-ref" id="fnref1">&lt;sup>1&lt;/sup>&lt;/a>. Even without the aftereffect, the perceptual-decision about one Gabor affects the perceptual-decision about the next; participants report orientations that consistently err toward the orientation of the most recently seen Gabor. Reports on the magnitude of the effect vary, but the error has an average maximum of less than a few degrees. However, serial dependence is affected by different manipulations than that the demonstration of Figures &lt;a href="#fig:tae">1&lt;/a> and &lt;a href="#fig:tae2">2&lt;/a>. For example, it appears insensitive to the location of the Gabors. This bias may therefore provide a unique way to study how current perception is not only biased by but toward the recent past.&lt;/p>
&lt;div class="figure">&lt;span id="fig:gabor">&lt;/span>
&lt;img src="https://psadil.github.io/psadil/post/serial-dependence/index.en_files/figure-html/gabor-1.gif" alt="Differently oriented Gabors presented with interspersed noise masks.." />
&lt;p class="caption">
Figure 3: Differently oriented Gabors presented with interspersed noise masks..
&lt;/p>
&lt;/div>
&lt;p>However, it remains unclear whether serial dependence is a bias of perceptual or post-perceptual processes. That is, does serial dependence alter participants’ perception of the Gabors, or does it alter how they report the orientation? The sequential timing of each trial – in which participants respond in a designated period after seeing the Gabor – does not imply that participants decide on an orientation only after they have finished perceiving the Gabor. For example, a participant can make decisions before the response period, and they can adopt a biased response strategy even before seeing the Gabor. Where and when to delineate between perception and decision, or whether they can be delineated, depends on assumptions about the relationship between perception and decisions. A tool like the &lt;a href="https://psadil.github.io/psadil/post/circular-diffusion-model-of-response-times/">circular diffusion model&lt;/a> can help make those assumptions explicit &lt;span class="citation">(&lt;a href="#ref-smith2016" role="doc-biblioref">Smith 2016&lt;/a>)&lt;/span>.&lt;/p>
&lt;div id="references" class="section level1 unnumbered">
&lt;h1>References&lt;/h1>
&lt;div id="refs" class="references csl-bib-body hanging-indent">
&lt;div id="ref-fischer2014" class="csl-entry">
Fischer, Jason, and David Whitney. 2014. &lt;span>“Serial Dependence in Visual Perception.”&lt;/span> &lt;em>Nature Neuroscience&lt;/em> 17 (5): 738.
&lt;/div>
&lt;div id="ref-smith2016" class="csl-entry">
Smith, Philip L. 2016. &lt;span>“Diffusion Theory of Decision Making in Continuous Report.”&lt;/span> &lt;em>Psychological Review&lt;/em> 123 (4): 425–51. &lt;a href="https://doi.org/10.1037/rev0000023">https://doi.org/10.1037/rev0000023&lt;/a>.
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="footnotes">
&lt;hr />
&lt;ol>
&lt;li id="fn1">&lt;p>The timing and spacing of this figure does not quite match a typical experiment. For example, participants take a few seconds to respond, so the amount of time between Gabors in this figure is too short.&lt;a href="#fnref1" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>Circular Diffusion Model of Response Times</title><link>https://psadil.github.io/psadil/post/circular-diffusion-model-of-response-times/</link><pubDate>Thu, 12 Sep 2019 00:00:00 +0000</pubDate><guid>https://psadil.github.io/psadil/post/circular-diffusion-model-of-response-times/</guid><description>
&lt;script src="https://psadil.github.io/psadil/post/circular-diffusion-model-of-response-times/index.en_files/header-attrs/header-attrs.js">&lt;/script>
&lt;p>Many cognitive experiments involve asking participants to answer questions that require circular responses (Figure &lt;a href="#fig:color">1&lt;/a>). What was the color of the shape you just saw? In which direction was the arrow pointing? How tilted was the bar? The answers required by these questions differ fundamentally from the more common, categorical responses required to questions. Was the color green or red? Was the arrow pointing left or right? Was the bar tilted more than 45 degrees from vertical, between 45-90, or more than 90 degrees? In the continuous case, the experimenter looses the ability to classify responses as either correct or incorrect, and an analysis must consider participants’ degree of inaccuracy, their relative error. Circularity adds the additional complication that a response can only be erroneous up to a point; if a person responds that a vertical bar is 3 degrees offset from vertical on trial one and 359 degrees on trial two, the analysis must acknowledge that the average is close to truth. Although many models exist that describe how a participant will respond when the choice is binary, models of these are much more limited.&lt;/p>
&lt;div class="figure">&lt;span id="fig:color">&lt;/span>
&lt;img src="https://psadil.github.io/psadil/post/circular-diffusion-model-of-response-times/index.en_files/figure-html/color-1.png" alt="Cognitive experiments often require participants to provide a circularly continuous response. A participant might be asked to study shapes of different colors, hold the colors of every shape in memory, and then report on the one shape's color." width="672" />
&lt;p class="caption">
Figure 1: Cognitive experiments often require participants to provide a circularly continuous response. A participant might be asked to study shapes of different colors, hold the colors of every shape in memory, and then report on the one shape’s color.
&lt;/p>
&lt;/div>
&lt;p>&lt;span class="citation">&lt;a href="#ref-smith2016" role="doc-biblioref">Smith&lt;/a> (&lt;a href="#ref-smith2016" role="doc-biblioref">2016&lt;/a>)&lt;/span> present a new model of how participants provide circularly continuous responses, called the circular diffusion model. It is a model of the decision-making process, analyzing both the numerical value participants provided and how long it took them to provide a response. The model extends the drift diffusion model of binary decisions &lt;span class="citation">(&lt;a href="#ref-ratcliff1978" role="doc-biblioref">Ratcliff 1978&lt;/a>)&lt;/span>. Like the drift diffusion model, the circular diffusion model casts perceptual decisions as a stochastic process of evidence accumulation to a threshold; evidence is accumulated over time, and when enough evidence has been reached the process terminates in a motor behavior. The model is not concerned with how evidence accumulates, just that it does. In a working memory experiment, evidence might accumulate through repeated probes of memory. In a perceptual-decision task, each saccade might provide a different amount of evidence. In both cases, evidence grows at an average rate, and when there is enough evidence for a decision that decision is made. The amount of time required to reach that threshold of evidence is the response time. The circular diffusion model, therefore, proposes that the responses of rapid decisions which require circularly continuous responses can be modeled as a particle drifting in two dimensions out towards a circular boundary.&lt;/p>
&lt;div class="figure">&lt;span id="fig:cdm">&lt;/span>
&lt;img src="https://psadil.github.io/psadil/post/circular-diffusion-model-of-response-times/index.en_files/figure-html/cdm-1.gif" alt="A diffusing particle models perceptual decisions. In this example, wherever the particle first crosses the circle corresponds to the response, and the amount of time required to reach the edge is their response time." />
&lt;p class="caption">
Figure 2: A diffusing particle models perceptual decisions. In this example, wherever the particle first crosses the circle corresponds to the response, and the amount of time required to reach the edge is their response time.
&lt;/p>
&lt;/div>
&lt;p>Using the circular diffusion model affords researchers the same advantages conferred by using the standard drift diffusion model: the decision-making process can be decomposed into parameters of the model, and those parameters have psychologically meaningful values. For example, a participant might respond quickly, but that could either occur because they accumulate evidence rapidly or because they set a low threshold for evidence. There are three key parameters in the model: 1) the average direction the particle drifts (towards what decision are participants mostly accumulating evidence?), 2) the average rate at which the particle drifts (how quickly do participants accumulate evidence?), and 3) the radius of the circular boundary (how conservative are participants?). Estimating these parameters for participants across different conditions of an experiment enables the researcher to “measure” each of these psychological constructs given participants’ behavior.&lt;/p>
&lt;div id="references" class="section level1 unnumbered">
&lt;h1>References&lt;/h1>
&lt;div id="refs" class="references csl-bib-body hanging-indent">
&lt;div id="ref-ratcliff1978" class="csl-entry">
Ratcliff, Roger. 1978. &lt;span>“A Theory of Memory Retrieval.”&lt;/span> &lt;em>Psychological Review&lt;/em> 85 (2): 59. &lt;a href="https://doi.org/10.1037/0033-295X.85.2.59">https://doi.org/10.1037/0033-295X.85.2.59&lt;/a>.
&lt;/div>
&lt;div id="ref-smith2016" class="csl-entry">
Smith, Philip L. 2016. &lt;span>“Diffusion Theory of Decision Making in Continuous Report.”&lt;/span> &lt;em>Psychological Review&lt;/em> 123 (4): 425–51. &lt;a href="https://doi.org/10.1037/rev0000023">https://doi.org/10.1037/rev0000023&lt;/a>.
&lt;/div>
&lt;/div>
&lt;/div></description></item><item><title>eyetracking with eyelink in psychtoolbox, now with oop</title><link>https://psadil.github.io/psadil/post/eyetracking-in-psychtoolbox-oop/</link><pubDate>Sat, 25 May 2019 00:00:00 +0000</pubDate><guid>https://psadil.github.io/psadil/post/eyetracking-in-psychtoolbox-oop/</guid><description>
&lt;script src="https://psadil.github.io/psadil/post/eyetracking-in-psychtoolbox-oop/index.en_files/header-attrs/header-attrs.js">&lt;/script>
&lt;p>I’ve started trying out &lt;a href="https://www.mathworks.com/discovery/object-oriented-programming.html">MATLAB’s OOP&lt;/a> after mounting suspicion that the way I’d been coding experiments basically involved making something that looked and behaved like an object–but did so in a convoluted and inefficient way. See this &lt;a href="https://psadil.github.io/psadil/post/eyetracking-init/">post on eyetracking with PTB&lt;/a> as proof.&lt;/p>
&lt;p>This post is brief, and is about as well thought out as a github gist/gitlab snippet.&lt;/p>
&lt;p>The two classes I’ll work with here is a Window class and a Tracker class. The window class has 3 methods. The first &lt;a href="https://www.mathworks.com/help/matlab/matlab_oop/class-constructor-methods.html">constructor&lt;/a> method exists just to create the object. The constructed object will have a few default properties of the class. The second method is open, which (can you guess?) calls the PTB functions to open an onscreen window. The open method is fancier than it needs to be for this post (note the PsychImaging configuration, and the optional debugLevel flag). The final window method is the &lt;a href="https://www.mathworks.com/help/matlab/matlab_oop/handle-class-destructors.html">desctructor&lt;/a>. The destructor method is one of the advantages of leaning on MATLAB’s OOP syntax. That method will get called whenever the Window object’s lifecycle has ended (which might happen from explicit deletion of the object, closing MATLAB, the object is no longer referenced in the call stack, etc).&lt;/p>
&lt;p>The second class is the Tracker class, which interfaces with Eyelink. The Window class is only present here because Eyelink needs an open window to run calibration. There are five Tracker methods, but they are either analogous to the Window objects methods (constructor, destructor) or were largely presented in the previous post.&lt;/p>
&lt;div id="window-object" class="section level2">
&lt;h2>Window Object&lt;/h2>
&lt;pre class="matlab">&lt;code>
classdef Window &amp;lt; handle
% Window handles opening and closing of screen
properties (Constant)
screenNumber = 0
% background color of screen
background = GrayIndex(Window.screenNumber)
end
properties
pointer
winRect
end
methods
function obj = Window()
end
function open(obj, skipsynctests, debuglevel)
PsychImaging(&amp;#39;PrepareConfiguration&amp;#39;);
PsychImaging(&amp;#39;AddTask&amp;#39;, &amp;#39;General&amp;#39;, &amp;#39;FloatingPoint16Bit&amp;#39;);
Screen(&amp;#39;Preference&amp;#39;, &amp;#39;SkipSyncTests&amp;#39;, skipsynctests);
switch debuglevel
% no debug. run as usual, without listening to keyboard input
% and also hiding the cursor
case 0
ListenChar(-1);
HideCursor;
[obj.pointer, obj.winRect] = ...
PsychImaging(&amp;#39;OpenWindow&amp;#39;, obj.screenNumber, obj.background);
% light debug: still open fullscreen window, but keep keyboard input
case 1
[obj.pointer, obj.winRect] = ...
PsychImaging(&amp;#39;OpenWindow&amp;#39;, obj.screenNumber, obj.background);
% full debug: only open transparent window
case 10
PsychDebugWindowConfiguration(0, .5)
[obj.pointer, obj.winRect] = ...
PsychImaging(&amp;#39;OpenWindow&amp;#39;, obj.screenNumber, obj.background);
end
% Turn on blendfunction for antialiasing of drawing dots
Screen(&amp;#39;BlendFunction&amp;#39;, obj.pointer, &amp;#39;GL_SRC_ALPHA&amp;#39;, &amp;#39;GL_ONE_MINUS_SRC_ALPHA&amp;#39;);
topPriorityLevel = MaxPriority(obj.pointer);
Priority(topPriorityLevel);
end
% will auto-close open windows and return keyboard control when
% this object is deleted
function delete(obj) %#ok&amp;lt;INUSD&amp;gt;
ListenChar(0);
Priority(0);
sca;
end
end
end
&lt;/code>&lt;/pre>
&lt;/div>
&lt;div id="tracker-object" class="section level2">
&lt;h2>Tracker Object&lt;/h2>
&lt;p>The tracker object will mostly do what it did in the &lt;a href="https://psadil.github.io/psadil/post/eyetracking-init/">previous post&lt;/a>. Same functionality, but the syntax is much cleaner than the heavy use of switch/case conditionals.&lt;/p>
&lt;pre class="matlab">&lt;code>classdef Tracker &amp;lt; handle
properties
% flag to be called in scripts which enable turning on or off the tracker
% in an experiment (e.g., when debug mode is on)
using_tracker logical = false
% name of the write. must follow eyelink conventions. alphanumeric only, no
% more than 8 characters
filename char = &amp;#39;&amp;#39;
% eyelink object structure. stores many relevant parameters
el
end
methods
function obj = Tracker(using_tracker, filename, window)
obj.using_tracker = using_tracker;
obj.filename = filename;
% run calibration for tracker (see method below)
calibrate(obj, window);
end
function calibrate(obj, window)
if obj.using_tracker
% Provide Eyelink with details about the graphics environment
% and perform some initializations. The information is returned
% in a structure that also contains useful defaults
% and control codes (e.g. tracker state bit and Eyelink key values).
obj.el = EyelinkInitDefaults(window.pointer);
if ~EyelinkInit(0, 1)
error(&amp;#39;\n Eyelink Init aborted \n&amp;#39;);
end
%Reduce FOV for calibration and validation. Helpful when the
% the stimulus is only in the center of the screen, or at places
% like the fMRI scanner at UMass where the eyes have a lot in front
% of them
Eyelink(&amp;#39;Command&amp;#39;,&amp;#39;calibration_area_proportion = 0.5 0.5&amp;#39;);
Eyelink(&amp;#39;Command&amp;#39;,&amp;#39;validation_area_proportion = 0.5 0.5&amp;#39;);
% open file to record data to
status = Eyelink(&amp;#39;Openfile&amp;#39;, obj.filename);
if status ~= 0
error(&amp;#39;\n Eyelink Init aborted \n&amp;#39;);
end
% Setting the proper recording resolution, proper calibration type,
% as well as the data file content;
Eyelink(&amp;#39;Command&amp;#39;,&amp;#39;screen_pixel_coords = %ld %ld %ld %ld&amp;#39;, 0, 0, window.winRect(3)-1, window.winRect(4)-1);
Eyelink(&amp;#39;message&amp;#39;, &amp;#39;DISPLAY_COORDS %ld %ld %ld %ld&amp;#39;, 0, 0, window.winRect(3)-1, window.winRect(4)-1);
% set calibration type to 5 point.
Eyelink(&amp;#39;Command&amp;#39;, &amp;#39;calibration_type = HV5&amp;#39;);
% set EDF file contents using the file_sample_data and
% file-event_filter commands
% set link data thtough link_sample_data and link_event_filter
Eyelink(&amp;#39;Command&amp;#39;, &amp;#39;file_event_filter = RIGHT,FIXATION,SACCADE,BLINK,MESSAGE,BUTTON,INPUT&amp;#39;);
Eyelink(&amp;#39;Command&amp;#39;, &amp;#39;link_event_filter = RIGHT,FIXATION,SACCADE,BLINK,MESSAGE,BUTTON,INPUT&amp;#39;);
% check the software version
% add &amp;quot;HTARGET&amp;quot; to record possible target data for EyeLink Remote
Eyelink(&amp;#39;command&amp;#39;, &amp;#39;file_sample_data = RIGHT,GAZE,HREF,GAZERES,AREA,HTARGET,STATUS,INPUT&amp;#39;);
Eyelink(&amp;#39;command&amp;#39;, &amp;#39;link_sample_data = RIGHT,GAZE,HREF,GAZERES,AREA,HTARGET,STATUS,INPUT&amp;#39;);
% make sure we&amp;#39;re still connected.
if Eyelink(&amp;#39;IsConnected&amp;#39;)~=1
error(&amp;#39;\n Eyelink Init aborted \n&amp;#39;);
end
% set sample rate in camera setup screen
Eyelink(&amp;#39;Command&amp;#39;, &amp;#39;sample_rate = %d&amp;#39;, 1000);
% opens up main calibration scheme
EyelinkDoTrackerSetup(obj.el);
end
end
function status = eyelink(obj, varargin)
% calls main Eyelink routines only when
% this tracker object property using_tracker==true.
status = [];
if obj.using_tracker
if nargin==2
% construct calls to eyelink that don&amp;#39;t output any
% status
if strcmp(varargin{1}, &amp;#39;StopRecording&amp;#39;) || ...
strcmp(varargin{1}, &amp;#39;Shutdown&amp;#39;) ||...
strcmp(varargin{1}, &amp;#39;SetOfflineMode&amp;#39;)
% magic happens here, where the variable argument input
% is expanded an repassed through to Eyelink()
Eyelink(varargin{:});
else
status = Eyelink(varargin{:});
end
% all calls to Eyelink that have more than two inputs (e.g., the
% name of a function with some parameters to that function) return
% some status
else
status = Eyelink(varargin{:});
end
end
end
% starts up the eyelink machine. call this once the start of each
% experiment. could modify function to also draw something special
% to the screen (e.g., a background image). this might be the kind
% of function to modify if you wanted to draw trial-by-trial material
% to the eyetracking computer
function startup(obj)
% Must be offline to draw to EyeLink screen
obj.eyelink(&amp;#39;SetOfflineMode&amp;#39;);
% clear tracker display and draw background img to host pc
obj.eyelink(&amp;#39;Command&amp;#39;, &amp;#39;clear_screen 0&amp;#39;);
% draw simple fixation cross as later reference
obj.eyelink(&amp;#39;command&amp;#39;, &amp;#39;draw_cross %d %d&amp;#39;, 1920/2, 1080/2);
% give image transfer time to finish
WaitSecs(0.1);
end
% destructor function will get called whenever tracker object is deleted (e.g.,
% this function is automatically called when MATLAB closes, meaning you can&amp;#39;t
% forget to close the file connection with the tracker computer).
function delete(obj)
% waitsecs occur because the filetransfer often takes a moment, and moving
% on too quickly will result in an error
% End of Experiment; close the file first
% close graphics window, close data file and shut down tracker
obj.eyelink(&amp;#39;StopRecording&amp;#39;);
WaitSecs(0.1); % Slack to let stop definitely happen
obj.eyelink(&amp;#39;SetOfflineMode&amp;#39;);
obj.eyelink(&amp;#39;CloseFile&amp;#39;);
WaitSecs(0.1);
obj.eyelink(&amp;#39;ReceiveFile&amp;#39;, obj.filename, fullfile(pwd,&amp;#39;events&amp;#39;), 1);
WaitSecs(0.2);
obj.eyelink(&amp;#39;Shutdown&amp;#39;);
end
end
end
&lt;/code>&lt;/pre>
&lt;/div>
&lt;div id="run-the-calibration-and-use-the-tracker" class="section level2">
&lt;h2>Run the calibration (and use the tracker)&lt;/h2>
&lt;p>Putting this together, the following script starts calibration, and outlines how this tracker could be used in an experiment.&lt;/p>
&lt;pre class="matlab">&lt;code>
%% input
% ------------------
skipsynctests = 2;
debuglevel = 0;
using_tracker = true;
%% setup
% ------------------
% boilerplate setup
PsychDefaultSetup(2);
% initialize window
window = Window();
% open that window
open(window, skipsynctests, debuglevel)
% Initialize tracker object
tracker = Tracker(using_tracker, &amp;#39;OOPDEMO.edf&amp;#39;, window);
% run calibration
tracker.startup();
% Let Eyelink know that the experiment starts now
tracker.eyelink(&amp;#39;message&amp;#39;, &amp;#39;SYNCTIME&amp;#39;);
%% Experiment/trial code
% ------------------
% note that we should not need to wait to start recording,
% given that the stimulus will always be drawn a bit later
% (determined by how often phase changes occur)
tracker.eyelink(&amp;#39;StartRecording&amp;#39;);
% trial/experiment happens here ...
tracker.eyelink(&amp;#39;StopRecording&amp;#39;);
% Wait moment to ensure that tracker is definitely finished with the last few samples
WaitSecs(0.001);
%% Cleanup
% ------------------
% closes connection to Eyelink system, saves file
delete(tracker);
% closes window, restores keyboard input
delete(window);
&lt;/code>&lt;/pre>
&lt;p>What’s nice about this syntax (as before) is that only very minimal changes are required you don’t want to call the Eyelink functions (e.g., if you’re testing on a computer that doesn’t have the Eyelink system connected, or you’re debugging other parts of the experiment). By changing just the input, the Eyelink functions won’t be called.&lt;/p>
&lt;pre class="matlab">&lt;code>
using_tracker = false;
% all the rest as above
% ...&lt;/code>&lt;/pre>
&lt;/div>
&lt;div id="summary" class="section level1">
&lt;h1>Summary&lt;/h1>
&lt;p>There’s not much to summarize because I haven’t explained much! Again, this post is largely just an attempt to revise what I now think is a poor implementation, presented in an earlier post.&lt;/p>
&lt;/div></description></item><item><title>A hierarchical Bayesian state trace analysis for assessing monotonicity while factoring out subject, item, and trial level dependencies</title><link>https://psadil.github.io/psadil/publication/sadil-2019-hierarchical/</link><pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate><guid>https://psadil.github.io/psadil/publication/sadil-2019-hierarchical/</guid><description/></item><item><title>A roadmap for understanding memory: Decomposing cognitive processes into operations and representations</title><link>https://psadil.github.io/psadil/publication/cowell-2019-roadmap/</link><pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate><guid>https://psadil.github.io/psadil/publication/cowell-2019-roadmap/</guid><description/></item><item><title>Connecting the dots without top-down knowledge: Evidence for rapidly-learned low-level associations that are independent of object identity.</title><link>https://psadil.github.io/psadil/publication/sadil-2019-connecting/</link><pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate><guid>https://psadil.github.io/psadil/publication/sadil-2019-connecting/</guid><description/></item><item><title>Basic Importance Sampling for Variance Reduction</title><link>https://psadil.github.io/psadil/post/importance-sampling/</link><pubDate>Sat, 10 Nov 2018 00:00:00 +0000</pubDate><guid>https://psadil.github.io/psadil/post/importance-sampling/</guid><description>
&lt;script src="https://psadil.github.io/psadil/post/importance-sampling/index.en_files/header-attrs/header-attrs.js">&lt;/script>
&lt;p>This post attempts to provide a brief intro to importance sampling, presenting a relatively simple situation in which importance sampling enables a speedup in Monte Carlo by reducing the variance of the estimate.&lt;/p>
&lt;p>A lot of the material will be an effort to remember material that was taught to be in a Stochastic Processes and Applications class. This post should be though of as my notes on the &lt;a href="http://people.math.umass.edu/~lr7q/m697u-spring2018/m697uhome.html">course notes&lt;/a>, with supplementation by &lt;a href="https://www.statlect.com/asymptotic-theory/importance-sampling" class="uri">https://www.statlect.com/asymptotic-theory/importance-sampling&lt;/a>. That is, it can safely be assumed that any content comes from those notes, excluding errors (which will be due to my translation)&lt;a href="#fn1" class="footnote-ref" id="fnref1">&lt;sup>1&lt;/sup>&lt;/a>.&lt;/p>
&lt;div id="what-is-importance-sampling" class="section level1">
&lt;h1>What is importance sampling?&lt;/h1>
&lt;p>Importance sampling comes up in the context of Monte Carlo, (MC) applications as a technique for reducing the variability in the MC estimator. As a reminder, MC can be used to estimate the expected value, &lt;span class="math inline">\(\mu\)&lt;/span>, of a random variable, &lt;span class="math inline">\(X\)&lt;/span>.&lt;/p>
&lt;p>&lt;span class="math display">\[
\mu = \mathbb{E}[h(x)] = \int h(x)p_X(x)\,dx
\]&lt;/span>&lt;/p>
&lt;p>The idea of Monte Carlo is that this expectation can be estimated by drawing &lt;span class="math inline">\(S\)&lt;/span> samples from the distribution &lt;span class="math inline">\(p_X\)&lt;/span>, where &lt;span class="math inline">\(X \sim p_X\)&lt;/span>&lt;/p>
&lt;p>&lt;span class="math display">\[
\hat{\mu} =\frac{1}{S}\sum_{s=1}^S h(x_s)
\]&lt;/span>&lt;/p>
&lt;p>where the subscript on &lt;span class="math inline">\(x\)&lt;/span> implies the &lt;span class="math inline">\(s^th\)&lt;/span> draw of &lt;span class="math inline">\(X\)&lt;/span>, and the hat over &lt;span class="math inline">\(\mu\)&lt;/span> indicates that the result is an estimate. Note that, since we’re starting from that equation, it is assumed that there is some way to produce random samples from the distribution &lt;span class="math inline">\(p_X\)&lt;/span>, and that the function, &lt;span class="math inline">\(h\)&lt;/span> is calculable for any &lt;span class="math inline">\(X\)&lt;/span>. Also, &lt;span class="math inline">\(h\)&lt;/span> might be something as simple as &lt;span class="math inline">\(h(x) = x\)&lt;/span> if the expectation should correspond to the mean of &lt;span class="math inline">\(x\)&lt;/span>].&lt;/p>
&lt;p>This is a powerful idea, though a general downside is that some &lt;span class="math inline">\(\mu\)&lt;/span> require many samples to obtain a reasonable estimate. The MC Standard Error (MCSE) provides an estimate of the approximation error, which is &lt;span class="math inline">\(\frac{1}{n} Var(h(X))\)&lt;/span>. This provides a metric by which to gauge different algorithms. In particular, if all else is equal, the estimator with lower variance will can obtain a given level of precision with a lower &lt;span class="math inline">\(S\)&lt;/span>.&lt;/p>
&lt;p>The basic idea with Importance Sampling (IS) is to draw samples from some alternative distribution, &lt;span class="math inline">\(p_Y\)&lt;/span>, which has the same support as &lt;span class="math inline">\(p_X\)&lt;/span>, then reweight those samples in accordance with the difference between &lt;span class="math inline">\(p_X\)&lt;/span> and &lt;span class="math inline">\(p_Y\)&lt;/span>.&lt;/p>
&lt;p>&lt;span class="math display">\[
\begin{aligned}
\mathbb{E}[h(x)] &amp;amp; = \int h(x)p_X(x) \,dx &amp;amp; \textrm{definition of expectation} \\
&amp;amp; = \int h(x)\frac{p_X(x)}{p_Y(x)}p_Y(x) \,dx &amp;amp; \textrm{multiplication by 1, assuming same support} \\
&amp;amp; \int h(y)\frac{p_X(y)}{p_Y(y)}p_Y(y) \,dy &amp;amp; \textrm{assuming same support} \\
&amp;amp; = \mathbb{E} \left[h(y)\frac{p_X(y)}{p_Y(y)} \right] &amp;amp; \textrm{our new importance sampling estimator}
\end{aligned}
\]&lt;/span>&lt;/p>
&lt;p>Recognize that there will often not be a single unique &lt;span class="math inline">\(p_Y\)&lt;/span>. The goal is to find a &lt;span class="math inline">\(p_Y\)&lt;/span> that results in lower MCSE. The MCSE for the importance sampling estimator is &lt;span class="math inline">\(\frac{1}{n}Var\left[h(y)\frac{p_X(y)}{p_Y(y)} \right]\)&lt;/span>. That will be used to gain an intuition for how to choose a useful &lt;span class="math inline">\(p_Y\)&lt;/span>.&lt;/p>
&lt;/div>
&lt;div id="why-does-importance-sampling-work" class="section level1">
&lt;h1>Why does importance sampling work?&lt;/h1>
&lt;p>One way to think about importance sampling is that, if we could sample from &lt;span class="math inline">\(h(y)\frac{p_X(y)}{p_Y(y)}\)&lt;/span> such that the result were constant, the variance would be 0, and so we’d only need a single sample. To see why, note that, for a constant &lt;span class="math inline">\(c\)&lt;/span>&lt;/p>
&lt;p>&lt;span class="math display">\[
\begin{aligned}
h(y)\frac{p_X(y)}{p_Y(y)} &amp;amp; = c \\
\implies p_Y(y)c &amp;amp; = h(y)p_X(y) \\
\implies p_Y(y) &amp;amp; \propto h(y)p_X(y) \\
\end{aligned}
\]&lt;/span>&lt;/p>
&lt;p>That is, &lt;span class="math inline">\(h(y)\frac{p_X(y)}{p_Y(y)}\)&lt;/span> will be constant whenever &lt;span class="math inline">\(p_Y(y)\)&lt;/span> is proportional to &lt;span class="math inline">\(h(y)p_X(x)\)&lt;/span>.&lt;/p>
&lt;p>&lt;span class="math display">\[
\begin{aligned}
p_Y(y) &amp;amp; = \frac{h(y)p_X(y)}{\int h(y)p_X(y)\,dy} \\
\implies p_Y(y) &amp;amp; = \frac{h(y)p_X(y)}{\mathbb{E}[h(X)]} \\
&amp;amp; = \frac{h(y)p_X(y)}{\mu} &amp;amp; \textrm {definition of }\mu
\end{aligned}
\]&lt;/span>&lt;/p>
&lt;p>Plugging this distribution into the IS estimator&lt;/p>
&lt;p>&lt;span class="math display">\[
\begin{aligned}
\frac{1}{S} \sum_{s=1}^S \frac{h(Y_s)p_X(Y_s)}{p_Y(Y_s)} &amp;amp; = \frac{1}{S} \sum_{s=1}^S \frac{h(Y_s)p_X(Y_s)}{\frac{h(Y_s)p_X(Y_s)}{\mathbb{E}[h(X_s)]}} \\
&amp;amp; = \frac{1}{S} S\mu \\
&amp;amp; = \mu
\end{aligned}
\]&lt;/span>&lt;/p>
&lt;p>So, regardless of &lt;span class="math inline">\(S\)&lt;/span>, the resulting estimator is always &lt;span class="math inline">\(\mu\)&lt;/span>.&lt;/p>
&lt;p>That’s almost useful, but this means that to get an optimal &lt;span class="math inline">\(p_Y\)&lt;/span> we need to know &lt;span class="math inline">\(\mathbb{E}[h(X)]\)&lt;/span>, which is by definition the &lt;span class="math inline">\(\mu\)&lt;/span> that we’re trying to estimate. Still, knowing what is optimal provides the intuition for why importance sampling works.&lt;/p>
&lt;p>There are two ideas going on here. First, the optimal &lt;span class="math inline">\(p_Y\)&lt;/span> is one which places higher density on regions where &lt;span class="math inline">\(h(X)\)&lt;/span> is high, as compared to &lt;span class="math inline">\(p_X\)&lt;/span>. Those “important” values are the ones that will determine the result of &lt;span class="math inline">\(h(x)\)&lt;/span>, so those are the ones that need to be altered the most (going from &lt;span class="math inline">\(p_X\)&lt;/span> to &lt;span class="math inline">\(p_Y\)&lt;/span>). Then, the second idea is that when averaging, we must account for the oversampling of these points – downweighting them by however much they were oversampled, which is the ratio &lt;span class="math inline">\(\frac{p_X(y)}{p_Y(y)}\)&lt;/span>.&lt;/p>
&lt;/div>
&lt;div id="using-is-to-reduce-variance" class="section level1">
&lt;h1>Using IS to reduce variance&lt;/h1>
&lt;p>Here’s an example of this working out. The value we’re trying to estimate will be, for &lt;span class="math inline">\(X \sim N(0,1)\)&lt;/span>&lt;/p>
&lt;p>&lt;span class="math display">\[
\mu = \int \phi(x-4)p_X(x)\,dx
\]&lt;/span>&lt;/p>
&lt;p>where &lt;span class="math inline">\(\phi\)&lt;/span> is the standard normal density function. This &lt;span class="math inline">\(h\)&lt;/span> is such that only values near 4 provide much contribution to the average.&lt;/p>
&lt;pre class="r">&lt;code>set.seed(1234)
hx &amp;lt;- function(x) {
return(dnorm(x - 4))
}
x &amp;lt;- seq(-5, 10, length.out = 1000)
plot(x, hx(x), type = &amp;quot;l&amp;quot;)
points(x, dnorm(x), col = &amp;quot;blue&amp;quot;, type = &amp;quot;l&amp;quot;)&lt;/code>&lt;/pre>
&lt;div class="figure">&lt;span id="fig:mismatch">&lt;/span>
&lt;img src="https://psadil.github.io/psadil/post/importance-sampling/index.en_files/figure-html/mismatch-1.png" alt="h(x) (black) has mostly 0 around where p_X (blue) has high density. The mismatch of these distributions will contribute to unnecessarily high variance of the estimator." width="672" />
&lt;p class="caption">
Figure 1: h(x) (black) has mostly 0 around where p_X (blue) has high density. The mismatch of these distributions will contribute to unnecessarily high variance of the estimator.
&lt;/p>
&lt;/div>
&lt;p>However, &lt;span class="math inline">\(X\)&lt;/span> will only rarely contain values that are near 4. Hence, the variance of the estimation will be relatively high. We can get an explicit formula for the variance.&lt;/p>
&lt;p>&lt;span class="math display">\[
Var(h(x)) = \mathbb{E}[h(x)^2] - \mathbb{E}[h(x)]^2
\]&lt;/span>&lt;/p>
&lt;p>A formula that involves calculating the expected value of this function&lt;/p>
&lt;p>&lt;span class="math display">\[
\begin{aligned}
\mathbb{E}[h(x)] &amp;amp; = \int_{-\infty}^{\infty} \left(\frac{\exp(- \frac{x^2}{2})}{\sqrt{2\pi}} \right)\left(\frac{\exp(- \frac{x^2}{2})}{\sqrt{2\pi}} \right) dx \\
&amp;amp; = \int_{-\infty}^{\infty} \frac{\exp (- x^2 + 4x - 8 )}{2\pi} dx \\
&amp;amp; = \frac{1}{2\pi} \int_{-\infty}^{\infty} \exp (- x^2 + 4x - 8 ) dx\\
&amp;amp; = \frac{1}{2\pi} \sqrt{\pi}\exp \left(\frac{4^2}{4}-8 \right) &amp;amp; \textrm{en.wikipedia.org/wiki/Gaussian_function} \\
&amp;amp; = \frac{1}{2 \exp(4) \sqrt{\pi}}
\end{aligned}
\]&lt;/span>&lt;/p>
&lt;p>Which we’ll save for now to use later&lt;/p>
&lt;pre class="r">&lt;code>mu &amp;lt;- 1/(2 * exp(4) * sqrt(pi))
mu&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] 0.005166746&lt;/code>&lt;/pre>
&lt;p>Returning to the variance calculation&lt;/p>
&lt;p>&lt;span class="math display">\[
\begin{aligned}
Var(h(x)) &amp;amp; = \left[ \int_{-\infty}^{\infty} \left(\frac{\exp(- \frac{(x-2)^2}{2})}{\sqrt{2\pi}} \right)^2 \left(\frac{\exp(- \frac{x^2}{2})}{\sqrt{2\pi}} \right) \,dx \right] - \mu^2 \\
&amp;amp; = \frac{1}{2\sqrt{2}\pi^{3/2}} \int_{-\infty}^{\infty} \exp\left(-\frac{3}{2}x^2+8x-16 \right) \,dx - \mu^2 \\
&amp;amp; = \frac{1}{2\sqrt{2}\pi^{3/2}} \sqrt{\frac{\pi}{3/2}}\exp \left(\frac{8^2}{6} -16 \right) \\
&amp;amp; = \frac{1}{2 \pi \sqrt{3} \exp(16/3)} - \mu^2
\end{aligned}
\]&lt;/span>&lt;/p>
&lt;pre class="r">&lt;code>1/(2 * pi * sqrt(3) * exp(16/3)) - mu^2&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] 0.0004169361&lt;/code>&lt;/pre>
&lt;div id="standard-mc-estimate-is-accurate-but-with-relatively-high-variance" class="section level3">
&lt;h3>Standard MC estimate is accurate, but with relatively high variance&lt;/h3>
&lt;p>Using an MC estimate,&lt;/p>
&lt;pre class="r">&lt;code>x &amp;lt;- rnorm(1e+06)
y &amp;lt;- hx(x)
var(y)&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] 0.0004270083&lt;/code>&lt;/pre>
&lt;p>Note also that the estimate (our target), is also accurate&lt;/p>
&lt;pre class="r">&lt;code>mean(y) - mu&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] 4.582938e-05&lt;/code>&lt;/pre>
&lt;/div>
&lt;div id="using-a-distribution-that-simply-matches-hx-is-also-not-so-great" class="section level3">
&lt;h3>Using a distribution that simply matches h(x) is also not-so-great&lt;/h3>
&lt;p>Now, let’s instead construct and IS estimator. One intuitive (but by no means optimal) attempt would be to use &lt;span class="math inline">\(Y \sim N(4,1)\)&lt;/span>, a distribution that matches with &lt;span class="math inline">\(h(x)\)&lt;/span> perfectly. Indeed, that will provide an accurate answer&lt;/p>
&lt;pre class="r">&lt;code>y &amp;lt;- rnorm(1e+06, mean = 4)
h &amp;lt;- hx(y)
IS &amp;lt;- h * dnorm(y, 0, 1)/dnorm(y, 4, 1)
mean(IS)&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] 0.005194268&lt;/code>&lt;/pre>
&lt;p>But, it turns out that the variance is about the same as before.&lt;/p>
&lt;pre class="r">&lt;code>var(IS)&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] 0.0004204215&lt;/code>&lt;/pre>
&lt;/div>
&lt;div id="the-proposal-distribution-needs-to-be-tuned-to-both-p_x-and-hx" class="section level3">
&lt;h3>The proposal distribution needs to be tuned to both p_X and h(x)&lt;/h3>
&lt;p>This is a somewhat subtle point of the derivation provided above. We &lt;em>don’t&lt;/em> just want a distribution that will be highest here &lt;span class="math inline">\(h(x)\)&lt;/span> is high. Instead, what we actually need is a distribution that will be highest when &lt;span class="math inline">\(h(x)p_X(x)\)&lt;/span> is high. That will be exactly where the two distributions intersect, at 2.&lt;/p>
&lt;pre class="r">&lt;code>x &amp;lt;- seq(-5, 10, length.out = 1000)
plot(x, hx(x), type = &amp;quot;l&amp;quot;)
points(x, dnorm(x), col = &amp;quot;blue&amp;quot;, type = &amp;quot;l&amp;quot;)
abline(v = 2)&lt;/code>&lt;/pre>
&lt;div class="figure">&lt;span id="fig:matching">&lt;/span>
&lt;img src="https://psadil.github.io/psadil/post/importance-sampling/index.en_files/figure-html/matching-1.png" alt="Same as above, but with line demonstrating intersection at x=2" width="672" />
&lt;p class="caption">
Figure 2: Same as above, but with line demonstrating intersection at x=2
&lt;/p>
&lt;/div>
&lt;pre class="r">&lt;code>y &amp;lt;- rnorm(1e+06, mean = 2)
h &amp;lt;- hx(y)
IS &amp;lt;- h * dnorm(y, 0)/dnorm(y, 2)
mean(IS)&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] 0.005165774&lt;/code>&lt;/pre>
&lt;pre class="r">&lt;code>var(IS)&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] 4.131799e-06&lt;/code>&lt;/pre>
&lt;p>The answer is still accurate, but the variance has been reduced by many factors. This means that a desired level of MCSE could be achieved with many fewer samples.&lt;/p>
&lt;p>One final demonstration, remember that &lt;span class="math inline">\(h(x)p_X(x)\)&lt;/span> describes a distribution. Hence it would be a mistake to try a &lt;span class="math inline">\(p_Y\)&lt;/span> that placed all of the density around that point of intersection. For example, let’s try &lt;span class="math inline">\(Y \sim N(2,0.1)\)&lt;/span>. Although that is centered on the region that contributes the largest values to the expectation, the largest values alone do not define the expectation; too much of the tails of &lt;span class="math inline">\(h(x)\)&lt;/span> are not included. Using this results is the worst variance.&lt;/p>
&lt;pre class="r">&lt;code>y &amp;lt;- rnorm(1e+06, mean = 2, 0.1)
h &amp;lt;- hx(y)
IS &amp;lt;- h * dnorm(y, 0)/dnorm(y, 2, 0.1)
mean(IS)&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] 0.002601838&lt;/code>&lt;/pre>
&lt;pre class="r">&lt;code>var(IS)&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] 0.006540712&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div id="other-references" class="section level1">
&lt;h1>Other References&lt;/h1>
&lt;ul>
&lt;li>&lt;a href="https://en.wikipedia.org/wiki/Gaussian_function#Integral_of_a_Gaussian_function">integral of Gaussian Function&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://people.math.umass.edu/~lr7q/m697u-spring2018/m697uhome.html">course notes&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.statlect.com/asymptotic-theory/importance-sampling">statlect&lt;/a>&lt;/li>
&lt;/ul>
&lt;/div>
&lt;div class="footnotes">
&lt;hr />
&lt;ol>
&lt;li id="fn1">&lt;p>this page is mostly a study page for upcoming comprehensive exams&lt;a href="#fnref1" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>Anonymizing MTurk WorkerIDs</title><link>https://psadil.github.io/psadil/post/anonymizing-mtruk-worker-ids/</link><pubDate>Tue, 07 Aug 2018 00:00:00 +0000</pubDate><guid>https://psadil.github.io/psadil/post/anonymizing-mtruk-worker-ids/</guid><description>
&lt;script src="https://psadil.github.io/psadil/post/anonymizing-mtruk-worker-ids/index.en_files/header-attrs/header-attrs.js">&lt;/script>
&lt;p>It may be the case that Amazon Mechanical Turk WorkerIDs are not anonymous. &lt;a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2228728">Lease et al., 2013&lt;/a> describe at length how personally identifying information may be exposed when a researcher shares WorkerIDs. It is unclear to me the extent to which Amazon constructs their WorkerIDs at present, given that one of their striking demonstrations did not apply to my WorkerID. That is, they describe simply googling the WorkerID and receiving a picture of the participant, along with their full name. My WorkerID turn up nothing. Though, I have only been a worker on MTurk for a short while, so maybe I’ve been lucky and my ID has just not yet been shared widely.&lt;/p>
&lt;p>Regardless, providing extra anonymity to participants isn’t too much trouble. This post serves as documentation for a brief script that takes a sqlite database produced by running an experiment in jspsych + psiturk and replaces all instances of the WorkerID with a more secure code.&lt;/p>
&lt;p>The script relies on five R libraries&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;a href="https://magrittr.tidyverse.org">magrittr&lt;/a>, for ease of writing&lt;/li>
&lt;li>&lt;a href="https://dplyr.tidyverse.org">dplyr&lt;/a>, through (dbplyr)[cran.r-project.org/web/packages/dbplyr/vignettes/dbplyr.html], serves as the way to interface with the sqlite database&lt;/li>
&lt;li>&lt;a href="https://github.com/jeroen/openssl#readme">openssl&lt;/a> constructs a more secure identifier for each participant that can still be used to cross reference them across studies&lt;/li>
&lt;li>&lt;a href="https://stringr.tidyverse.org">stringr&lt;/a> does the work of replacing instances of the WorkerID with the more secure code&lt;/li>
&lt;li>&lt;a href="https://github.com/docopt/docopt.R">docopt&lt;/a>, wraps up the Rscript such that it can be called from the command line (in an environment in which Rscript is the name of a function. i.e., may be Rscript.exe in Windows powershell)&lt;/li>
&lt;/ol>
&lt;pre class="r">&lt;code>#!/usr/bin/env Rscript
# Anonymize participants database NOTE: always overrides the file --outfile
library(docopt)
doc &amp;lt;- &amp;quot;Usage:
anonymize_db.R [-i DBNAME] KEY OUTFILE
anonymize_db.R -h
Options:
-i --infile DBNAME sqlite database filename from which to read [default: participants_raw.db]
-t --table TABLE name of table within database to anonymize [default: participants]
-h --help show this help text
Arguments:
KEY key to salt WorkerIDs for extra security
OUTFILE sqlite database filename to write&amp;quot;
opt &amp;lt;- docopt(doc)
library(magrittr)
library(dplyr)
library(stringr)
library(openssl)
db &amp;lt;- dplyr::src_sqlite(opt$infile) %&amp;gt;%
dplyr::tbl(opt$table) %&amp;gt;%
dplyr::collect() %&amp;gt;%
dplyr::mutate(uniqueid = stringr::str_replace(uniqueid, workerid, openssl::sha256(workerid,
key = opt$KEY)), datastring = dplyr::case_when(is.na(datastring) ~ datastring,
TRUE ~ stringr::str_replace_all(datastring, workerid, openssl::sha256(workerid,
key = opt$KEY))), workerid = openssl::sha256(workerid, key = opt$KEY))
message(paste0(&amp;quot;read raw database: &amp;quot;, opt$infile))
con &amp;lt;- DBI::dbConnect(RSQLite::SQLite(), opt$OUTFILE)
dplyr::copy_to(con, db, opt$table, temporary = FALSE, indexes = list(&amp;quot;uniqueid&amp;quot;),
overwrite = TRUE)
DBI::dbDisconnect(con)
message(paste0(&amp;quot;wrote anonymized database: &amp;quot;, opt$OUTFILE))
message(paste0(&amp;quot;Store your KEY securely if you want the same WorkerIDs to create the same HMACs!&amp;quot;))&lt;/code>&lt;/pre>
&lt;p>As stated in the initial string of this script, a typical call might be&lt;/p>
&lt;p>&lt;code>anonymize_db.R longandsecurelystoredsalt participants.db&lt;/code>&lt;/p>
&lt;p>which will read in the sqlite database &lt;code>participants_raw.db&lt;/code> (default for –infile), convert all instances of WorkerID into a hash-digest with the sha256 algorithm, and store the result in a new sqlite database called &lt;code>participants.db&lt;/code>.&lt;/p>
&lt;p>The general workflow would be to include in your .gitignore the raw database output by psiturk. That way, the raw database is never uploaded into any repository. Then, when you are ready to host the experiment, you pull your repository as usual. As an extra step, you will now need to separately move around your raw database such that when you run the next experiment psiturk will know which workers have already participated. After collecting data, retrieve the database and run this anonymization script on it. The newly created database can then be bundled with your repository.&lt;/p>
&lt;p>This is a bit of extra work (i.e., you must manually send the database, retrieve the database, then anonymize it). However, the whole point is to avoid making it easy to download something with potentially identifying information.&lt;/p>
&lt;div id="gotchas" class="section level1">
&lt;h1>Gotchas&lt;/h1>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;p>This function will overwrite any database of the same name as OUTFILE. Though, that’s often not an issue. If you’ve anonymized a database (call the result &lt;code>participants.db&lt;/code>), added new participants to the same raw database, and then anonymize the raw database again, those participants that were anonymized in the first round will be re-anonymized and included in the new result.&lt;/p>&lt;/li>
&lt;li>&lt;p>If you want this function to convert a given WorkerID into a consistent code, you’ll need to call it with the same value for KEY.&lt;/p>&lt;/li>
&lt;li>&lt;p>It would be more secure to use a salt of random length for each participant separately.&lt;/p>&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>Amazon Mechanical Turk in cMAP-CEMNL, part 1</title><link>https://psadil.github.io/psadil/post/mechanical-turk-part-i/</link><pubDate>Fri, 20 Jul 2018 00:00:00 +0000</pubDate><guid>https://psadil.github.io/psadil/post/mechanical-turk-part-i/</guid><description>
&lt;script src="https://psadil.github.io/psadil/post/mechanical-turk-part-i/index.en_files/header-attrs/header-attrs.js">&lt;/script>
&lt;div id="goals" class="section level1">
&lt;h1>Goals&lt;/h1>
&lt;ol style="list-style-type: decimal">
&lt;li>Provide high-level overview of the tools used to run an MTurk study&lt;/li>
&lt;li>Highlight steps at which to be careful while setting the study up&lt;/li>
&lt;/ol>
&lt;p>Part 2 will cover a basic project&lt;/p>
&lt;/div>
&lt;div id="the-current-lab-practice-is-to-string-together-many-different-tools" class="section level1">
&lt;h1>The current lab practice is to string together many different tools&lt;/h1>
&lt;p>Setting up a study on MTurk can roughly be divided into three needs.&lt;/p>
&lt;ul>
&lt;li>We need some way to code the experiment
&lt;ul>
&lt;li>Packages
&lt;ul>
&lt;li>&lt;a href="https://www.jspsych.org">jspsych&lt;/a>&lt;/li>
&lt;li>jquery &lt;a href="https://www.w3schools.com/jquery/default.asp">w3schools&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>Languages
&lt;ul>
&lt;li>Javascript &lt;a href="https://www.w3schools.com/js/default.asp">w3schools&lt;/a>&lt;/li>
&lt;li>CSS &lt;a href="https://www.w3schools.com/css/default.asp">w3schools&lt;/a>&lt;/li>
&lt;li>HTML &lt;a href="https://www.w3schools.com/html/default.asp">w3schools&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>Useful concepts
&lt;ul>
&lt;li>Turns out that, in javascript, line 2 might not run after line 1! It’s helpful to read about &lt;a href="https://javascript.info/callbacks">asynchronous programming in javascript&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://about.gitlab.com/2016/06/03/ssg-overview-gitlab-pages-part-1-dynamic-x-static/">Static sites, dynamic sites, and static site generators&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>We need some way to organize that experimental code such that it is advertised to turkers, Amazon is informed when a Turker finishes the experiment, their responses are stored in a database, and we can approve the turkers’ work
&lt;ul>
&lt;li>Packages
&lt;ul>
&lt;li>&lt;a href="https://psiturk.readthedocs.io/en/latest/">psiturk&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://flask.palletsprojects.com/en/2.0.x/">flask&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.palletsprojects.com/p/jinja/">jinja&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>Languages
&lt;ul>
&lt;li>&lt;a href="https://python.swaroopch.com/">Python&lt;/a>, &lt;a href="https://www.anaconda.com/download/">Anaconda distribution&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://cran.r-project.org/web/packages/dbplyr/vignettes/dbplyr.html">SQLite&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>Useful concepts
&lt;ul>
&lt;li>&lt;a href="https://taylorwhitten.github.io/blog/RSQLite1">what is a database?&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.howtogeek.com/66214/how-to-forward-ports-on-your-router/">what is an ip address?&lt;/a>&lt;/li>
&lt;li>Psiturk runs on python 2, not python 3. You may want to be using an &lt;a href="https://conda.io/docs/user-guide/tasks/manage-environments.html">Anaconda virtual environment&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>We need a server to host the above components such that they are accessible via an internet connection
&lt;ul>
&lt;li>Services
&lt;ul>
&lt;li>Amazon Mechanical Turk: follow the &lt;a href="https://psiturk.readthedocs.io/en/latest/amt_setup.html#">psiturk documentation!&lt;/a>&lt;/li>
&lt;li>Amazon EC2: follow the &lt;a href="https://psiturk.readthedocs.io/en/latest/amazon_ec2.html">psiturk documentation!&lt;/a>&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>useful concepts
&lt;ul>
&lt;li>definitely keep the psiturk documentation open in a tab somewhere&lt;/li>
&lt;/ul>&lt;/li>
&lt;/ul>&lt;/li>
&lt;/ul>
&lt;p>In reverse, the EC2 instance is the server that will host the experiment. Psiturk organizes a bunch of packages (which are primarily written in python) to function as the ‘backend,’ coordinating the different webpages that the turk worker will see. Although psiturk also includes a javascript library that could be used to write the actual experiment, it will probably be much easier to write the experiment with the jspsych package.&lt;/p>
&lt;p>At the next deeper level, it’s useful to know how HTML, javascript, and CSS work to make websites run.&lt;/p>
&lt;p>Next, the template part comes from the idea of a static site generator. The particular SSG is jinja. It’s useful to know about Flask, because the Flask syntax defines custom routes (e.g., helping determine which stimuli to show a given participant). Jinja might be useful, but I haven’t found it necessary to know more than the basics of what is an SSG.&lt;/p>
&lt;p>&lt;a href="https://gureckislab.org/courses/spring14/online_data_collection/">Todd Gureckis’ videotapped course lectures&lt;/a>
&lt;a href="https://bradylab.ucsd.edu/ttt/index.html">Tim Brady’s Mechanical Turk Tutorial&lt;/a>
&lt;a href="https://wilmabainbridge.com/bigdataclass.html">Wilma Bainbridge organized a Big Data Tutorial at VSS 2018&lt;/a>. Note, this involves the package psitoolkit. That seemed like an okay alternative, but I worried that I would encounter a situation that psitoolkit wasn’t equipped to handle and would be stuck. Working with the psiturk + jspsych ensured that there would be the flexibility to run pretty much any kind of experiment&lt;/p>
&lt;/div>
&lt;div id="gotchas-and-extra-notes" class="section level1">
&lt;h1>Gotcha’s and extra notes&lt;/h1>
&lt;ul>
&lt;li>&lt;p>Psiturk runs on python 2, not 3. If you’re trying to install psiturk and you immediately start getting errors, make sure to check which python version you’re on&lt;/p>&lt;/li>
&lt;li>&lt;p>when setting up EC2, pay careful attention to the IP configuration settings. To be able to use ssh to access the instance, you’ll need to have your IP address match the IP it’s expecting. Or, just set it to receive traffic from ‘Anywhere.’ Likewise, make sure that the custom TCP is set to receive traffic from Anywhere&lt;/p>&lt;/li>
&lt;li>&lt;p>Do read the tutorial on &lt;a href="https://javascript.info/callbacks">asynchronous programming in javascript&lt;/a>. It can be really confusing when you’re trying to debug and variables aren’t defined&lt;/p>&lt;/li>
&lt;li>&lt;p>Speaking of debugging, your friend will be the ‘developer tools’ in whatever browser you’re using. Right click on the experiment and check ‘view source’ to get access to a console&lt;/p>&lt;/li>
&lt;li>&lt;p>Using the psiturk &lt;code>debug&lt;/code> command will attempt to open up the experiment in the browser. This will only work if no browser is currently open.&lt;/p>&lt;/li>
&lt;li>&lt;p>if an experiment involves a lot of media, the media can be optimized a bit using something like &lt;a href="http://optipng.sourceforge.net/">optipng&lt;/a>. This keeps the png looking fine but decreases load time&lt;/p>&lt;/li>
&lt;/ul>
&lt;/div></description></item><item><title>Privacy Policy</title><link>https://psadil.github.io/psadil/privacy/</link><pubDate>Thu, 28 Jun 2018 00:00:00 +0100</pubDate><guid>https://psadil.github.io/psadil/privacy/</guid><description>&lt;p>&amp;hellip;&lt;/p></description></item><item><title>Terms</title><link>https://psadil.github.io/psadil/terms/</link><pubDate>Thu, 28 Jun 2018 00:00:00 +0100</pubDate><guid>https://psadil.github.io/psadil/terms/</guid><description>&lt;p>&amp;hellip;&lt;/p></description></item><item><title>eyetracking with eyelink in psychtoolbox</title><link>https://psadil.github.io/psadil/post/eyetracking-init/</link><pubDate>Tue, 05 Jun 2018 00:00:00 +0000</pubDate><guid>https://psadil.github.io/psadil/post/eyetracking-init/</guid><description>
&lt;script src="https://psadil.github.io/psadil/post/eyetracking-init/index.en_files/header-attrs/header-attrs.js">&lt;/script>
&lt;p>UPDATE: I now think that the examples I’ve presented here obscure the interface with Eyelink. Much cleaner to use MATLAB’s object oriented programming. This is covered in &lt;a href="https://psadil.github.io/psadil/post/eyetracking-in-psychtoolbox-oop/">another post&lt;/a>.&lt;/p>
&lt;p>This post is designed as minimal documentation for using the Eyelink software at the UMass Amherst &lt;a href="https://www.umass.edu/ials/hmrc">hMRC&lt;/a>. The goals are very modest&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>Provide sample &lt;a href="http://psychtoolbox.org">Psychtoolbox&lt;/a> (PTB) and MATLAB code for integrating eyelink&lt;/li>
&lt;li>Explain a few parameters that you might want to change in your experiment&lt;/li>
&lt;/ol>
&lt;p>The main audience includes members of the cMAP and CEMNL labs at UMass, but other users of the hMRC may also benefit. This post includes various lines of code throughout this post, but the full files can be downloaded from the links at the bottom. Many of those links are private and will only work if you are a member of one of those labs.&lt;/p>
&lt;p>NOTE: This post is not designed to be a full introduction to the Eyelink toolbox within PTB. I’m not qualified to give a detailed tutorial. These are just a few bits of code that I have found useful. But, my needs have so far been really simple (i.e., make a record of where the eyes were during each run so that runs can be discarded if fixations during that run deviate more than x degrees from the center of the screen). The main resource in this post is probably the collection of links in the next section.&lt;/p>
&lt;div id="background-links-installing-extra-software" class="section level1">
&lt;h1>Background links + installing extra software&lt;/h1>
&lt;p>You’ll need to download the Eyelink API provided by SR Research. To do that, register an account &lt;a href="https://www.sr-support.com">here&lt;/a>. Note that they moderate the accounts fairly heavily, so it may take 24 hrs+ for the registration to go though. Once you’re registered, you can download the developers kit API ( &lt;a href="https://www.sr-support.com/forum/downloads/eyelink-display-software/39-eyelink-developers-kit-for-windows-windows-display-software">Windows&lt;/a>, &lt;a href="https://www.sr-support.com/forum/downloads/eyelink-display-software/46-eyelink-developers-kit-for-linux-linux-display-software">Linux&lt;/a> ). You’ll need that kit to be able to call Eyelink functions from within matlab (otherwise you get an error about missing mex files whenever you search for help pages). Registering also gives access to a support forum.&lt;/p>
&lt;p>Before moving to the next session, it may make sense to look through their &lt;a href="https://www.sr-support.com/forum/downloads/manuals">manuals&lt;/a>. If you have access to our box folder, here’s a link to the relevant &lt;a href="https://umass.box.com/s/1nr9m302wqn5l2jd9kaf9guv8ngqa9wp">Eyelink II manual&lt;/a> and the &lt;a href="https://umass.box.com/s/n8ki3br7watw2niuangxxflj6ulpnk67q">Data Viewer&lt;/a>. The manuals are, well, manuals, but reading through them takes less time than their length might suggest. If you are not a member of our lab, you may be able to ask a member of the hMRC to share the manuals.&lt;/p>
&lt;p>Without a licensing key, the version of the data viewer that can be downloaded is more or less useless (but, &lt;a href="https://www.sr-support.com/forum/downloads/data-analysis/4557-eyelink-data-viewer?4434-EyeLink-Data-Viewer=">here it is&lt;/a>). Instead, for working with the data in R, see &lt;a href="https://github.com/jashubbard/edfR">edfR&lt;/a> and &lt;a href="https://github.com/jashubbard/itrackR">itrackR&lt;/a>. Note that these are only working on Mac and Linux. So, you may need to be working on the server to install / use those libraries. Alternatively, you can also read the edf files directly into matlab using &lt;a href="https://www.sr-support.com/forum/downloads/data-analysis/5446-edfmex-reading-edf-data-directly-into-matlab">EDFMEX&lt;/a>. However, I won’t be able to help much with using these packages, given that I only discovered them while writing this post.&lt;/p>
&lt;p>Kwan-Jin Jung wrote a technical note about the eyetracking system, &lt;a href="https://www.umass.edu/ials/sites/default/files/hmrc_tn_eye_monitoring_during_fmri_scan.pdf">see here&lt;/a>, and here’s the &lt;a href="https://www.sr-research.com/products/eyelink-1000-plus/#LongRangeMount">advertisement for our tracker&lt;/a>.&lt;/p>
&lt;/div>
&lt;div id="sec:init" class="section level1">
&lt;h1>Initializing Eyelink&lt;/h1>
&lt;p>This section walks through a function that initializes the eyelink system. The first step to interfacing with the Eyelink is to call the PTB command &lt;a href="https://web.archive.org/web/20171214112707/http://docs.psychtoolbox.org/EyelinkInitDefaults">&lt;code>EyelinkInitDefaults&lt;/code>&lt;/a>. This defines a struct with a number of default parameters, &lt;code>el&lt;/code> about how the eyetracker will operate. I generally don’t want all of those defaults, so the function below modifies them as needed. After the parameters in &lt;code>el&lt;/code> have been modified, this function calls &lt;a href="https://web.archive.org/web/20171214035622/http://docs.psychtoolbox.org/EyelinkUpdateDefaults">&lt;code>EyelinkUpdateDefaults(el)&lt;/code>&lt;/a> to indicate to inform the eyelink system that the parameters should change.&lt;/p>
&lt;p>The main other point of this function is to start the eyetracker calibration. That should be done at the start of each run.&lt;/p>
&lt;pre class="matlab">&lt;code>
function [el, exit_flag] = setupEyeTracker( tracker, window, constants )
% SET UP TRACKER CONFIGURATION. Main goal is to modify defaults set in EyelinkInitDefaults.
%{
REQUIRED INPUT:
tracker: string, either &amp;#39;none&amp;#39; or &amp;#39;T60&amp;#39;
window: struct containing at least the fields
window.background: background color (whatever was set during call to e.g., PsychImaging(&amp;#39;OpenWindow&amp;#39;, window.screenNumber, window.background))
window.white: numeric defining the color white for the open window (e.g., window.white = WhiteIndex(window.screenNumber);)
window.pointer: scalar pointing to main screen (e.g., [window.pointer, window.winRect] = PsychImaging(&amp;#39;OpenWindow&amp;#39;, ...
window.screenNumber,window.background);)
window.winRect; PsychRect defining size of main window (e.g., [window.pointer, window.winRect] = PsychImaging(&amp;#39;OpenWindow&amp;#39;, ...
window.screenNumber,window.background);)
constants: struct containing at least
constants.eyelink_data_fname: string defining eyetracking data to be saved. Cannot be longer than 8 characters (before file extention). File extension must be &amp;#39;.edf&amp;#39;. (e.g., constants.eyelink_data_fname = [&amp;#39;scan&amp;#39;, num2str(input.runnum, &amp;#39;%02d&amp;#39;), &amp;#39;.edf&amp;#39;];)
OUTPUT:
if tracker == &amp;#39;T60&amp;#39;
el: struct defining parameters that have been set up about the eyetracker (see EyelinkInitDefaults)
if tracker == &amp;#39;none&amp;#39;
el == []
exit_flag: string that can be used to check whether this function exited successfully
SIDE EFFECTS:
When tracker == &amp;#39;T60&amp;#39;, calibration is started
%}
%%
exit_flag = &amp;#39;OK&amp;#39;;
switch tracker
case &amp;#39;T60&amp;#39;
% Provide Eyelink with details about the graphics environment
% and perform some initializations. The information is returned
% in a structure that also contains useful defaults
% and control codes (e.g. tracker state bit and Eyelink key values).
el = EyelinkInitDefaults(window.pointer);
% overrride default gray background of eyelink, otherwise runs end
% up gray! also, probably best to calibrate with same colors of
% background / stimuli as participant will encounter
el.backgroundcolour = window.background;
el.foregroundcolour = window.white;
el.msgfontcolour = window.white;
el.imgtitlecolour = window.white;
el.calibrationtargetcolour=[window.white window.white window.white];
EyelinkUpdateDefaults(el);
if ~EyelinkInit(0, 1)
fprintf(&amp;#39;\n Eyelink Init aborted \n&amp;#39;);
exit_flag = &amp;#39;ESC&amp;#39;;
return;
end
%Reduce FOV
Eyelink(&amp;#39;command&amp;#39;,&amp;#39;calibration_area_proportion = 0.5 0.5&amp;#39;);
Eyelink(&amp;#39;command&amp;#39;,&amp;#39;validation_area_proportion = 0.48 0.48&amp;#39;);
% open file to record data to
i = Eyelink(&amp;#39;Openfile&amp;#39;, constants.eyelink_data_fname);
if i ~= 0
fprintf(&amp;#39;\n Cannot create EDF file \n&amp;#39;);
exit_flag = &amp;#39;ESC&amp;#39;;
return;
end
Eyelink(&amp;#39;command&amp;#39;, &amp;#39;add_file_preamble_text &amp;#39;&amp;#39;Recorded by NAME OF EXPERIMENT&amp;#39;&amp;#39;&amp;#39;);
% Setting the proper recording resolution, proper calibration type,
% as well as the data file content;
Eyelink(&amp;#39;command&amp;#39;,&amp;#39;screen_pixel_coords = %ld %ld %ld %ld&amp;#39;, 0, 0, window.winRect(3)-1, window.winRect(4)-1);
Eyelink(&amp;#39;message&amp;#39;, &amp;#39;DISPLAY_COORDS %ld %ld %ld %ld&amp;#39;, 0, 0, window.winRect(3)-1, window.winRect(4)-1);
% set calibration type.
Eyelink(&amp;#39;command&amp;#39;, &amp;#39;calibration_type = HV5&amp;#39;);
% set EDF file contents using the file_sample_data and
% file-event_filter commands
% set link data thtough link_sample_data and link_event_filter
Eyelink(&amp;#39;command&amp;#39;, &amp;#39;file_event_filter = LEFT,RIGHT,FIXATION,SACCADE,BLINK,MESSAGE,BUTTON,INPUT&amp;#39;);
Eyelink(&amp;#39;command&amp;#39;, &amp;#39;link_event_filter = LEFT,RIGHT,FIXATION,SACCADE,BLINK,MESSAGE,BUTTON,INPUT&amp;#39;);
% check the software version
% add &amp;quot;HTARGET&amp;quot; to record possible target data for EyeLink Remote
Eyelink(&amp;#39;command&amp;#39;, &amp;#39;file_sample_data = LEFT,RIGHT,GAZE,HREF,GAZERES,AREA,HTARGET,STATUS,INPUT&amp;#39;);
Eyelink(&amp;#39;command&amp;#39;, &amp;#39;link_sample_data = LEFT,RIGHT,GAZE,HREF,GAZERES,AREA,HTARGET,STATUS,INPUT&amp;#39;);
% make sure we&amp;#39;re still connected.
if Eyelink(&amp;#39;IsConnected&amp;#39;)~=1 &amp;amp;&amp;amp; input.dummymode == 0
exit_flag = &amp;#39;ESC&amp;#39;;
return;
end
% possible changes from EyelinkPictureCustomCalibration
% set sample rate in camera setup screen
Eyelink(&amp;#39;command&amp;#39;, &amp;#39;sample_rate = %d&amp;#39;, 1000);
% Will call the calibration routine
EyelinkDoTrackerSetup(el);
case &amp;#39;none&amp;#39;
el = [];
end
end
&lt;/code>&lt;/pre>
&lt;p>Here are a few parts of that function that you will probably want to adapt for your experiment.&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>The various color arguments&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>Eyelink changes the background color of whatever screen is open. So, these colors (e.g., &lt;code>el.backgroundcolour&lt;/code>) should match whatever background your stimuli will be displayed on.&lt;/li>
&lt;/ul>
&lt;ol start="2" style="list-style-type: decimal">
&lt;li>&lt;code>Eyelink('command','calibration_area_proportion = 0.5 0.5');&lt;/code> and &lt;code>Eyelink('command','validation_area_proportion = 0.48 0.48');&lt;/code>&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>The setup at the scanner has a hard time tracking eyes that are fixating near the edges of the screen. The issue is bad enough that it can be almost impossible to calibrate the tracker when the calibration dots appear on the edges. I only really use the eyetracker to have a record confirming that participants were more-or-less fixating during a run, so good calibration at the edges isn’t important to me. For this reason, I reduce the size of the calibration.&lt;/li>
&lt;/ul>
&lt;ol start="3" style="list-style-type: decimal">
&lt;li>Related to 2: &lt;code>Eyelink('command', 'calibration_type = HV5');&lt;/code>&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>This sets the calibration routine to only use 5 dots, rather than 9. Again, my needs are pretty simple and calibration can be challenging, so 5 seems good enough.&lt;/li>
&lt;/ul>
&lt;ol start="4" style="list-style-type: decimal">
&lt;li>Wrapping the function in a switch argument (e.g., &lt;code>tracker ==&lt;/code>)&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>See the next section for some of the logic in writing code with a switch statement or two that all depends on how an initial variable is set&lt;a href="#fn1" class="footnote-ref" id="fnref1">&lt;sup>1&lt;/sup>&lt;/a>.&lt;/li>
&lt;/ul>
&lt;/div>
&lt;div id="the-eyelink-functions" class="section level1">
&lt;h1>The Eyelink functions&lt;/h1>
&lt;p>In &lt;code>setupEyeTracker&lt;/code>, you may have noticed many calls that took the following format &lt;a href="https://web.archive.org/web/20171214045939/http://docs.psychtoolbox.org/Eyelink">&lt;code>Eyelink('dosomethingspecial');&lt;/code>&lt;/a>. Commands like these are PTB’s way of communicating with the Eyelink software.&lt;/p>
&lt;p>There are a few such functions that you’ll need to include to record any usable data. First, the function we defined above, &lt;code>setupEyeTracker&lt;/code>, called the function &lt;a href="https://web.archive.org/web/20171214112703/http://docs.psychtoolbox.org/EyelinkDoTrackerSetup">&lt;code>EyelinkDoTrackerSetup(el)&lt;/code>&lt;/a>. This is a function internal to PTB. It runs the calibration routine. So, you’ll want a call to &lt;code>[el, exitflag] = setupEyeTracker( input.tracker, window, constants );&lt;/code> somewhere early in your code. I rerun the calibration at the start of each experimental run.&lt;/p>
&lt;p>Next, the following commands make sure that you’ve turned on the eyetracker&lt;/p>
&lt;pre class="matlab">&lt;code>% Must be offline to draw to EyeLink screen
Eyelink(&amp;#39;Command&amp;#39;, &amp;#39;set_idle_mode&amp;#39;);
% clear tracker display
Eyelink(&amp;#39;Command&amp;#39;, &amp;#39;clear_screen 0&amp;#39;);
Eyelink(&amp;#39;StartRecording&amp;#39;);
% always wait a moment for recording to have definitely started
WaitSecs(0.1);&lt;/code>&lt;/pre>
&lt;p>Eyelink will save it’s files in a specialized format&lt;a href="#fn2" class="footnote-ref" id="fnref2">&lt;sup>2&lt;/sup>&lt;/a>. For that file, it’s useful to mark when the experiment has actually started. So, include a command like&lt;/p>
&lt;pre class="matlab">&lt;code>Eyelink(&amp;#39;message&amp;#39;, &amp;#39;SYNCTIME&amp;#39;);&lt;/code>&lt;/pre>
&lt;p>to mark the start. Since this will probably be run in the scanner, a sensible time to place that would be shortly after receiving the scanner trigger, but before the next flip.&lt;/p>
&lt;p>When you’re done with the experiment run &lt;a href="https://web.archive.org/web/20171214045939/http://docs.psychtoolbox.org/Eyelink">&lt;code>Eyelink('Command', 'set_idle_mode');&lt;/code>&lt;/a> before saving data. Here’s an example of a short routine to save the data. I’ve defined a variable &lt;code>constants.eyelink_data_fname&lt;/code> to be a string that ends in ‘.edf’. Note that the filename can be no longer than 8 characters and cannot contain any special characters (only digits and letters).&lt;/p>
&lt;pre class="matlab">&lt;code>% the Eyelink(&amp;#39;ReceiveFile&amp;#39;) function does not wait for the file
% transfer to complete so you must have the entire try loop
% surrounding the function to ensure complete transfer of the EDF.
try
fprintf(&amp;#39;Receiving data file &amp;#39;&amp;#39;%s&amp;#39;&amp;#39;\n&amp;#39;, constants.eyelink_data_fname );
status = eyetrackerFcn(&amp;#39;ReceiveFile&amp;#39;);
if status &amp;gt; 0
fprintf(&amp;#39;ReceiveFile status %d\n&amp;#39;, status);
end
if 2==exist(edfFile, &amp;#39;file&amp;#39;)
fprintf(&amp;#39;Data file &amp;#39;&amp;#39;%s&amp;#39;&amp;#39; can be found in &amp;#39;&amp;#39;%s&amp;#39;&amp;#39;\n&amp;#39;, constants.eyelink_data_fname, pwd );
end
catch
fprintf(&amp;#39;Problem receiving data file &amp;#39;&amp;#39;%s&amp;#39;&amp;#39;\n&amp;#39;, constants.eyelink_data_fname );
end
&lt;/code>&lt;/pre>
&lt;/div>
&lt;div id="sample-script" class="section level1">
&lt;h1>Sample script&lt;/h1>
&lt;p>Unfortunately, attempting to call these function from a computer that does not have Eyelink’s software installed will produce an error. This makes developing and testing an experimental script challenging, because if we litter our code with calls to &lt;a href="https://web.archive.org/web/20171214045939/http://docs.psychtoolbox.org/Eyelink">&lt;code>Eyelink(...)&lt;/code>&lt;/a>, then when we’re not at the scanner computer we need to comment out all of those lines. I have no faith that I’ll remember to uncomment all of these lines when I’m at the scanner each time, so when I’m writing code that calls these functions I place them in a wrapper. Credit goes to &lt;a href="https://people.umass.edu/whopper/">Will Hopper&lt;/a> for showing me this strategy when designing functions that receive input.&lt;/p>
&lt;p>The main idea is two wrap all calls to &lt;code>Eyelink(...)&lt;/code> with a function that starts like this&lt;/p>
&lt;pre class="matlab">&lt;code>
function eyelinkFcn = makeEyelinkFcn(handlerName)
valid_types = {&amp;#39;none&amp;#39;,&amp;#39;T60&amp;#39;};
assert(ismember(handlerName, valid_types),...
[&amp;#39;&amp;quot;handlerType&amp;quot; argument must be one of the following: &amp;#39; strjoin(valid_types,&amp;#39;, &amp;#39;)])
switch handlerName
case &amp;#39;T60&amp;#39;
eyelinkFcn = @T60;
case &amp;#39;none&amp;#39;
eyelinkFcn = @do_nothing;
end
% more code to follow
end
&lt;/code>&lt;/pre>
&lt;p>The outer function, &lt;code>makeEyelinkFcn&lt;/code> receives as input the variable &lt;code>handlerName&lt;/code>, which can be either &lt;code>none&lt;/code> or &lt;code>T60&lt;/code>. Depending on that variable, the output to eyelinkFcn is then a call to an anonymous function which implements the actual calls to Eyelink. When &lt;code>handlerName == 'T60'&lt;/code>, &lt;code>makeEyelinkFcn&lt;/code> returns a function that is going to try to call various &lt;code>Eyelink(...)&lt;/code> routines (shown below). But, when &lt;code>handlerName == 'none'&lt;/code> &lt;code>makeEyelinkFcn&lt;/code> will return a function that does nothing.&lt;/p>
&lt;p>This enables the writing of code that will call the eyelink functions when desired (e.g., when at the scanner), but calls to those functions can also be avoided when desired (by calling &lt;code>makeEyeLinkFcn('none')&lt;/code> instead of &lt;code>makeEyeLinkFcn('T60')&lt;/code>).&lt;/p>
&lt;pre class="matlab">&lt;code>
% ...
eyetrackerFcn = makeEyelinkFcn(input.tracker);
eyetrackerFcn(&amp;#39;message&amp;#39;, &amp;#39;SYNCTIME&amp;#39;);
% ...
&lt;/code>&lt;/pre>
&lt;p>So long as input.tracker is taking different values, there’s no need to comment or uncomment when I’m working on a computer that has or doesn’t have an eyelink hooked up&lt;a href="#fn3" class="footnote-ref" id="fnref3">&lt;sup>3&lt;/sup>&lt;/a>.&lt;/p>
&lt;p>The remainder of this script defines the local function &lt;code>T60&lt;/code>, which allows all of the necessary wrapping to the different &lt;code>Eyelink(...)&lt;/code> commands.&lt;/p>
&lt;pre class="matlab">&lt;code>
function eyelinkFcn = makeEyelinkFcn(handlerName)
valid_types = {&amp;#39;none&amp;#39;,&amp;#39;T60&amp;#39;};
assert(ismember(handlerName, valid_types),...
[&amp;#39;&amp;quot;handlerType&amp;quot; argument must be one of the following: &amp;#39; strjoin(valid_types,&amp;#39;, &amp;#39;)])
switch handlerName
case &amp;#39;T60&amp;#39;
eyelinkFcn = @T60;
case &amp;#39;none&amp;#39;
eyelinkFcn = @do_nothing;
end
function status = T60(varargin)
status = [];
switch varargin{1}
case &amp;#39;EyelinkDoDriftCorrection&amp;#39;
% Do a drift correction at the beginning of each trial
% Performing drift correction (checking) is optional for
% EyeLink 1000 eye trackers.
EyelinkDoDriftCorrection(varargin{2},[],[],0);
case &amp;#39;Command&amp;#39;
Eyelink(&amp;#39;Command&amp;#39;, varargin{2})
case &amp;#39;ImageTransfer&amp;#39;
%transfer image to host
transferimginfo = imfinfo(varargin{2});
[width, height] = Screen(&amp;#39;WindowSize&amp;#39;, 0);
% image file should be 24bit or 32bit b5itmap
% parameters of ImageTransfer:
% imagePath, xPosition, yPosition, width, height, trackerXPosition, trackerYPosition, xferoptions
transferStatus = Eyelink(&amp;#39;ImageTransfer&amp;#39;,transferimginfo.Filename,...
0, 0, transferimginfo.Width, transferimginfo.Height, ...
width/2-transferimginfo.Width/2 ,height/2-transferimginfo.Height/2, 1);
if transferStatus ~= 0
fprintf(&amp;#39;*****Image transfer Failed*****-------\n&amp;#39;);
end
case &amp;#39;StartRecording&amp;#39;
Eyelink(&amp;#39;StartRecording&amp;#39;);
case &amp;#39;Message&amp;#39;
if nargin == 2
Eyelink(&amp;#39;Message&amp;#39;, varargin{2});
elseif nargin == 3
Eyelink(&amp;#39;Message&amp;#39;, varargin{2}, varargin{3});
elseif nargin == 4
Eyelink(&amp;#39;Message&amp;#39;, varargin{2}, varargin{3}, varargin{4});
end
case &amp;#39;StopRecording&amp;#39;
Eyelink(&amp;#39;StopRecording&amp;#39;);
case &amp;#39;CloseFile&amp;#39;
Eyelink(&amp;#39;CloseFile&amp;#39;);
case &amp;#39;ReceiveFile&amp;#39;
Eyelink(&amp;#39;ReceiveFile&amp;#39;);
case &amp;#39;EyeAvailable&amp;#39;
status = Eyelink(&amp;#39;EyeAvailable&amp;#39;);
end
end
function do_nothing(varargin)
% do nothing with arguments
end
end
&lt;/code>&lt;/pre>
&lt;/div>
&lt;div id="extra-resourcess" class="section level1">
&lt;h1>Extra Resourcess&lt;/h1>
&lt;p>For examples of these methods in action, check out &lt;a href="https://github.com/psadil/VTF">an experiment on Voxel Tuning Functions&lt;/a>. In particular, see &lt;a href="https://github.com/psadil/VTF/blob/master/lib/PsychSetup/setupEyeTracker.m">setupEyeTracker&lt;/a>, &lt;a href="https://github.com/psadil/VTF/blob/master/lib/PsychEyelink/makeEyelinkFcn.m">makeEyelinkFcn&lt;/a>. That repository also has examples of using the value returned by &lt;code>makeEyelinkFcn&lt;/code> in &lt;a href="https://github.com/psadil/VTF/blob/master/lib/PsychTasks/runContrast.m">runContrast&lt;/a>. Note that the repository may change from time to time and might not match the code in this post exactly. To download the exact files defined above, see &lt;a href="https://psadil.github.io/psadil/files/matlab/setupEyeTracker.m">setupEyeTracker&lt;/a>, &lt;a href="https://psadil.github.io/psadil/files/matlab/makeEyelinkFcn.m">makeEyelinkFcn&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://link.springer.com/article/10.3758/BF03195489">Here’s the original publication&lt;/a> that introduced the Eyelink interface to PTB.&lt;/p>
&lt;p>Also, for inspiration about the cool experiments that can be run with Eyelink’s software, see the &lt;a href="https://github.com/kleinerm/Psychtoolbox-3/tree/master/Psychtoolbox/PsychHardware/EyelinkToolbox/EyelinkDemos">PTB Demos&lt;/a>. See a list of &lt;code>Eyelink&lt;/code> functions &lt;a href="http://psychtoolbox.org/docs/EyelinkToolbox">here&lt;/a>. You’ll need to look at this page if you want access to the help files for these commands on a computer without Eyelink installed.&lt;/p>
&lt;p>Finally, thanks to &lt;a href="https://www.umass.edu/pbs/people/ramiro-reyes">Ramiro&lt;/a> for sharing a PTB script that got me started with Eyelink.&lt;/p>
&lt;/div>
&lt;div class="footnotes">
&lt;hr />
&lt;ol>
&lt;li id="fn1">&lt;p>though, I’ve already broken some of the logic I outline in that section by having more than one function with a switch statement.&lt;a href="#fnref1" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn2">&lt;p>The options relating to saving data are for another post. It seems like you can do quite a lot with the Eyelink Data Viewer when various event tags have been set up properly (see &lt;a href="https://umass.box.com/s/n8ki3br7watw2niuangxxflj6ulpk67q">manual, on box&lt;/a> ), but my needs are so simple that I haven’t bothered digging too deeply.&lt;a href="#fnref2" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn3">&lt;p>Of course, a similar effect could be achieved by littering the experimental code with a bunch of &lt;code>if then else&lt;/code> statements. However, this method has the advantage of massively reducing the number of switch statements in the code. Fewer switch statements can be easier to follow and modify, because most of the effect of the &lt;code>input.tracker&lt;/code> variable can be localized to a single function (the definition of &lt;code>makeEyelinkFcn&lt;/code>)&lt;a href="#fnref3" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>A hierarchical Bayesian model for inferring neural tuning functions from voxel tuning functions</title><link>https://psadil.github.io/psadil/publication/sadil-2018-hierarchical/</link><pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate><guid>https://psadil.github.io/psadil/publication/sadil-2018-hierarchical/</guid><description/></item><item><title>A computational model of perceptual and mnemonic deficits in medial temporal lobe amnesia</title><link>https://psadil.github.io/psadil/publication/sadil-2017-computational/</link><pubDate>Thu, 27 Apr 2017 00:00:00 +0000</pubDate><guid>https://psadil.github.io/psadil/publication/sadil-2017-computational/</guid><description/></item><item><title>Hippocampal engagement during recall depends on memory content</title><link>https://psadil.github.io/psadil/publication/ross-2017-hippocampal/</link><pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate><guid>https://psadil.github.io/psadil/publication/ross-2017-hippocampal/</guid><description/></item></channel></rss>