[{"authors":["psadil"],"categories":null,"content":"I am a post-doctoral researcher in Biostatistics. During graduate school, I studied the neural and cognitive mechanisms of visual perception and memory in the labs of Drs. Rosemary Cowell and David Huber. My research involved adapting a neural network for modeling cognition, investigating associative learning and recollection for low-level visual information, looking at sequential effects in perceptual decisions, and developing new ways to apply Bayesian inference to behavioral or neuroimaging data.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1653490997,"objectID":"2e8993328fb03f26f17be4e2191be102","permalink":"https://psadil.github.io/psadil/author/patrick-sadil/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/psadil/author/patrick-sadil/","section":"authors","summary":"I am a post-doctoral researcher in Biostatistics. During graduate school, I studied the neural and cognitive mechanisms of visual perception and memory in the labs of Drs. Rosemary Cowell and David Huber. My research involved adapting a neural network for modeling cognition, investigating associative learning and recollection for low-level visual information, looking at sequential effects in perceptual decisions, and developing new ways to apply Bayesian inference to behavioral or neuroimaging data.","tags":null,"title":"Patrick Sadil","type":"authors"},{"authors":[],"categories":["gist"],"content":"How to store a NifTi as a TFRecord Patrick Sadil 2022-04-23\nA recent project required sending brain images to TensorFlow. Unfortunately, the data exceeded memory and so during training would need to be read from the disk. Poking around the TF documentation, it seems like a recommended way to do this is to store the images as a TFRecord. The steps for doing that are collected in this gist1.\nThere are five main steps\n load the images as numpy array, perform an preprocessing (e.g., resizing, masking), serialize the preprocessed images and store with their labels (all in memory) as a tf.train.Example, store (on disk) the examples as TFRecord, and finally create a tf.Data.Dataset pipeline that serves the examples to the model.  import math from numbers import Number import numpy as np import tensorflow as tf import nibabel as nb from nilearn import plotting from typing import List import numpy.typing as npt  Preprocess A full implementation might involve several preprocessing steps (e.g., registration, masking, cropping), but in this gist we\u0026rsquo;ll just rescale the images to have intensity values that range from 0-1.\ndef to_numpy(img: nb.Nifti1Image) -\u0026gt; np.ndarray: return np.asanyarray(img.dataobj) def load_and_preprocess(img: str) -\u0026gt; np.ndarray: # convert to numpy array and preprocess nii = to_numpy(nb.load(img)) # rescale to 0-1 preprocessed = (nii - nii.min())/(nii.max() - nii.min()).astype(np.float32) return preprocessed  Since this method of storing data was new for me, I wanted to ensure that I didn\u0026rsquo;t mess up the data. For that, I mainly relied on a basic plot of the images.\ndef plot_array(img: np.ndarray) -\u0026gt; None: nii = nb.Nifti1Image(img, affine=np.eye(4)*2) plotting.plot_anat(nii)  This first step is standard neuroimaging, but just to check that the functions are working preprocess an example brain and see how it looks.\n# example images packaged with fsl, found at $FSLDIR/data/standard img = ['MNI152lin_T1_2mm.nii.gz'] preprocesed = load_and_preprocess(img[0]) plot_array(preprocesed)  Great! That looks like a brain. I\u0026rsquo;ll use it as a reference to ensure that the roundtrip processing, serializing and unserializing, returns the arrays we need.\nSerialize Here\u0026rsquo;s where TensorFlow starts. As those that came before have always done, we\u0026rsquo;ll rely on these incantations.\ndef _bytes_feature(value: bytes): \u0026quot;\u0026quot;\u0026quot; Returns a bytes_list from a string / byte. Example: _bytes_feature(b'\\x00') _bytes_feature(b'a') \u0026quot;\u0026quot;\u0026quot; return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value])) def _float_feature(value: Number): \u0026quot;\u0026quot;\u0026quot; Returns a float_list from a float / double. Example: _float_feature(2) _float_feature(2.) \u0026quot;\u0026quot;\u0026quot; return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))  Those functions seem to help ensure that the serialized data ends up with appropriate types. But the full steps involve serializing both the image and the label together.\ndef serialize_example(img: str, label: Number): # convert to numpy array and preprocess preprocessed = load_and_preprocess(img) # Notice how the numpy array is converted into a byte_string. That's serialization in memory! feature = { 'label': _float_feature(label), 'image_raw': _bytes_feature(tf.io.serialize_tensor(preprocessed).numpy()) } return tf.train.Example(features=tf.train.Features(feature=feature))  To get the image back out, we need two more functions: one to unpack/parse the example (decode_example) and another to unserialize/parse (parse_1_example) the unpacked example.\ndef decode_example(record_bytes) -\u0026gt; dict: example = tf.io.parse_example( record_bytes, features = { \u0026quot;label\u0026quot;: tf.io.FixedLenFeature([], dtype=tf.float32), 'image_raw': tf.io.FixedLenFeature([], dtype=tf.string) } ) return example def parse_1_example(example): \u0026quot;\u0026quot;\u0026quot; Note that the network I was using worked with 3D images, and so the batches of data were of shape `(batch_size, x_dim, y_dim, z_dim, 1)`, rather than what is typical for 2d images: `(batch_size, x_dim, y_dim, n_channels)`. \u0026quot;\u0026quot;\u0026quot; X = tf.io.parse_tensor(example['image_raw'], out_type=tf.float32) # the images output by tf.io.parse_tensor will have shape (x_dim, y_dim, z_dim), which is to say that they're missing the channels dimension. expand_dims is used to indicate channel (i.e., be explicit about grayscale) return tf.expand_dims(X, 3), example['label']  At this point, we have functions for preprocessing the images, serializing them2, and packing each of them along with their labels into a tf.train.Example. This is all for converting the data into a format that can then be more easily written to and read from the disk.\nWrite serialized examples as TFRecords So far, everything has been in memory. Next comes a function that performs the above steps sequentially on several examples, and along the way writes the examples as a TFRecord.3\ndef write_records(niis: List[str], labels: npt.ArrayLike, n_per_record: int, outfile: str) -\u0026gt; None: \u0026quot;\u0026quot;\u0026quot; store list of niftis (and associated label) into tfrecords for use as dataset Args: niis: files that will be preprocessed and stored in record labels: true label associated with each element in niis. these are the \u0026quot;y\u0026quot; n_per_record: number of examples to store in each record. TF documentation advises that the files are +100Mb. Around 400 images cropped images at 2mm resolution seems to work. outfile: base prefix to use when writing the records Returns: Nothing, but the records will be written to disk. \u0026quot;\u0026quot;\u0026quot; n_niis = len(niis) n_records = math.ceil(len(niis) / n_per_record) for i, shard in enumerate(range(0, n_niis, n_per_record)): print(f\u0026quot;writing record {i} of {n_records-1}\u0026quot;) with tf.io.TFRecordWriter( f\u0026quot;{outfile}_{i:0\u0026gt;3}-of-{n_records-1:0\u0026gt;3}.tfrecords\u0026quot;, options= tf.io.TFRecordOptions(compression_type=\u0026quot;GZIP\u0026quot;) ) as writer: for nii, label in zip(niis[shard:shard+n_per_record], labels[shard:shard+n_per_record]): example = serialize_example(img=nii, label=label) writer.write(example.SerializeToString())  Create Dataset For this gist, let\u0026rsquo;s store several copies of the MNI 2mm brain.\n# (e.g., put nifti of label MNI152_T1_1mm_brain.nii.gz in the working directory) n_imgs = 3 mni_nii = ['MNI152lin_T1_2mm.nii.gz'] * n_imgs # store examples in each tfrecord. number of examples per record is configurable. # aim for as many examples as produces files of size \u0026gt; 100M prefix = \u0026quot;tmp\u0026quot; write_records(mni_nii, np.arange(n_imgs), n_imgs, prefix)  writing record 0 of 0 2022-05-13 15:00:56.962382: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: SSE4.1 SSE4.2 AVX AVX2 FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.  Calling the above will write a TFRecord file to disk. To read that record, define a pipeline that will create a tf.Data.dataset.\ndef get_batched_dataset(files, batch_size: int = 32) -\u0026gt; tf.data.Dataset: # Note: an actual pipeline would probably include at least the shuffle and prefetch methods dataset = ( tf.data.Dataset.list_files(files) # note shuffling is on by default, changes order when there are multiple records .flat_map(lambda x: tf.data.TFRecordDataset(x, compression_type=\u0026quot;GZIP\u0026quot;)) .map(decode_example) .map(parse_1_example) .batch(batch_size) ) return dataset  Now, use that function to read the records back.\n# a full dataset will have a list with many records list_of_records=[f'{prefix}*.tfrecords'] ds = get_batched_dataset(list_of_records, batch_size=2)  That dataset, ds, was our goal in the gist. It can be passed to methods of the tf.keras.Model, including [tf.keras.Model.fit()(https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit)], serving up more data than can fit in memory.\nBut first, the serialization is a lot, so it is a good idea to verify that the images look okay when loaded. To do so, pluck a single example from the dataset.\n(Xs, Ys) = next(ds.as_numpy_iterator())  The dimensions of the labels are relatively easy. It\u0026rsquo;s a 1d array with as many elements as are in the batch.\n# (batch_size, ) Ys.shape  (2,)  The order will depend on what shuffling is embedded in the dataset pipeline. In this case, there was no shuffling and so we should expect that the order is preserved.\nYs  array([0., 1.], dtype=float32)  The data, Xs, also has a predictable shape.\n# (batch_size, x_dim, y_dim, z_dim, 1) Xs.shape  (2, 91, 109, 91, 1)  Take that first element in the batch and plot.\nparsed_img = np.squeeze(Xs[0,]) plot_array(parsed_img)  That looks great! Just in case, let\u0026rsquo;s check more explicitly\nnp.array_equal(preprocesed, parsed_img)  True  Yay! done\n  Inspiration for writing came from responding to this question on NeuroStars, and also from an urge to try a python-based post.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n This serializing stuff is spooky black magic. I\u0026rsquo;m going to skip over those details and instead leave this reference, a journey through serializing in R https://blog.djnavarro.net/posts/2021-11-15_serialisation-with-rds/.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n It seems to me that this storage on disk involves a second serialization step (where the first was done by tf.io.serialize_tensor in serialize_example). Even so, I assume the roundtrip isn\u0026rsquo;t so much of a big deal, considering that the data can be served to the model in parallel.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","date":1650672e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653490997,"objectID":"0303d0fb97f703386e177becbd2c43f6","permalink":"https://psadil.github.io/psadil/post/tf-dataset-from-3d-nifti/","publishdate":"2022-04-23T00:00:00Z","relpermalink":"/psadil/post/tf-dataset-from-3d-nifti/","section":"post","summary":"How to store a NifTi as a TFRecord Patrick Sadil 2022-04-23\nA recent project required sending brain images to TensorFlow. Unfortunately, the data exceeded memory and so during training would need to be read from the disk. Poking around the TF documentation, it seems like a recommended way to do this is to store the images as a TFRecord. The steps for doing that are collected in this gist1.\nThere are five main steps","tags":["python","mri"],"title":"How to store a NifTi as a TFRecord","type":"post"},{"authors":[],"categories":["research"],"content":"  knitr::opts_chunk$set(echo = TRUE) set.seed(1234) library(dplyr) library(ggplot2) library(tidyr) I’m working on a project involving serial dependence. The project involves disentangling a dependence on the previous orientation from a dependence on the previous response. Unfortunately, there is a common way for a dependence on the previous response to be spurious, due to the oblique effect1. The first reference I’ve seen for this is a master’s thesis by Fritsche (2016). I didn’t follow that explanation, and so I’m using this post to explain how the oblique effect causes a spurious dependence on the previous response.\nFirst, let’s show that the confound is real. Data will be generated with an oblique effect, and there will be no dependence between trials – neither on the previous orientation nor the previous response. There will be no response variability, meaning that errors will only be caused by the oblique effect. Since the data are simulated without dependencies, any dependence that emerges will necessarily be spurious.\n# helper functions for converting between angles and degrees rad \u0026lt;- function(degree) degree * pi / 180 deg \u0026lt;- function(radian) radian * 180 / pi # magnitude of oblique effect oblique \u0026lt;- rad(-22.5) d0 \u0026lt;- tibble( orientation = runif(5000, 0, pi)) %\u0026gt;% mutate( trial = 1:n(), oblique = oblique*sin(orientation*4), response = rnorm(n(), orientation, 0) + oblique)  To help generate the data, define a helper functions that calculates the signed, shortest angle between two angles (measured in radians).\n#\u0026#39; @param deg1 numeric degree #\u0026#39; @param deg2 numeric degree #\u0026#39; #\u0026#39; @return #\u0026#39; signed difference between inputs, wrapped to +-pi #\u0026#39; output is shortest distance to the first input #\u0026#39; #\u0026#39; @examples #\u0026#39; # pi/2 is 45 degrees clockwise from pi, so the output is pi/2 #\u0026#39; ang_diff(pi/2, pi) #\u0026#39; # pi is 45 degrees counterclockwise from pi/2, so the output is -pi/2 #\u0026#39; ang_diff(pi, pi/2) #\u0026#39; # notice the discontinuity when the shortest angle switches direction #\u0026#39; ang_diff(pi/2 - .01, 0) #\u0026#39; ang_diff(pi/2 + .01, 0) ang_diff \u0026lt;- function(deg1, deg2){ stopifnot(length(deg1) == length(deg2)) diff \u0026lt;- ( deg1 - deg2 + pi/2 ) %% pi - pi/2 out \u0026lt;- dplyr::if_else(diff \u0026lt; -pi/2, diff + pi, diff) return(out) } Then use the function ang_diff to calculate errors, and to calculate the relative orientation difference between the current trial and either the previous orientation or the previous response.\nd \u0026lt;- d0 %\u0026gt;% mutate( prev_response = lag(response), prev_orientation = lag(orientation), error = ang_diff(orientation, response), orientation_diff = ang_diff(orientation, prev_orientation), response_diff = ang_diff(orientation, prev_response)) %\u0026gt;% filter(trial \u0026gt; 1) %\u0026gt;% mutate(across(where(is.double), deg)) Plot errors as a function of the current orientation to confirm that there is an oblique effect.\nd %\u0026gt;% ggplot(aes(x=orientation, y=error)) + geom_point() + geom_smooth( formula = y ~ sin(rad(x)*4), method = \u0026quot;lm\u0026quot;, se = FALSE) + scale_y_continuous( breaks = c(-20, -10, 0, 10, 20), labels = c(\u0026quot;CCW\u0026quot;, -10, 0, 10, \u0026quot;CW\u0026quot;)) + scale_x_continuous( name = \u0026quot;orientation on current trial\u0026quot;, labels = c(0, 90, 180), breaks = c(0, 90, 180))  Figure 1: The simulated data exhibit a clear oblique effect.  Is there a dependence on either the previous response or previous orientation?\nd %\u0026gt;% pivot_longer( cols=c(orientation_diff, response_diff), names_to = \u0026quot;covariate\u0026quot;, names_pattern = \u0026quot;(orientation|response)\u0026quot;, values_to = \u0026quot;x\u0026quot;) %\u0026gt;% ggplot(aes(x=x, y=error)) + geom_point() + facet_wrap(~covariate) + geom_smooth( method = \u0026quot;gam\u0026quot;, formula = y ~ s(x, bs = \u0026quot;cc\u0026quot;, k=9)) + scale_x_continuous( name = \u0026quot;relative orientation/response on previous trial\u0026quot;, labels = c(-90, 0, 90), breaks = c(-90, 0, 90))  Figure 2: Errors do not depend on the previous orientation, but they do depend on the previous response. Since the data were generated without that dependence, it must be spurious.  What’s going on? There are two key factors: first, the oblique effect operates on the previous trial to make some previous responses more likely than others, and second the oblique effect operates on the current trial to make certain previous responses more likely to have errors in a consistent direction. To be precise, I’ll use the following terminology.\nTerminology Trials will be indexed by natural numbers. The “current trial” will be referred to as trial \\(n\\), and the “previous trial” as trial \\(n-1\\). The orientation and responses on each trial will be thought of as sequences2. The variable \\(O_n\\) means the orientation on trial \\(n\\) (i.e., the current trial), whereas the variable \\(O_{n-1}\\) means the orientation on trial \\(n-1\\) (i.e., the previous trial). Similarly, the variable \\(R_n\\) means the response on trial \\(n\\), whereas the variable \\(R_{n-1}\\) means the response on trial \\(n-1\\).\nAll angles (e.g., \\(O_n\\) and \\(R_n\\)) use the convention that \\(0^\\circ\\) is horizontal, \\(45^\\circ\\) is one quarter rotation counterclockwise from horizontal (e.g., at 1:30 on a clock), \\(90^\\circ\\) is vertical, etc. However, differences between angles are reported such that a positive value implies a clockwise shift (i.e., moving forward on the clock) and a negative value implies a counterclockwise shift. For example, an error of \\(10^\\circ\\) means that \\(R_n\\) is \\(10^\\circ\\) clockwise from \\(O_n\\). This means that we can determine an “attraction” effect based on whether the sign of the error on trial \\(n\\) matches the sign of the difference between \\(O_n\\) and either \\(O_{n-1}\\) or \\(R_{n-1}\\). Conversely, a “repulsive” effect is when the error and differences have mismatched signs.\n Explanation First, consider a specific sequence of trials that could produce a spurious effect. To help with the explanation, the trials are colored based on the current trial.\nd %\u0026gt;% pivot_longer( cols=c(orientation_diff, response_diff), names_to = \u0026quot;covariate\u0026quot;, names_pattern = \u0026quot;(orientation|response)\u0026quot;, values_to = \u0026quot;x\u0026quot;) %\u0026gt;% ggplot(aes(x=x, y=error)) + geom_point(aes(color=oblique)) + scale_color_gradient2(low = scales::muted(\u0026quot;blue\u0026quot;), high = scales::muted(\u0026quot;red\u0026quot;)) + facet_wrap(~covariate) + geom_smooth( method = \u0026quot;gam\u0026quot;, formula = y ~ s(x, bs = \u0026quot;cc\u0026quot;, k=9)) + scale_y_continuous( breaks = c(-20, -10, 0, 10, 20), labels = c(\u0026quot;CCW\u0026quot;, -10, 0, 10, \u0026quot;CW\u0026quot;)) + scale_x_continuous( name = \u0026quot;relative orientation/response on previous trial\u0026quot;, labels = c(-90, 0, 90), breaks = c(-90, 0, 90))  Figure 3: Same figure as above, but colored based on the magnitude and direction of the oblique effect.  When \\(O_{n-1}\\) is \\(0^\\circ\\), the oblique effect will have not caused an error. So, for \\(R_{n-1}\\) to be \\(22.5^\\circ\\) clockwise to \\(O_n\\), then \\(O_n\\) could be, itself \\(22.5^\\circ\\). But when \\(O_n\\) is \\(22.5^\\circ\\), the oblique effect will cause an error; \\(R_n\\) will be a clockwise error, in the same direction as \\(R_{n-1}\\). Since \\(R_n\\) exhibits an error in the direction of \\(R_{n-1}\\), it will look like \\(R_{n-1}\\) caused an attraction.\nMore importantly, when the oblique effect acts on trial \\(n-1\\), it will cause responses to collect along the cardinal axes. That is, regardless of the orientation on trial \\(n-1\\), \\(R_{n-1}\\) will be close to either \\(0^\\circ\\) or \\(90^\\circ\\). This means that, whenever \\(O_n\\) is close to \\(22.5^\\circ\\), the oblique effect’s influence on the previous response, \\(R_{n-1}\\), makes it more likely that \\(R_{n-1}\\) will be approximately \\(22.5^\\circ\\) clockwise from \\(O_n\\), and then the oblique effect on trial \\(n\\) will further push the response toward \\(R_{n-1}\\).\nWe can see this play out empirically by looking at \\(O_n\\) as a function of \\(R_{n-1}\\); when \\(R_{n-1}\\) is close to \\(22.5^\\circ\\), there is an over-representation of orientations for which the oblique effect will bias responses toward the previous response.\nd %\u0026gt;% filter(between(response_diff, 21, 24)) %\u0026gt;% ggplot(aes(x=orientation)) + geom_histogram(bins=30) + scale_x_continuous( name = \u0026quot;orientation on current trial\u0026quot;, labels = c(0, 90, 180), breaks = c(0, 90, 180)) + geom_vline(xintercept = c(22.5, 112.5), color=\u0026quot;blue\u0026quot;)   Figure 4: When the previous orientation is 22.5 clockwise, the current orientation tends to be either 22.5 or 112.5, which is when the oblique effect causes maximal error.  We can think about this from the other direction, too; when \\(R_{n-1}\\) is \\(22.5^\\circ\\) clockwise from \\(O_n\\), it’s relatively difficult for \\(O_n\\) to be around \\(67.5^\\circ\\). For example, when \\(O_n=67.5\\), the previous response could be \\(22.5^\\circ\\) if \\(O_n=45^\\circ\\), but nearly no other orientation would work; when \\(O_n\\) is near but not exactly \\(45^\\circ\\), the oblique effect on trial \\(n-1\\) will push \\(R_{n-1}\\) away from \\(45^\\circ\\), away from a response that could be \\(22.5^\\circ\\) clockwise to \\(O_n\\). This is important because, if it is rare for trial \\(n\\) to have both \\(O_n=67.5\\) and \\(R_{n-1}\\) be \\(22.5^\\circ\\) clockwise from \\(O_n\\), then the oblique effect will be imbalanced.\nTogether, this means that when the oblique effect on trial \\(n\\) causes a maximal clockwise error, the oblique effect on trial \\(n-1\\) makes it more likely that the previous response is also clockwise and less likely that it’s counterclockwise. The result is a spurious dependence on the previous response.\nWe can see this play out more generally by looking at the current orientation as a function of the previous orientations and responses.\nd %\u0026gt;% pivot_longer( cols = c(orientation_diff, response_diff), names_to = \u0026quot;covariate\u0026quot;, names_pattern = \u0026quot;(response|orientation)\u0026quot;) %\u0026gt;% ggplot(aes(x=orientation, y=value)) + facet_wrap(~covariate) + geom_point() + coord_fixed() + scale_y_continuous( name = \u0026quot;relative orientation/response on previous trial\u0026quot;, labels = c(-90, 0, 90), breaks = c(-90, 0, 90)) + scale_x_continuous( name = \u0026quot;orientation on current trial\u0026quot;, labels = c(0, 90, 180), breaks = c(0, 90, 180))  Figure 5: The current orientation is unrelated to the previous orientation, but there is a dependency on the previous response.  As expected, there is no relationship between \\(O_n\\) and \\(O_{n-1}\\), but there is a strong relationship between \\(O_n\\) and \\(R_{n-1}\\). When \\(O_n \\in (0,45)\\), then it’s likely that \\(R_{n-1} \\in (0,22.5)\\) (clockwise), or \\(R_{n-1} \\in (-90, -67.5)\\) (counterclockwise). The figure below shows the same data, but now the data are colored according to how the oblique effect will cause errors on trial \\(n\\).\nd %\u0026gt;% na.omit() %\u0026gt;% select(-response, -error) %\u0026gt;% pivot_longer( cols = c(orientation_diff, response_diff), names_to = \u0026quot;covariate\u0026quot;, names_pattern = \u0026quot;(response|orientation)\u0026quot;) %\u0026gt;% ggplot(aes(x=orientation, y=value)) + facet_wrap(~covariate) + geom_point(aes(color=oblique)) + scale_color_gradient2(low = scales::muted(\u0026quot;blue\u0026quot;), high = scales::muted(\u0026quot;red\u0026quot;)) + coord_fixed() + scale_y_continuous( name = \u0026quot;relative orientation/response on previous trial\u0026quot;, labels = c(-90, 0, 90), breaks = c(-90, 0, 90)) + scale_x_continuous( name = \u0026quot;orientation on current trial\u0026quot;, labels = c(0, 90, 180), breaks = c(0, 90, 180))  Figure 6: This is the same plot as above, but now the data have been colored according to how the oblique effect will cause errors. The point is that the errors caused by the oblique effect on trial \\(n\\) are balanced when looking at the orientation on the previous trial (consider a horizontal slice), but no such balancing happens when looking at the response on the previous trial.  Fortunately, this spurious bias isn’t too hard to adjust for3. But the point is that it would be a mistake to look at just a dependence on the previous orientation if there is an oblique effect; analyses must adjust for the oblique effect.\nI’m not sure if there is a similar issue with other domains (e.g., when participants discriminate tones, pain, faces, etc). Perhaps edge effects could cause a similar issue (e.g., if people are more likely to respond to the ends or middle of the scale)?\nFritsche, Matthias. 2016. “To Smooth or Not to Smooth: Investigating the Role of Serial Dependence in Stabilising Visual Perception.” Master’s thesis, Radboud University.  Wei, Xue-Xin, and Alan A Stocker. 2015. “A Bayesian Observer Model Constrained by Efficient Coding Can Explain’anti-Bayesian’percepts.” Nature Neuroscience 18 (10): 1509.     This effect occurs when participants are asked to report orientations. Participants are differently accurate across the range of orientations; they are maximally accurate when reporting \\(0^\\circ\\), \\(45^\\circ\\), \\(90^\\circ\\), and \\(135^\\circ\\), but minimally accurate at intermediate orientations (\\(22.5^\\circ\\), \\(67.5^\\circ\\), etc). The errors can either be clockwise or counterclockwise, depending on the experiment. For an overview, see Wei and Stocker (2015).↩︎\n This won’t be used, but sequence of orientations could be written \\((O_n)_{n\\in\\mathbb{N}}\\), and the sequence of responses \\((R_n)_{n\\in\\mathbb{N}}\\). Selecting a particular trial involves dropping the parentheses; \\((O_n)_{n\\in\\mathbb{N}}\\) emphasizes the whole sequence, whereas \\(O_n\\) means take a particular (but arbitrary) element of the sequence. I am not a mathematician, and this post is a quick and dirty explanation mostly meant for later me, so don’t expect formality.↩︎\n In a regression model of the errors, it would suffice to include a sinusoidal term.↩︎\n   ","date":1620345600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653490997,"objectID":"841a8cf03d79ab266544b9cda1c2b169","permalink":"https://psadil.github.io/psadil/post/2021-05-07-spurious-serial-dependencies/","publishdate":"2021-05-07T00:00:00Z","relpermalink":"/psadil/post/2021-05-07-spurious-serial-dependencies/","section":"post","summary":"knitr::opts_chunk$set(echo = TRUE) set.seed(1234) library(dplyr) library(ggplot2) library(tidyr) I’m working on a project involving serial dependence. The project involves disentangling a dependence on the previous orientation from a dependence on the previous response. Unfortunately, there is a common way for a dependence on the previous response to be spurious, due to the oblique effect1. The first reference I’ve seen for this is a master’s thesis by Fritsche (2016). I didn’t follow that explanation, and so I’m using this post to explain how the oblique effect causes a spurious dependence on the previous response.","tags":["psychology"],"title":"Spurious Serial Dependencies","type":"post"},{"authors":["Patrick Sadil","David E Huber","Rosemary A Cowell"],"categories":[],"content":"","date":1614971696,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653490997,"objectID":"e86d8f813f1d2970547d8975bf8fbee2","permalink":"https://psadil.github.io/psadil/publication/sadil-2021-nmm/","publishdate":"2021-03-05T14:14:56-05:00","relpermalink":"/psadil/publication/sadil-2021-nmm/","section":"publication","summary":"Many cognitive neuroscience theories assume that changes in behavior arise from changes in the tuning properties of neurons (e.g., Dosher \u0026 Lu 1998, Ling, Liu, \u0026 Carrasco 2009). However, direct tests of these theories with electrophysiology are rarely feasible with humans. Non-invasive functional magnetic resonance imaging (fMRI) produces voxel tuning, but each voxel aggregates hundreds of thousands of neurons, and voxel tuning modulation is a complex mixture of the underlying neural responses. We developed a pair of statistical tools to address this problem, which we refer to as NeuroModulation Modeling (NMM). NMM advances fMRI analysis methods, inferring the response of neural subpopulations by leveraging modulations at the voxel-level to differentiate between different forms of neuromodulation. One tool uses hierarchical Bayesian modeling and model comparison while the other tool uses a non-parametric slope analysis. We tested the validity of NMM by applying it to fMRI data collected from participants viewing orientation stimuli at high- and low-contrast, which is known from electrophysiology to cause multiplicative scaling of neural tuning (e.g., Sclar \u0026 Freeman 1982). In seeming contradiction to ground truth, increasing contrast appeared to cause an additive shift in orientation tuning of voxel-level fMRI data. However, NMM indicated multiplicative gain rather than an additive shift, in line with single-cell electrophysiology. Beyond orientation, this approach could be applied to determine the form of neuromodulation in any fMRI experiment, provided that the experiment tests multiple points along a stimulus dimension to which neurons are tuned (e.g., direction of motion, isoluminant hue, pitch, etc.).","tags":[],"title":"NeuroModulation Modeling (NMM): Inferring the form of neuromodulation from fMRI tuning functions","type":"publication"},{"authors":["Patrick Sadil","Rosemary A Cowell","David E Huber"],"categories":[],"content":"","date":1610998530,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653490997,"objectID":"3fd72167fcd16d0ac3d8b24e0fe49094","permalink":"https://psadil.github.io/psadil/publication/sadil-2021-serialdependence/","publishdate":"2021-01-18T14:35:30-05:00","relpermalink":"/psadil/publication/sadil-2021-serialdependence/","section":"publication","summary":"Visual perceptual decisions can be altered by recent experience. In the “serial dependence” effect, participants’ responses to visual stimuli appear to be biased toward (i.e., attracted to) recently encountered stimuli. Fischer and Whitney (2014) proposed that serial dependence reflects a “continuity field” that promotes visual stability by biasing perception toward the recent past. However, when participants are relatively accurate on the prior trial, there is no discernible difference between attraction to the prior stimulus and attraction to the prior response. To tease apart these alternative explanations of the attraction effect, we developed two complementary analysis techniques that rely on participants’ naturally occurring errors on a trial-by-trial basis, identifying any effect of the prior stimulus and, separately, any effect of the prior response (i.e., each effect could be attractive, repulsive, or absent). Applying these techniques to serial dependence data from a new experiment and four previously published studies, including Fischer and Whitney’s, we found that serial dependencies reflect an attraction to the previous response and repulsion from the previous stimulus, with these effects cancelling each other to different degrees for different experiments. In no case did we find evidence of an attraction to the prior stimulus. These results are consistent with literatures that predate the serial dependence effect: Attraction to prior responses is routinely observed in a wide variety of paradigms and repulsion from prior stimuli is ubiquitous, such as in the tilt aftereffect.","tags":[],"title":"The Yin-yang of Serial Dependence Effects: Every Response is both an Attraction to the Prior Response and a Repulsion from the Prior Stimulus","type":"publication"},{"authors":[],"categories":[],"content":"  A friend recently moved away from Western Mass, so I wanted to send them a gift to help remind them of the area. I also happened to want to learn a bit more about Generative Adversarial Networks (GANs).\nGoals  Briefly overview GANs Link to fun code for acquiring data from Google Street View Share a neat set of photographs  Note that this isn’t an ‘intro to GANs’. If that’s what you’re after, keep browsing.\n Overview Computers have gotten very good at extracting information from images, particularly at identifying images’ contents. Such categorization is powerful, but it often requires access to labeled data – hundreds of thousands of pictures for which we can tell the computer: this is a cat, that is a dog, no that’s also a dog, yes that’s a dog but it’s also a husky. However, many applications remain where computer-aided categorization would be invaluable, but for which there isn’t sufficient labeled data. If an algorithm can learn to recognize the subtle features distinguishing 120 dog breeds, it could probably learn visual features that help radiologists locate potential anomalies. But the guess-and-check strategy, despite being sufficient for many advanced computer vision algorithms, flounders when it has access to only a few hundred training examples. Computers have the potential to do some very clever things, but there is not always enough data to supervise their training.\nTo mitigate a lack of data, one developing solution is a GAN. A common analogy for these networks envisions art forgery (e.g.), a forger and a critic collaborating to learn about an artist. The forger paints fake works in the style of van Gough, while the critic distinguishes the fake from the real van Goughs. For the forger to succeed, it must paint the essences of van Gough: the reductionist features like the strokes and the yellows, and the holistic feelings of urgency and presence. For the critic to succeed, it must identify those essences, learning the sharp boundaries between longing and yearning. As the forgeries improve, the critic becomes more discerning, further inspiring the forger. Although the networks are taught the essences – the labels – explicitly, the two together learn about van Gough. And they’ll learn without supervision.\nAfter learning, the critic can be deployed for standard categorization tasks (e.g., aiding medical diagnoses). But the training also produces another useful machine, a machine that is capable of generating images. Predictably, there are challenges to training a generator that is capable of producing good quality, large, and diverse images. But I didn’t need the images to be stellar, so long as their content was clear (to a human). A lack of photorealism – imperfect training – could make the pictures more interesting1. To make a gift, I wanted a forger that could paint New England.\n Setup I wanted the forger to generate images of New England, so I first needed a bunch of pictures of New England. I have photographed a few hundred pictures, but this wouldn’t be nearly enough2. Instead, I relied on a combination of Google’s Street View Static and Directions APIs. The Street View API gives a picture associated with a location, and those locations were provided by the Directions API. The repository for the network has the details, but the result was that I could input an origin and a destination – meandering through a few waypoints – and download whatever the Street View Car recorded when it traveled along those directions3. In the end, I collected ~25,000 images.\n25k may feel like a lot of images. But skimming online suggested that even 25k would not have been enough to adequately constrain the networks. GANs may not require labeled examples, but they are still data-hungry. Given my relatively small dataset, I picked an adversarial architecture that incorporates a few extra tricks to glean information from smaller datasets: stylegan2-ada4. To train the network, I used free credits on the Google’s cloud console.\n Curated Samples After one day of training5, the network started producing some useful images.\nThese six images are fake, produced from a collaboration to learn about New England.\n I chose these six – and the seventh at the top – because they illustrate a few fun features of what the GAN learned. For example, the GAN learned, very early, that pictures of New England always have, in the bottom corners, the word “Google”6. That machine learning can produce realistic text surprises me (e.g., if the face is weird, how are all of the pixels in place to spell out a word?!). I assume that text comes out clean because most lettering is tightly constrained. That is, when the forger paints something that could be categorized as lettering, the critic severely constrains those pixels; fuzzy letters betray forgery, and real photographs don’t have nonsense like UNS;QD*LKJ. So if the training images contain enough text that the generator starts producing letters, there is also enough text for the critic to learn what text is realistic.\nThe forger had difficulty with buildings. I downloaded mostly images of the highways connecting cities. This means that there were enough cityscapes for the GAN to generate buildings, but relative to a road, it was much slower at learning the intricacies of a building. Of course, the roads are imperfect, too (the telephone pole in the upper middle ripples, the upper left has too many roads, the colors of the painted lines mismatch, etc). But unlike, say, a bad photoshop, these errors have a kind of global coherence that, subjectively, allows the images to seem not fake but instead surreal.\n  If I wanted perfect pictures, I could have just used a camera.↩︎\n The van Gough example is slightly misleading; in practice, van Gough didn’t paint enough pictures to train a GAN. Training a GAN from scratch doesn’t require labeled data, but it still requires many images. There are tricks that could help a GAN, but simply training his images would likely be insufficient.↩︎\n Having not owned a car during graduate school, I found it funny that these networks learned about New England through its highways↩︎\n But also, they provided a helpful docker image, functions to prep the data, and decent documentation. This is a good reminder about the benefits of polishing a repository.↩︎\n After one day, the training error was still decreasing. But I was using a preemptible virtual machine, and so after 24 hours it was automatically shutdown.↩︎\n I removed the text from the curated examples, but it can be seen in the preview image at the top.↩︎\n   ","date":1610841600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653490997,"objectID":"fc512ca5dbd26fa2ff1baadb85da8b2c","permalink":"https://psadil.github.io/psadil/post/2021-01-02-gan-mass/","publishdate":"2021-01-17T00:00:00Z","relpermalink":"/psadil/post/2021-01-02-gan-mass/","section":"post","summary":"A friend recently moved away from Western Mass, so I wanted to send them a gift to help remind them of the area. I also happened to want to learn a bit more about Generative Adversarial Networks (GANs).\nGoals  Briefly overview GANs Link to fun code for acquiring data from Google Street View Share a neat set of photographs  Note that this isn’t an ‘intro to GANs’.","tags":["python","machine learning"],"title":"New England GAN","type":"post"},{"authors":[],"categories":["experiments"],"content":"     Many experiments require counterbalancing sequences of trials. For example, I’m currently running an experiment on serial dependence1. In my experiment, participants report the orientation of a grating2 stimulus on each trial. The serial dependence effect is how their responses on one trial depend on either the orientation of the previous trial or their response on that trial. To tease apart the effects of prior stimuli from prior responses, I’m manipulating the visual contrast of the gratings ( Michelson contrast ). There are three levels of contrast: high, low, and zero (at zero contrast, there is no grating stimulus). This experiment will only need a few of the eight possible pairs of contrasts, and I’d like a sequence of trials that does not have any filler trials. So I need a flexible way to generate sequences of contrast.\nIt turns out that this problem can be formulated as constructing an Eulerian, directed cycle. There are likely other ways 3, but I think this is a neat approach. I won’t talk much about why any of this works, primarily because I don’t feel qualified to do so. However, the post includes a script that implements the algorithm, and checks that it has worked. So, hopefully it’ll be useful to at least a future me. But before discussing an Eulerian circuit, let’s talk about formulating the stimulus conditions as a graph.\nTrials can be represented with a graph All potential sequences of trials will be represented as a graph. The graphs nodes will correspond to conditions, and edges between the nodes will correspond to allowable transitions. To represent these graphs, I’ll use the DiagrammeR package.\n# library(DiagrammeR) library(magrittr) # library(dplyr) In the graph of my experiment, there will be three nodes for each of the three conditions (Figure 1).\nnodes \u0026lt;- DiagrammeR::create_node_df( n = 3, label = c(\u0026quot;zero\u0026quot;,\u0026quot;low\u0026quot;,\u0026quot;high\u0026quot;)) DiagrammeR::create_graph(nodes_df = nodes) %\u0026gt;% DiagrammeR::render_graph(layout = \u0026quot;tree\u0026quot;)   {\"x\":{\"diagram\":\"digraph {\\n\\ngraph [layout = \\\"neato\\\",\\n outputorder = \\\"edgesfirst\\\",\\n bgcolor = \\\"white\\\"]\\n\\nnode [fontname = \\\"Helvetica\\\",\\n fontsize = \\\"10\\\",\\n shape = \\\"circle\\\",\\n fixedsize = \\\"true\\\",\\n width = \\\"0.5\\\",\\n style = \\\"filled\\\",\\n fillcolor = \\\"aliceblue\\\",\\n color = \\\"gray70\\\",\\n fontcolor = \\\"gray50\\\"]\\n\\nedge [fontname = \\\"Helvetica\\\",\\n fontsize = \\\"8\\\",\\n len = \\\"1.5\\\",\\n color = \\\"gray80\\\",\\n arrowsize = \\\"0.5\\\"]\\n\\n \\\"1\\\" [label = \\\"zero\\\", fillcolor = \\\"#F0F8FF\\\", fontcolor = \\\"#000000\\\", pos = \\\"0,1!\\\"] \\n \\\"2\\\" [label = \\\"low\\\", fillcolor = \\\"#F0F8FF\\\", fontcolor = \\\"#000000\\\", pos = \\\"1,1!\\\"] \\n \\\"3\\\" [label = \\\"high\\\", fillcolor = \\\"#F0F8FF\\\", fontcolor = \\\"#000000\\\", pos = \\\"2,1!\\\"] \\n}\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]} Figure 1: Node represent experimental conditions.  In my experiment, I want trials to go from low to high, zero to high, or high to high, high to low, and high to zero (Figure 2). Including only these five types of transitions means excluding a few of the possible edges that could be in the graph. For example, I do not want any zero contrast trials to follow any other zero contrast trials, nor do I want a low contrast trial to follow a zero contrast trial.\nedges \u0026lt;- DiagrammeR::create_edge_df( from = c(1,2,3,3,3), to = c(3,3,3,1,2)) DiagrammeR::create_graph( nodes_df = nodes, edges_df = edges) %\u0026gt;% DiagrammeR::render_graph( layout = \u0026quot;tree\u0026quot;)   {\"x\":{\"diagram\":\"digraph {\\n\\ngraph [layout = \\\"neato\\\",\\n outputorder = \\\"edgesfirst\\\",\\n bgcolor = \\\"white\\\"]\\n\\nnode [fontname = \\\"Helvetica\\\",\\n fontsize = \\\"10\\\",\\n shape = \\\"circle\\\",\\n fixedsize = \\\"true\\\",\\n width = \\\"0.5\\\",\\n style = \\\"filled\\\",\\n fillcolor = \\\"aliceblue\\\",\\n color = \\\"gray70\\\",\\n fontcolor = \\\"gray50\\\"]\\n\\nedge [fontname = \\\"Helvetica\\\",\\n fontsize = \\\"8\\\",\\n len = \\\"1.5\\\",\\n color = \\\"gray80\\\",\\n arrowsize = \\\"0.5\\\"]\\n\\n \\\"1\\\" [label = \\\"zero\\\", fillcolor = \\\"#F0F8FF\\\", fontcolor = \\\"#000000\\\", pos = \\\"0,2!\\\"] \\n \\\"2\\\" [label = \\\"low\\\", fillcolor = \\\"#F0F8FF\\\", fontcolor = \\\"#000000\\\", pos = \\\"1,2!\\\"] \\n \\\"3\\\" [label = \\\"high\\\", fillcolor = \\\"#F0F8FF\\\", fontcolor = \\\"#000000\\\", pos = \\\"0.5,1!\\\"] \\n \\\"1\\\"-\\\"3\\\" \\n \\\"2\\\"-\\\"3\\\" \\n \\\"3\\\"-\\\"3\\\" \\n \\\"3\\\"-\\\"1\\\" \\n \\\"3\\\"-\\\"2\\\" \\n}\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]} Figure 2: Directed edges between nodes represent allowable transitions.  Constructing a sequence of trials will correspond to walking along the edges, from node to node. That walk will be Eulerian if each edge is be visited exactly once. With so few edges, it’s easy enough to visualize an Eulerian walk through the edges. One possible Eulerian walk (a cycle4, even) is shown in Figure 3.\nedges_labelled \u0026lt;- DiagrammeR::create_edge_df( from = c(3,2,3,3,1), to = c(2,3,3,1,3), label = as.character(1:5)) DiagrammeR::create_graph( nodes_df = nodes, edges_df = edges_labelled) %\u0026gt;% DiagrammeR::render_graph( layout = \u0026quot;tree\u0026quot;)   {\"x\":{\"diagram\":\"digraph {\\n\\ngraph [layout = \\\"neato\\\",\\n outputorder = \\\"edgesfirst\\\",\\n bgcolor = \\\"white\\\"]\\n\\nnode [fontname = \\\"Helvetica\\\",\\n fontsize = \\\"10\\\",\\n shape = \\\"circle\\\",\\n fixedsize = \\\"true\\\",\\n width = \\\"0.5\\\",\\n style = \\\"filled\\\",\\n fillcolor = \\\"aliceblue\\\",\\n color = \\\"gray70\\\",\\n fontcolor = \\\"gray50\\\"]\\n\\nedge [fontname = \\\"Helvetica\\\",\\n fontsize = \\\"8\\\",\\n len = \\\"1.5\\\",\\n color = \\\"gray80\\\",\\n arrowsize = \\\"0.5\\\"]\\n\\n \\\"1\\\" [label = \\\"zero\\\", fillcolor = \\\"#F0F8FF\\\", fontcolor = \\\"#000000\\\", pos = \\\"0,2!\\\"] \\n \\\"2\\\" [label = \\\"low\\\", fillcolor = \\\"#F0F8FF\\\", fontcolor = \\\"#000000\\\", pos = \\\"1,2!\\\"] \\n \\\"3\\\" [label = \\\"high\\\", fillcolor = \\\"#F0F8FF\\\", fontcolor = \\\"#000000\\\", pos = \\\"0.5,1!\\\"] \\n\\\"3\\\"-\\\"2\\\" [label = \\\"1\\\"] \\n\\\"2\\\"-\\\"3\\\" [label = \\\"2\\\"] \\n\\\"3\\\"-\\\"3\\\" [label = \\\"3\\\"] \\n\\\"3\\\"-\\\"1\\\" [label = \\\"4\\\"] \\n\\\"1\\\"-\\\"3\\\" [label = \\\"5\\\"] \\n}\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]} Figure 3: The numbers trace an Eulerian cycle on this graph.  The cycle in Figure 3 implies a workable sequence of six trials, but the use of this Eulerian conceptualization will be how it automates creating much longer sequences. For example, to achieve 21 trials the edges could be replicated four times. Figure 4 shows the graph with replicated edges, and already it looks too messy to traverse by sight. A real experiment will involve hundreds of trials, meaning that we’d like a way to automatically traverse an Eulerian circuit.\nedges_messy \u0026lt;- DiagrammeR::create_edge_df( from = rep(c(3,2,3,3,1), each=4), to = rep(c(2,3,3,1,3), each=4)) DiagrammeR::create_graph( nodes_df = nodes, edges_df = edges_messy) %\u0026gt;% DiagrammeR::render_graph(layout = \u0026quot;tree\u0026quot;)   {\"x\":{\"diagram\":\"digraph {\\n\\ngraph [layout = \\\"neato\\\",\\n outputorder = \\\"edgesfirst\\\",\\n bgcolor = \\\"white\\\"]\\n\\nnode [fontname = \\\"Helvetica\\\",\\n fontsize = \\\"10\\\",\\n shape = \\\"circle\\\",\\n fixedsize = \\\"true\\\",\\n width = \\\"0.5\\\",\\n style = \\\"filled\\\",\\n fillcolor = \\\"aliceblue\\\",\\n color = \\\"gray70\\\",\\n fontcolor = \\\"gray50\\\"]\\n\\nedge [fontname = \\\"Helvetica\\\",\\n fontsize = \\\"8\\\",\\n len = \\\"1.5\\\",\\n color = \\\"gray80\\\",\\n arrowsize = \\\"0.5\\\"]\\n\\n \\\"1\\\" [label = \\\"zero\\\", fillcolor = \\\"#F0F8FF\\\", fontcolor = \\\"#000000\\\", pos = \\\"0,2!\\\"] \\n \\\"2\\\" [label = \\\"low\\\", fillcolor = \\\"#F0F8FF\\\", fontcolor = \\\"#000000\\\", pos = \\\"1,2!\\\"] \\n \\\"3\\\" [label = \\\"high\\\", fillcolor = \\\"#F0F8FF\\\", fontcolor = \\\"#000000\\\", pos = \\\"0.5,1!\\\"] \\n \\\"3\\\"-\\\"2\\\" \\n \\\"3\\\"-\\\"2\\\" \\n \\\"3\\\"-\\\"2\\\" \\n \\\"3\\\"-\\\"2\\\" \\n \\\"2\\\"-\\\"3\\\" \\n \\\"2\\\"-\\\"3\\\" \\n \\\"2\\\"-\\\"3\\\" \\n \\\"2\\\"-\\\"3\\\" \\n \\\"3\\\"-\\\"3\\\" \\n \\\"3\\\"-\\\"3\\\" \\n \\\"3\\\"-\\\"3\\\" \\n \\\"3\\\"-\\\"3\\\" \\n \\\"3\\\"-\\\"1\\\" \\n \\\"3\\\"-\\\"1\\\" \\n \\\"3\\\"-\\\"1\\\" \\n \\\"3\\\"-\\\"1\\\" \\n \\\"1\\\"-\\\"3\\\" \\n \\\"1\\\"-\\\"3\\\" \\n \\\"1\\\"-\\\"3\\\" \\n \\\"1\\\"-\\\"3\\\" \\n}\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]} Figure 4: Replicating edges quickly complicates the graph.   Hierholzer’s algorithm automates Eulerian cycles Fortunately, there exists and algorithm for making Eulerian cycles that is both simple to implement and quick to run. First, here is a helper function to replicate edges, replicate_edges.\n#\u0026#39; replicate_edges #\u0026#39; #\u0026#39; @param edge_df output of DiagrammeR::create_edge_df (will only need columns `to` and `from`) #\u0026#39; @param n_reps integer number of times that the edges should be replicated #\u0026#39; #\u0026#39; @return replicated edge dataframe replicate_edges \u0026lt;- function(edge_df, n_reps){ replicate(n_reps, edge_df, simplify = FALSE) %\u0026gt;% dplyr::bind_rows() %\u0026gt;% dplyr::mutate(id = 1:dplyr::n()) } The next function will generate the Eulerian circuit, walk_circuit. It will take in an edge dataframe (possibly replicated) and output a vector containing the nodes listed in the order that they were reached. Again, I won’t spend too long explaining why this works. But the basic idea is to traverse the edges, deleting edges as you walk along them. You’ll eventually reach a dead-end. If there are still more edges, then backtrack until you can travel along an edge that will result in a different dead-end. Save a list of the nodes that were traveled while backtracking, and these nodes will contain the circuit.\n#\u0026#39; walk_circuit #\u0026#39; #\u0026#39; @param edge_df edge dataframes #\u0026#39; @param curr_v vertex at which to start the circuit #\u0026#39; #\u0026#39; @return vector consisting of Eulerian circuit along edges #\u0026#39; #\u0026#39; @details modified python script from https://gregorulm.com/finding-an-eulerian-path/. walk_circuit \u0026lt;- function(edge_df, curr_v){ # helpful to have the edges stored by node adj \u0026lt;- edge_df %\u0026gt;% dplyr::group_split(from) # vector to store final circuit circuit \u0026lt;- c() # Maintain a stack to keep vertices # start from given node curr_path \u0026lt;- curr_v while (length(curr_path)){ # If there\u0026#39;s a remaining edge if (nrow(adj[[curr_v]])){ # Push the vertex curr_path \u0026lt;- c(curr_path,curr_v) # Find the next vertex using an edge next_v_ind \u0026lt;- sample.int(nrow(adj[[curr_v]]), size=1) next_v \u0026lt;- adj[[curr_v]]$to[next_v_ind] # and remove that edge adj[[curr_v]] \u0026lt;- adj[[curr_v]][-next_v_ind,] # Move to next vertex curr_v \u0026lt;- next_v } else{ # back-track to find remaining circuit circuit \u0026lt;- c(circuit, curr_v) # Back-tracking curr_v \u0026lt;- tail(curr_path, n = 1) curr_path \u0026lt;- head(curr_path, n = -1) } } return(rev(circuit)) } Now replicate the edges twice and go for and Eulerian tour.\nedges_twice \u0026lt;- replicate_edges(edges, 2) walk_circuit(edges_twice, 3) ## [1] 3 1 3 2 3 1 3 3 2 3 3 This sequence is small enough that it’s feasible to verify the Eulerian property by hand, but it’ll be nice to have automate the checking. That is the purpose of this next function, check_blocking.\n#\u0026#39; check_blocking #\u0026#39; #\u0026#39; @param circuit output of walk_circuit #\u0026#39; @param nodes nodes_df, output of DiagrammeR::create_node_df. Used to label which nodes were visited during the walk #\u0026#39; #\u0026#39; @return tbl containing the counts of each transition type contained in the circuit. #\u0026#39; If all went well, the counts should be equal check_blocking \u0026lt;- function(circuit, nodes){ tibble::tibble(contrast = circuit, .name_repair = \u0026quot;check_unique\u0026quot;) %\u0026gt;% dplyr::mutate( trial = 1:dplyr::n(), contrast = nodes$label[contrast], last_contrast = dplyr::lag(contrast)) %\u0026gt;% dplyr::filter(trial \u0026gt; 1) %\u0026gt;% dplyr::group_by(contrast, last_contrast) %\u0026gt;% dplyr::summarise(n = dplyr::n(), .groups = \u0026quot;drop\u0026quot;) } Now, generate a sequence of 101 trials,\nedges_large \u0026lt;- replicate_edges(edges, n_reps = 20) circuit \u0026lt;- walk_circuit(edges_large, 3) circuit ## [1] 3 2 3 3 1 3 2 3 2 3 3 1 3 1 3 3 1 3 3 2 3 1 3 2 3 2 3 1 3 3 1 3 3 2 3 3 3 ## [38] 1 3 1 3 1 3 3 2 3 2 3 3 2 3 2 3 1 3 1 3 3 2 3 1 3 1 3 3 2 3 1 3 3 3 3 3 2 ## [75] 3 3 1 3 2 3 3 2 3 1 3 1 3 3 1 3 2 3 2 3 1 3 3 2 3 2 3 and check that each transition happened equally often\ncheck_blocking(circuit, nodes) %\u0026gt;% knitr::kable()   contrast last_contrast n    high high 20  high low 20  high zero 20  low high 20  zero high 20      Well, I was running. Out of precaution for COVID-19, it currently seems like a bad idea to try to collect more participants. And UMass is closed for the rest of the semester.↩︎\n adjacent black and white lines cropped to a circle, where the transitions between luminance follows a sinusoid↩︎\n In this particular case, a simpler solution would be to assign each pair of contrasts a number. For example,\nhigh -\u0026gt; high low -\u0026gt; high zero -\u0026gt; high  An appropriate sequence could be generated by simply permuting the numbers. For example 2, 3, 1, 3, 2 In that case, the sequence of trials would be low high zero high high high zero high low high. This works because the second trial of each of the transitions are high. But what if you also wanted a few low-\u0026gt;low and zero-\u0026gt;zero transitions, but wanted neither low-\u0026gt;zero nor zero-\u0026gt;low? By simply permuting the number codes, a zero-\u0026gt;zero transition could appear right after a low-\u0026gt;low transition, but to do that would require a filler low-zero.↩︎\n a cycle or circuit is a walk that starts and ends at the same node↩︎\n   ","date":1584144e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653490997,"objectID":"e311a73c2cfd7a8ada3de3f8af08c756","permalink":"https://psadil.github.io/psadil/post/counterbalanced-continuous-designs-with-eulerian-walks/","publishdate":"2020-03-14T21:42:18-04:00","relpermalink":"/psadil/post/counterbalanced-continuous-designs-with-eulerian-walks/","section":"post","summary":"Many experiments require counterbalancing sequences of trials. For example, I’m currently running an experiment on serial dependence1. In my experiment, participants report the orientation of a grating2 stimulus on each trial. The serial dependence effect is how their responses on one trial depend on either the orientation of the previous trial or their response on that trial. To tease apart the effects of prior stimuli from prior responses, I’m manipulating the visual contrast of the gratings ( Michelson contrast ).","tags":["R","dissertation","fmri"],"title":"counterbalanced continuous designs with eulerian walks","type":"post"},{"authors":null,"categories":[],"content":"  Visual perception research has produced many illusions. Stare at a waterfall for a minute, then look away and the whole world appears in motion, traveling upwards (Addams 1834). Given proper lighting, black paper can appear white, but placing a white piece of paper nearby colors the first dark gray (Gelb 1929; as cited by Cataliotti and Gilchrist 1995). Inspecting two sets of black lines – horizontal lines that obscure a solid red field like a picket fence, along with vertical lines that obscure a solid green field – causes the black lines alone to induce a perception of color, an illusory shading that can last for days (McCollough 1965). Such illusions reveal the intricacies of visual perception, kludges and all1. The pizzazz of the illusions affords visual perception a kind of scientific rigor; the effects are obviously real and reproducible, so to satisfyingly explain how visual perception works so well also requires explaining how atypical visual environments can so often dupe vision.\nHowever, research on visual perception inevitably strays from fascinating and easily demonstrable illusions. Of course, even without the glamour of classic visual illusions an effect can still be a reliable and valid object of research. But as the effect becomes more subtle, observing the effect requires increasingly complex analyses. Unfortunately, the most complex analyses, when misapplied, can also transmute noise into something that appears noteworthy2. So when a subtle effect relies entirely on a complex analysis, the effect becomes suspect. To highlight the obviousness of an effect, it can help to visualize and revisualize the data.\nA few weeks ago, I posted about an effect that warrants revisualizing, the apparent stability of visual perception. I discussed this stability as one of the reasons that perception exhibits serial dependence (Fischer and Whitney 2014), which is the tendency of visual perception to be slightly erroneous, resembling not an accurate reproduction of the input it receives but a mixture of current input and input from the past. This dependence may occur during perception to refine the inherently erratic input provided by the retina. I attempt to demonstrate the need for refinement with Figures 1-3. Figure 1 shows a stimulus from an ongoing experiment. In the experiment, the participant was required to simply hold their gaze still. The figure is a heatmap, showing that this participant successfully fixated on a small region of the stimulus. However, the heatmap obscures how fixating on a “small” region implies ample movement. Figure 2 recapitulates, in real time, how the gaze wandered during fixation. But then Figure 2 obscures what that wandering means for the visual system; whenever the eye moves, the retina receives (and so must then output) a different image. With Figure 3, I attempt to visualize what these eye movements mean for the retinal image. In Figure 3, the movements of the gaze are transferred to the stimulus, revealing how the retina receives a twitching stimulus. The effect to explain here is why fixating at the dot in Figure 1 – given that the eyes move as shown in Figure 2 – does not elicit the jumpy movie depicted in Figure 3, but instead elicits the stable image of Figure 1.\n Figure 1: Example stimulus, with heatmap of eye positions during one trial overlayed. Participants viewed such grating stimuli, each for five seconds. They were instructed to fixate on the central magenta dot. The blob to the left of the dot indicates where this participant looked during the trial; the brightest regions held their gaze for the most time.   Figure 2: Time course of fixations from Figure 1. The blue dot shows where the participant looked at each moment. Fixations were recorded at 1000 Hz, but this video has been downsampled to 10 Hz.   Figure 3: Approximate example of the retinal image from Figure 2. While the gaze travels throughout the visual environment, that environment is largely stable. Yet the travelling gaze constantly alters the image imprinted on the retina.  References Addams, Roberts. 1834. “An Account of a Peculiar Optical Phenomenon Seen After Having Looked at a Moving Body.” The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science 5 (29): 373–74.  Cataliotti, Joseph, and Alan Gilchrist. 1995. “Local and Global Processes in Surface Lightness Perception.” Perception \u0026amp; Psychophysics 57 (2): 125–35.  Fischer, Jason, and David Whitney. 2014. “Serial Dependence in Visual Perception.” Nature Neuroscience 17 (5): 738.  Gelb, Adhémar. 1929. “Die \"Farbenkonstanz\" Der Sehdinge.” In Handbuch Der Normalen Und Pathologischen Physiologie, 594–678. Springer-Verlag.  McCollough, Celeste. 1965. “Color Adaptation of Edge-Detectors in the Human Visual System.” Science 149 (3688): 1115–16.     To personally experience the illusions mentioned here – and many others – explore Michael Bach’s website.↩︎\n Full disclosure: I write this as someone who has written a paper on a novel development of an already obscure analysis, a development that I needed to support the claims in another paper. I am a kettle.↩︎\n   ","date":1574294400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653490997,"objectID":"e053a50df2bfaefa9190ecb53844c7c7","permalink":"https://psadil.github.io/psadil/post/thoughts-on-eye-movement/","publishdate":"2019-11-21T00:00:00Z","relpermalink":"/psadil/post/thoughts-on-eye-movement/","section":"post","summary":"Visual perception research has produced many illusions. Stare at a waterfall for a minute, then look away and the whole world appears in motion, traveling upwards (Addams 1834). Given proper lighting, black paper can appear white, but placing a white piece of paper nearby colors the first dark gray (Gelb 1929; as cited by Cataliotti and Gilchrist 1995). Inspecting two sets of black lines – horizontal lines that obscure a solid red field like a picket fence, along with vertical lines that obscure a solid green field – causes the black lines alone to induce a perception of color, an illusory shading that can last for days (McCollough 1965).","tags":[],"title":"thoughts on eye movement","type":"post"},{"authors":null,"categories":["dissertation"],"content":"  Science produces models that provide parsimonious descriptions of the world. In cognitive psychology, models regularly compete to explain a few phenomena. But models can survive experiment after experiment, both because of the difficulty of capturing participant’s nuanced behavior, and because models often make highly overlapping predictions. In these cases, a model succeeds through its relative parsimony.\nIn cognitive psychology, measures of information criteria, specifically Akaike’s and the Bayesian information criteria (Schwarz 1978; Akaike 1973), determine a winning model. These criteria measure complexity by tallying the number of parameters in a model. Long maths and specific assumptions justify the claim that a model with more parameters is more complex than a model with fewer parameters, and so does our intuition that a model is complex if it has many moving parts. Unfortunately, the assumptions fail in common situations, such as when the models are fit in a Bayesian rather than frequentist setting. An alphabet soup of other information criteria exists (in addition to Akaike’s the Bayesian criteria, there is the DIC, WAIC, KIC, NIC, TIC, etc), and these other criteria assign complexity more complexly. These criteria are sensitive not only to the number of parameters in a model but also to the varied roles that a parameter can have. They assign the complexity of a model based on the model’s number of ‘effective parameters.’\nFor intuition on why tallying the number of parameters is an insufficient measure complexity, consider two models of response time. Both models assume that response times are distributed according to a normal distribution. In this simple example, the variability of the distributions are known, and so the models have only a single free parameter, which is the average response time. In one model, that average can be any number, a value from negative to positive infinity. This is the kind of model implicitly assumed when we conduct a t-test on the averages of response times. Of course the model is a simplification of response times, but this model also has the glaring flaw that it allows the average response time to be negative; a participant cannot respond to stimulation before the stimulus appears. The second model addresses this flaw by adding the constraint that the average response time cannot be negative. Although the second model is more constrained, the models have the same number of parameters. The second model can only account for half of the patterns of data as the first; the second model is twice as parsimonious as the first (Gelman, Hwang, and Vehtari 2014). To adjudicate between these model requires a measure that is sensitive to complexity but does not simply tally the number of parameters in each model.\nReferences Akaike, Hirotogu. 1973. “Information Theory and an Extension of the Maximum Likelihood Principle.” In Proceedings of the Second International Symposium on Information Theory, 267–81.  Gelman, Andrew, Jessica Hwang, and Aki Vehtari. 2014. “Understanding Predictive Information Criteria for Bayesian Models.” Statistics and Computing 24 (6): 997–1016. https://doi.org/10.1007/s11222-013-9416-2.  Schwarz, Gideon. 1978. “Estimating the Dimension of a Model.” The Annals of Statistics 6 (2): 461–64. https://doi.org/10.1214/aos/1176344136.    ","date":1573776e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653490997,"objectID":"f3c41ade187ff4d875dc0c5b820d5d5f","permalink":"https://psadil.github.io/psadil/post/half-of-a-parameter/","publishdate":"2019-11-15T00:00:00Z","relpermalink":"/psadil/post/half-of-a-parameter/","section":"post","summary":"Science produces models that provide parsimonious descriptions of the world. In cognitive psychology, models regularly compete to explain a few phenomena. But models can survive experiment after experiment, both because of the difficulty of capturing participant’s nuanced behavior, and because models often make highly overlapping predictions. In these cases, a model succeeds through its relative parsimony.\nIn cognitive psychology, measures of information criteria, specifically Akaike’s and the Bayesian information criteria (Schwarz 1978; Akaike 1973), determine a winning model.","tags":[],"title":"Half of a parameter","type":"post"},{"authors":null,"categories":["dissertation"],"content":"  In last week’s post, I discussed how some experiments in cognitive psychology require researchers to pick a differently intense stimulus for each participant. In particular, I discussed a procedure for picking an intensity that elicits positive responses on approximately half of trials, the staircase procedure. In the staircase procedure, the researcher increases the intensity after every positive response and decreases the intensity after every negative response. If the participant completes another set of trials in which the intensity is fixed to the average of the intensities that were used during the staircase, the participant will provide positive responses approximately half of the time. But a researcher may want participants to give positive responses with a different proportion. A different proportion of responses can be achieved by transforming the staircase (Levitt 1971).\nThe original staircase produced an intensity that elicited half positive responses by balancing the proportion of positive and negative responses. Intuitively, to elicit a higher proportion of positive responses, the transformed staircase must tilt this balance by converging on a more intense stimulus. Any staircase affects responses by changing the stimulus intensity as a function of how participants behave. The staircase that changes the intensity after every response is called a one-up, one-down staircase; one negative response causes the intensity to go up, one positive response causes the intensity to go down. A transformed staircase can make a positive response more likely by making intensity decreases less likely. The names of transformed staircases are analogous to the one-up, one-down label: a one-up, two-down staircase increases the stimulus after any negative response and decreases the intensity after two positive responses; a two-up, three-down staircase increases the intensity after two negative responses and only decreases the intensity after three positive responses; and so on. Altering when the staircase increments the intensity alters the intensity at which the staircase converges.\nWe can use algebra to calculate the proportion of positive responses elicited by the intensity converged on by a staircase. As an example, consider a staircase that increases the intensity after a single positive response but decreases the intensity after two negative responses, a one-up, two-down staircase. Since the one-up, two-down regime results in fewer decreases than the one-up, one-down staircase, we should expect that the proportion of positive responses will be higher than half. For the calculation, note that there are three possible sequences of responses that result in an intensity change. Two of these sequences cause an increase, either a single negative or a positive followed by a negative. For the algebra later, let \\(p(x|i)\\) be the probability of obtaining a positive response at stimulus intensity, \\(i\\). Our goal is to solve for this probability. Participants can only provide either positive or negative responses, so the probability of obtaining a negative response to that stimulus is \\(1-p(x|i)\\). The probability of increasing the stimulus intensity away from intensity \\(i\\), \\(p(\\text{up|i})\\) is the sum of the probabilities for the two sequences:\n\\[ \\begin{equation} p(\\text{up|i}) = (1-p(x|i)) + p(x|i)(1-p(x|i)) \\\\ \\tag{1} \\end{equation} \\]\nThere is only a single way in which the staircase decreases intensity: the participant must provide two positive responses in a row. The probability of the intensity decreasing away from \\(i\\), \\(p(\\text{down|i})\\) is equal to the probability of two positive responses to that stimulus, or\n\\[ \\begin{equation} p(\\text{down|i}) = p(x|i)^2 \\\\ \\tag{2} \\end{equation} \\]\nCombining equations (1) and (2) will give a relationship that determines the proportion of positive responses participants will tend to provide under this staircase. To see how to combine these equations, remember that the one-up, one-down staircase converged on an intensity for which the proportion of positive and negative responses were equal. This equality occurred because at that intensity, the probability of an up and down step were equally likely. So, to determine at which intensity the one-up, two-down staircase converges, we must determine the probability of a positive response that will make an up and down step likely in the one-up, two-down staircase. That is, we set the right hand sides of equations (1) and (2) to be equal, and solve for \\(p(x|i)\\)1.\n\\[ \\begin{equation} \\begin{aligned} p(x|i)(1-p(x|i)) + (1-p(x|i)) \u0026amp; = p(x|i)^2 \\\\ \\implies p(x|i) - p(x|i)^2 + 1-p(x|i) \u0026amp; = p(x|i)^2 \\\\ \\implies -2 p(x|i)^2 \u0026amp; = -1 \\\\ \\implies p(x|i)\u0026amp; = \\frac{1}{\\sqrt{2}} \\\\ \u0026amp; \\approx 0.707 \\end{aligned} \\tag{3} \\end{equation} \\]\nEquation (3) shows that a one-up, two-down staircase will converge on a stimulus intensity that elicits approximately 70% positive responses (Figure 1). As one way to see that this solution makes sense, relate this solution back to the probabilities of making either an up or down step. By equation (2), this solution implies that at this intensity, that an up step occurs with a 50% probability. As desired, any sequence that elicits a transition has an equal chance of being one that elicits either an up or down step.\nThe original staircase procedure capitalized on the idea that the stimulus intensity which elicits half positive responses can be estimated by starting from an arbitrary intensity, changing the intensity on every trial based on whether a participant responded positively or negatively, and then retroactively looking at which intensities were shown. The transformed staircase enables estimation of an intensity that elicits different behavior. Similar algebra to that outlined in this post can be used to determine the proportion of positive responses elicited by other staircases. Unfortunately, most proportions will not have an easy staircase regime. Moreover, complex staircases will only change the stimulus intensity infrequently, requiring more trials to estimate the converged upon intensity stably. However, the proportions reachable by simple staircase are often good enough; rare is the experiment that requires, not 70.7% but 73% positive responses. And the staircase procedure did not require any knowledge of the exact shape of the psychometric function, just that there was a psychometric function. The simplicity of the transformed staircase makes it an attractive way to pick an intensity.\n Figure 1: A sequence of trials with stimulus intensity governed by a one-up, two-down staircase. With this staircase, the intensity increases after a single negative response but decreases only after two positive responses. After enough trials, the average of the stimulus intensities shown to participants will elicit approximately 70% positive responses (dashed line). The intensity resulting from a one-up, one-down staircase is shown for comparison (solid line).  References Levitt, H. 1971. “Transformed up-down Methods in Psychoacoustics.” The Journal of the Acoustical Society of America 49 (2B): 467–77.     The equation will have two solutions, but one of those solutions will be negative. A negative value is not an actual solution, because we are dealing with probabilities and so there is an additional constraint that \\(0 \\leq p(x|i) \\leq 1\\)↩︎\n   ","date":1573171200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653490997,"objectID":"a2b53dc193f938d22789a71b5797fc70","permalink":"https://psadil.github.io/psadil/post/staircases-for-thresholds-part2/","publishdate":"2019-11-08T00:00:00Z","relpermalink":"/psadil/post/staircases-for-thresholds-part2/","section":"post","summary":"In last week’s post, I discussed how some experiments in cognitive psychology require researchers to pick a differently intense stimulus for each participant. In particular, I discussed a procedure for picking an intensity that elicits positive responses on approximately half of trials, the staircase procedure. In the staircase procedure, the researcher increases the intensity after every positive response and decreases the intensity after every negative response. If the participant completes another set of trials in which the intensity is fixed to the average of the intensities that were used during the staircase, the participant will provide positive responses approximately half of the time.","tags":[],"title":"Staircases for Thresholds, Part 2","type":"post"},{"authors":null,"categories":["dissertation"],"content":"  Performing any experiment on cognition requires deciding which stimuli to use. Some experiments require participants to make many errors, requiring the stimuli to be challenging. In other experiments, participants must respond accurately, requiring stimuli that are easy but not so easy that participants lose attention. Moreover, participants behave idiosyncratically, so to avoid wasting either the researchers’ or participants’ time the stimuli ought to be tailored to each participant. To decide which stimuli to use, researchers can rely on a psychometric function (Figure 1). These functions describe a relationship between the intensity of a stimulus and how a participant responds to that stimulus, when responses can be classified as either a positive or negative. Precisely what is meant by ‘intensity’ and ‘positive or negative’ depends on the experimental task, but they roughly correspond to the amount of stimulation on each trial and how difficult it is to notice that stimulation. In a task in which participants must detect a pure tone that is occasionally presented over white noise, the intensity could be the volume of the tone and participants’ responses are positive when they detect the tone. With a psychometric function, deciding on a stimulus translates to picking the proportion of trials that should receive positive responses – picking the desired difficulty – and then using the intensity that elicits that behavior. This replaces the task of picking stimuli with inferring participants’ psychometric functions.\n Figure 1: A psychometric curve with threshold intensity. Psychometric curves relate the intensity of stimulation to the perception of stimulation, or the proportion positive responses. There are many parameterizations of these functions, but they are typically sigmoidal. The intensity at which, on average, half of responses are positive is often of interest. This intensity is called the threshold.  To infer psychometric functions, standard procedures exist, though these procedures have varied efficiency. In particular, when only a single stimulus intensity is required, it would be inefficient to estimate the entire function. Consider a researcher attempting to elicit half positive responses, behavior elicited by the so called threshold stimulus intensity. A simple procedure to infer the psychometric function involves presenting a wide range of stimulus intensities, fitting the function to the data, and using the estimated function to infer the threshold. Each datum increases the precision of the estimate, but some data will be more useful than others. Intensities close to the tails of the function will pin down the function at those tails, but functions with different thresholds can behave similarly in their tails (Figure 2). The threshold is most tightly constrained by responses to stimuli at the threshold (Levitt 1971). Therefore, an ideal procedure to estimate the threshold intensity would involve repeatedly presenting the threshold intensity. The ideal procedure is unfeasible, since if the threshold were known there would be no need for inference. But although the exact threshold intensity cannot be presented on every trial, certain procedures enable most trials to approximate the ideal.\n Figure 2: Psychometric functions with different thresholds. Although behavior at the tails of these functions are similar, they have different thresholds.  One type of procedure that locates the threshold intensity, both simply and efficiently, is called a staircase. A staircase procedure changes the stimulus intensity on every trial based on how participants respond. A staircase that locates the threshold increases stimulus intensity after a participant makes a negative response and decreases the intensity after a participant responds positively. Even when the first stimulus has an intensity far from the threshold (Figure 3), the staircase brings the intensity to the threshold, a convergence that is ensured by the psychometric function. For example, when intensity is lower than the threshold, a participant tends to make negative responses. After a negative response, the contrast is increased. With an increased contrast, the participant will be more likely to make a positive response. If the intensity is still lower then then threshold, the participant will likely provide another negative response, causing the intensity increase further. After enough trials with intensity too low, the intensity will be pushed towards the threshold. If the intensity strays from the threshold, the same dynamics push the threshold back to threshold. The staircase forces the intensity to remain close to the threshold.\n Figure 3: Example sequence of trials in which stimulus intensity is controlled by a staircase procedure. After each negative response, the intensity increases, and after each positive response the intensity decreases. Although the initial intensity was much lower than the threshold, the staircase brings the intensity close to threshold and then keeps it there.  References Levitt, H. 1971. “Transformed up-down Methods in Psychoacoustics.” The Journal of the Acoustical Society of America 49 (2B): 467–77.    ","date":1572393600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653490997,"objectID":"312f5db1a52fe1ae79b6adbcd56c2556","permalink":"https://psadil.github.io/psadil/post/staircases-for-thresholds/","publishdate":"2019-10-30T00:00:00Z","relpermalink":"/psadil/post/staircases-for-thresholds/","section":"post","summary":"Performing any experiment on cognition requires deciding which stimuli to use. Some experiments require participants to make many errors, requiring the stimuli to be challenging. In other experiments, participants must respond accurately, requiring stimuli that are easy but not so easy that participants lose attention. Moreover, participants behave idiosyncratically, so to avoid wasting either the researchers’ or participants’ time the stimuli ought to be tailored to each participant. To decide which stimuli to use, researchers can rely on a psychometric function (Figure 1).","tags":[],"title":"Staircases for Thresholds","type":"post"},{"authors":null,"categories":[],"content":"  Functional magnetic resonance imaging records brain activity with spatially distinct voxels, but this segmentation will be misaligned with a brain’s meaningful boundaries. The segmentation results in some voxels recording activity from different types of tissue – types that are both neural an non-neural – but even voxels that exclusively sample gray matter can span functionally distinct cortex. For example, a 3T scanner allows voxels in the range of 1.5-3 mm\\(^3\\), but orientation columns have an average width of 0.8 mm (Yacoub, Harel, and Uğurbil 2008). Studying orientation columns with such low resolution requires statistical tools.\nOne statistical tool models voxel activity as a linear combination of the activity of a small number of neural channels (Brouwer and Heeger 2009; Kay et al. 2008). These models are called forward models, describing how the channel activity transforms into voxel activity. In early sensory cortex, the channels are analogous to cortical columns. In later cortex, the channels are more abstract dimensions of a representational space. Developing a forward model requires assuming not only how many channels contribute of a voxel’s activity, but also the tuning properties of those channels. With these assumptions, regression allows inferring the contribution of each channel to each voxel’s activity. Let \\(N\\) be the number of observations for each voxel, \\(M\\) be the number of voxels, and \\(K\\) be the number of channels within a voxel. The forward model specifies that the data (\\(B\\), \\(M \\times N\\)) result from a weighted combination of the assumed channels responses (\\(C\\), \\(K \\times N\\)), where the weights (\\(W\\), \\(M \\times K\\)) are unknown.\n\\[ B = WC \\]\nTaking the pseudoinverse of the channel matrix and multiplying the result by the data gives an estimate of the weight matrix:\n\\[ \\widehat{W} = BC^T(CC^T)^{-1} \\]\nAssumptions about \\(C\\) are assumptions about how the channels encode stimuli. Different encoding schemes can be instantiated with different \\(C\\), and any method for comparing linear models could be used to compare the schemes.\nThe forward encoding model enables comparison of static encoding schemes, but neural encoding schemes are dynamic. Attentional fluctuations, perceptual learning, and stimulation history all modulate neural tuning functions (McAdams and Maunsell 1999; Reynolds, Pasternak, and Desimone 2000; Siegel, Buschman, and Miller 2015; Yang and Maunsell 2004). To explore modulations with functional magnetic resonance imaging, some researchers have inverted the encoding model (Garcia, Srinivasan, and Serences 2013; Rahmati, Saber, and Curtis 2018; Saproo and Serences 2014; Scolari, Byers, and Serences 2012; Sprague and Serences 2013; Vo, Sprague, and Serences 2017). The inversion is a variation of cross validation. The method estimates the weight matrix with only some of the data (e.g., all data excluding a single run). The held out data, \\(B_H\\), contains observations from all experimental condition across which the tuning functions might vary. The encoding model is inverted by multiplying the pseudoinverse of the weight matrix with the held out data to estimate a new channel response matrix.\n\\[ \\widehat{C} = \\widehat{W}^T(\\widehat{W}\\widehat{W}^T)^{-1}B_H \\]\nThe new channel response matrix estimates how the channels respond in each experimental condition.\nAlthough validation studies demonstrated that the inverted encoding model enables inferences that recapitulate some modulations observed with electrophysiology (Sprague et al. 2018; Sprague, Saproo, and Serences 2015), the inversion also misleads inferences about certain fundamental modulations (Gardner and Liu 2019; Liu, Cable, and Gardner 2018). In particular, increasing the contrast of an orientation increases the gain of neurons tuned to orientation without altering their tuning bandwidth (Alitto and Usrey 2004; Sclar and Freeman 1982; Skottun et al. 1987), but the inverted encoding model (incorrectly) suggests that higher contrast decreases bandwidth (Liu, Cable, and Gardner 2018). Inferences are misled because the estimated channel responses are constrained by the initial assumptions about \\(C\\) (Gardner and Liu 2019). Using the encoding model to study modulations requires a way to estimate the contribution of each channel without assuming a fixed channel response function.\nReferences Alitto, Henry J, and W Martin Usrey. 2004. “Influence of Contrast on Orientation and Temporal Frequency Tuning in Ferret Primary Visual Cortex.” Journal of Neurophysiology 91 (6): 2797–2808.  Brouwer, Gijs Joost, and David J Heeger. 2009. “Decoding and Reconstructing Color from Responses in Human Visual Cortex.” Journal of Neuroscience 29 (44): 13992–4003.  Garcia, Javier O, Ramesh Srinivasan, and John T Serences. 2013. “Near-Real-Time Feature-Selective Modulations in Human Cortex.” Current Biology 23 (6): 515–22.  Gardner, Justin L, and Taosheng Liu. 2019. “Inverted Encoding Models Reconstruct an Arbitrary Model Response, Not the Stimulus.” eNeuro 6 (2).  Kay, Kendrick N, Thomas Naselaris, Ryan J Prenger, and Jack L Gallant. 2008. “Identifying Natural Images from Human Brain Activity.” Nature 452 (7185): 352.  Liu, Taosheng, Dylan Cable, and Justin L Gardner. 2018. “Inverted Encoding Models of Human Population Response Conflate Noise and Neural Tuning Width.” Journal of Neuroscience 38 (2): 398–408.  McAdams, Carrie J, and John HR Maunsell. 1999. “Effects of Attention on Orientation-Tuning Functions of Single Neurons in Macaque Cortical Area V4.” Journal of Neuroscience 19 (1): 431–41.  Rahmati, Masih, Golbarg T Saber, and Clayton E Curtis. 2018. “Population Dynamics of Early Visual Cortex During Working Memory.” Journal of Cognitive Neuroscience 30 (2): 219–33.  Reynolds, John H, Tatiana Pasternak, and Robert Desimone. 2000. “Attention Increases Sensitivity of V4 Neurons.” Neuron 26 (3): 703–14.  Saproo, Sameer, and John T Serences. 2014. “Attention Improves Transfer of Motion Information Between V1 and MT.” Journal of Neuroscience 34 (10): 3586–96.  Sclar, G, and RD Freeman. 1982. “Orientation Selectivity in the Cat’s Striate Cortex Is Invariant with Stimulus Contrast.” Experimental Brain Research 46 (3): 457–61.  Scolari, Miranda, Anna Byers, and John T Serences. 2012. “Optimal Deployment of Attentional Gain During Fine Discriminations.” Journal of Neuroscience 32 (22): 7723–33.  Siegel, Markus, Timothy J Buschman, and Earl K Miller. 2015. “Cortical Information Flow During Flexible Sensorimotor Decisions.” Science 348 (6241): 1352–55.  Skottun, Bernt C, Arthur Bradley, Gary Sclar, Izumi Ohzawa, and Ralph D Freeman. 1987. “The Effects of Contrast on Visual Orientation and Spatial Frequency Discrimination: A Comparison of Single Cells and Behavior.” Journal of Neurophysiology 57 (3): 773–86.  Sprague, Thomas C, Kirsten CS Adam, Joshua J Foster, Masih Rahmati, David W Sutterer, and Vy A Vo. 2018. “Inverted Encoding Models Assay Population-Level Stimulus Representations, Not Single-Unit Neural Tuning.” eNeuro 5 (3).  Sprague, Thomas C, Sameer Saproo, and John T Serences. 2015. “Visual Attention Mitigates Information Loss in Small-and Large-Scale Neural Codes.” Trends in Cognitive Sciences 19 (4): 215–26.  Sprague, Thomas C, and John T Serences. 2013. “Attention Modulates Spatial Priority Maps in the Human Occipital, Parietal and Frontal Cortices.” Nature Neuroscience 16 (12): 1879.  Vo, Vy A, Thomas C Sprague, and John T Serences. 2017. “Spatial Tuning Shifts Increase the Discriminability and Fidelity of Population Codes in Visual Cortex.” Journal of Neuroscience 37 (12): 3386–3401.  Yacoub, Essa, Noam Harel, and Kâmil Uğurbil. 2008. “High-Field fMRI Unveils Orientation Columns in Humans.” Proceedings of the National Academy of Sciences 105 (30): 10607–12.  Yang, Tianming, and John HR Maunsell. 2004. “The Effect of Perceptual Learning on Neuronal Responses in Monkey Visual Area V4.” Journal of Neuroscience 24 (7): 1617–26.    ","date":1571875200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653490997,"objectID":"b42a88b9fb6759168715f22b15d6803c","permalink":"https://psadil.github.io/psadil/post/forward-encoding-model/","publishdate":"2019-10-24T00:00:00Z","relpermalink":"/psadil/post/forward-encoding-model/","section":"post","summary":"Functional magnetic resonance imaging records brain activity with spatially distinct voxels, but this segmentation will be misaligned with a brain’s meaningful boundaries. The segmentation results in some voxels recording activity from different types of tissue – types that are both neural an non-neural – but even voxels that exclusively sample gray matter can span functionally distinct cortex. For example, a 3T scanner allows voxels in the range of 1.","tags":["fmri"],"title":"Forward encoding model","type":"post"},{"authors":null,"categories":["dissertation"],"content":"  Perceptual decisions can be deconstructed with evidence accumulation models. These models formalize expectations about how participants behave, when that behavior involves repeatedly sampling information towards until surpassing a necessary threshold of information. At a cognitive level, the different models instantiate the components differently, but at a neural level the models rely on common mechanisms. To accumulate evidence the models assume two distinct populations of neurons. One population responds to available information. This population can be thought of as a sensory population, such that each neuron in the population represents one of the available options. The second population listens to the first, transforming the sensory activity into evidence for each decision and accumulating the evidence through time. This second population can be called an integrating population. While the location of the sensory population depends on the information that needs to be represented, the location of the integrating population depends on the required behavior. If participants must make decisions about orientations, the sensory population might be striatal neurons tuned to different orientations. If participants make decisions with saccades, the integrating population might be in the frontal eye fields. Understanding how these two populations reveals different ways that decisions can be biased.\n Figure 1: Sensory channels evenly represent orientations. The curves from the sensory population represent the average activity of each neuron to a given orientation. Although the curves show average activity, at any given moment the actual activity of each neuron may be higher or lower. The integrating curve reflects the average evidence that the integrating population will record. The flatness of the integrating curve reflects an unbiased representation. Only eight neurons from the sensory population are shown to avoid overcrowding.  For the integrating population to accumulate evidence, it must transform the activity of the sensory population into a meaningful signal. Figure 1 depicts that transformation when participants must report orientations. Each neuron in the sensory population responds most strongly to a specific orientation, but all of them are active whenever an orientation is present. The function describing how a sensory neuron respond to different orientations is called the neuron’s tuning function. The integrating population will respond according to some other function of that sensory activity. One simple integrating function associates each neuron with its preferred orientation; the integrating population then tallies evidence based on whichever neuron is most active. This function requires the sensory population to represent each orientation with at least one neuron. If there are enough sensory neurons, the integrating population will be able to accumulate evidence for each orientation without bias.\n Figure 2: Modulations to activity of the sensory population will provide the integrating population a biased representation of the orientation. Unlike in Figure 1, the sensory population in both panels provides the integrating population with an uneven representation of orientation.  The tuning characteristics of sensory neurons are variable, and this variability causes biases to emerge in the evidence accumulation process (Figure 2). One common alteration is an increased gain, whereby the tuning function is multiplied by some value. When the gains of tuning functions are altered heterogeneously, a neuron may have a higher activity even when the orientation it is responsible for is not present. The neurons with the highest gain will bias the evidence gathered by the integrating population. Alternatively, tuning functions might shift, along with the orientation each neuron signals. The shift causes the sensory population to over-represent of some orientations and leave others underrepresented. These alterations can provide advantages in certain circumstances. For example, a heterogeneously increased gain will be useful when some orientations are known to be more likely than others, and a shift will be useful when different orientations require differently precise responses. But to accumulate evidence without bias, the sensory population must restore more uniform tuning.\n","date":1571356800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653490997,"objectID":"bbef368816f18e0350aebeab5f6c0d9c","permalink":"https://psadil.github.io/psadil/post/neural-modulation-for-serial-dependence/","publishdate":"2019-10-18T00:00:00Z","relpermalink":"/psadil/post/neural-modulation-for-serial-dependence/","section":"post","summary":"Perceptual decisions can be deconstructed with evidence accumulation models. These models formalize expectations about how participants behave, when that behavior involves repeatedly sampling information towards until surpassing a necessary threshold of information. At a cognitive level, the different models instantiate the components differently, but at a neural level the models rely on common mechanisms. To accumulate evidence the models assume two distinct populations of neurons. One population responds to available information.","tags":[],"title":"Modulations to tuning functions can bias evidence accumulation","type":"post"},{"authors":null,"categories":["dissertation"],"content":"  One framework for understanding perception casts it as inference: just as a statistician uncovers noisy data to uncover patterns, an organism perceives when it converts sensations into guesses about its environment. The framework not concrete enough to be called a theory of perception, since it is not clear what data could falsify it1. But the framework can remind perceptual researchers about the many strategies available for modeling the world. Do perceiving organisms employ similar strategies?\nOne property that distinguishes many statistical strategies is a tradeoff between bias and variability Consider this tradeoff with an example. A statistician must estimate the average height of college-level soccer players. The true average could be uncovered by measuring the height of every player at every college–the statistician would not need inference. But the statistician is constrained by limited resources. They can only measure the players from a single college, though they may measure the heights of any student at the college. The statistician must now decide between an unbiased but variable or biased but precise strategy. Measuring only the soccer players gives an unbiased estimate, but with so few players the team’s average may be far from the true average. The statistician cannot be confident that the single team resembles all teams. Alternatively, the statistician may supplement their estimate with the heights of players from another, related sport. Since ultimate frisbee players may have similar heights to soccer players, incorporating their heights into the estimate may counteract any anomalously sized soccer players. However, incorporating even a single player from another sport biases the estimate, in the sense that the average height of all soccer players will not equal the average height of all soccer players and the one ultimate player2. One strategy – measure only soccer players – would give the true answer if there were enough resources, but the other strategy – measure everyone that is similar to a soccer player – may approximate the truth well with limited resources.\nThe bias-variability tradeoff gives a functional interpretation to the perceptual effect called serial dependence (Fischer and Whitney 2014; Cicchini, Mikellidou, and Burr 2018). Serial dependence occurs when participants judge perceptual stimuli across many trials, and their judgments on one trial depend on their immediately preceding judgment. Like the constrained statistician, participants may not process each stimulus completely: participants only see stimuli for brief durations, their judgments are made after the stimuli are masked, and their attention fluctuates throughout the hundreds of trials. The bias is often attractive, meaning that participants’ judgments reflect a blending of the stimuli on the current and previous trials. Serial dependence may reflect a strategy – not necessarily intentional – that reduces variability across judgments by combining information. Although the strategy biases the judgments, it may help each individual estimate approach truth.\nReferences Cicchini, G. M., K. Mikellidou, and D. C Burr. 2018. “The Functional Role of Serial Dependence.” Proceedings of the Royal Society B 285 (1890): 20181722.  Fischer, Jason, and David Whitney. 2014. “Serial Dependence in Visual Perception.” Nature Neuroscience 17 (5): 738.     A perceptual system does need to be limited to the statistical tools that have already been developed, so even a demonstration that organisms don’t employ any known statistical tool would not rule out the framework.↩︎\n Assume that the ultimate player is not as tall as the average soccer player↩︎\n   ","date":1570752e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653490997,"objectID":"ce88e6cb683bf1508e7098167d51c45e","permalink":"https://psadil.github.io/psadil/post/serial-dependence-reflects-a-preference-for-low-variability/","publishdate":"2019-10-11T00:00:00Z","relpermalink":"/psadil/post/serial-dependence-reflects-a-preference-for-low-variability/","section":"post","summary":"One framework for understanding perception casts it as inference: just as a statistician uncovers noisy data to uncover patterns, an organism perceives when it converts sensations into guesses about its environment. The framework not concrete enough to be called a theory of perception, since it is not clear what data could falsify it1. But the framework can remind perceptual researchers about the many strategies available for modeling the world.","tags":[],"title":"serial dependence reflects a preference for low variability","type":"post"},{"authors":null,"categories":["dissertation"],"content":"  Cognitive experiments can require participants to complete hundreds of trials, but completing so many trials invariably alters participants’ behavior. Their behavior late in the experiment can depend on their behavior early in the experiment. Although such dependence can be an experimental confound, the dependence itself can provide clues about cognition. One simple kind of dependence occurs through learning; hundreds of trials provides participants ample practice. A more subtle dependence can emerge between sequential trials, an effect called serial dependence. Theoretical interpretations of serial dependence vary, and some of that variability may relate to how the dependence is measured. In this post, I review a statistical method commonly used to analyze serial dependence and discuss one way that method can fail.\nI will focus on the analysis of an orientation judgment task, in which participants simply see an oriented bar on each trial, remember the bar’s orientation for a short period, and then report the orientation. Participants’ responses on one trial can depend on the orientation they saw in the previous trial. The dependence follows a Gaussian’s derivative function. Figure 1A shows a Gaussian function with its derivative, and Figure 1B shows the derivative modeling a range of different serial dependence patterns. The derivative captures three key features of the data. First, different changes in orientation between trials result in serial dependencies of different magnitude. The responsiveness of dependence is captured by the width of the derivative. Second, serial dependence can have a different magnitude. The magnitude is captured by the amplitude of the derivative. Finally, responses on the current trial can either be attracted towards or repulsed away from the orientation of the previous trial. The direction of the effect is captured with the sign of the amplitude. The direction of the effect–and the experimental manipulations that change that direction–are often critical to different theoretical interpretations of serial dependence.\n Figure 1: A) A Gaussian function and its derivative. B) The derivative captures how errors on the current trial can depend on how the relationship between the orientation seen in the current and previous trials. Positive values on the horizontal axis signify a clockwise difference and negative values a counterclockwise difference. Likewise, positive errors signify responses on the current trial which were clockwise to the true orientation, and negative errors are counterclockwise. When errors are in the same direction as the difference in orientations, the error is said to be attractive. Otherwise, the error is repulsive. Whether errors are attractive or repulsive is given by the sign of the derivative’s amplitude.  Although the Gaussian’s derivative adequately models the serial dependence between trials with similar orientations (less than 45 degree differences), the derivative fits poorly the dependencies following large changes. When sequential trials have a large orientation difference, the sign of the dependence often changes; small orientation differences can elicit an attractive dependence even while large differences are repulsive. These sign flips are called the peripheral bumps, and they are not captured by the Gaussian’s derivative. If the bumps are large enough, they can interpretations about the sign to of dependencies following small changes can be inverted (Figure 2). Unfortunately, noticing the peripheral bumps can be hard with sparse data. But even with sparse data, the width of the best-fitting derivative can help identify bumps. If the best-fitting derivative is abnormally wide (with peaks larger than approximately 35 degrees), then the derivative is tracking dependencies wider than it should. In that circumstance, it may be best to focus analyses on only the trials with smaller orientation differences.\n Figure 2: Misfits of the Gaussian’s derivative. The dots give hypothetical data. The data were generated with a function whose average is traced by the dashed line. The data were fit with a derivative of Guassian function, and the best-fitting derivative is shown with a solid line. The derivative does not match the data-generating function.  ","date":1570060800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653490997,"objectID":"514353eda7ac46c020d01c07dbeda527","permalink":"https://psadil.github.io/psadil/post/derivative-of-gaussian-for-serial-dependence/","publishdate":"2019-10-03T00:00:00Z","relpermalink":"/psadil/post/derivative-of-gaussian-for-serial-dependence/","section":"post","summary":"Cognitive experiments can require participants to complete hundreds of trials, but completing so many trials invariably alters participants’ behavior. Their behavior late in the experiment can depend on their behavior early in the experiment. Although such dependence can be an experimental confound, the dependence itself can provide clues about cognition. One simple kind of dependence occurs through learning; hundreds of trials provides participants ample practice. A more subtle dependence can emerge between sequential trials, an effect called serial dependence.","tags":[],"title":"derivative of gaussian for serial dependence","type":"post"},{"authors":null,"categories":["dissertation"],"content":"  Perceiving the world requires representing the world in neural tissue. A neuron is tuned to perceivable information when different values of that information cause the neuron to fire at a different rate. For example, most visual neurons are tuned to spatial location. The spatial tuning could be measured by placing a recording electrode in a neuron in a macaque’s visual cortex while the macaque fixated on the center of a computer monitor and a picture moved across that monitor. The electrode would report higher activity only when the picture was in certain parts of the macaque’s visual field. The function relating the position of the picture to the neuron’s activity is the tuning function. Such functions often resembles a bivariate Gaussian (Figure 1). To study these tuning functions is to study how these neurons represent the world.\n Figure 1: The bivariate Gaussian represents a neuron’s receptive field. The neuron will be most responsive to information that overlaps with the bright yellow regions. Since this the upper left portion of the box is slightly encompassed by the receptive field, the neuron’ might fire slightly more rapidly as compared to its baseline rate.  Sensory neurons are tuned to many other features such as orientation, color, pitch, direction of motion. Most neurons tuned to one visual feature are also tuned to spatial location, so understanding a neuron’s spatial can facilitate understanding its other sensitivities1. The tuning functions of neurons even have a special name, their receptive field. However, it is often unfeasible to record from individual neurons in humans, and instead only non-invasive neuroimaging methods are available. But these non-invasive methods have low spatial resolution. Even the relatively well spatially resolved technique of functional magnetic resonance imaging reflects the aggregated activity of 10e5 - 10e6 neurons.\nFortunately, the retinotopic arrangement of visual neurons facilitates relating the spatial tuning of a voxel2 to the receptive field of individual neurons. A retinotopic arrangement means that neighboring neurons are tuned to neighboring locations in the visual field; for example, neurons tuned to things in the fovea cluster together and neurons tuned to peripheral locations surround that cluster. This retinotopic arrangement implies that all neurons sampled by a voxel represent nearby regions in the visual environment. Referring to the neurons in a voxel as a population, the receptive field of a voxel is called a population receptive field. Studying population receptive fields alone cannot reveal how individual neurons contribute to a coherent perceptual experience, but studying them can reveal how the populations respond as a group.\nTo chart out all of the mountainous population receptive fields in visual cortex is called population receptive field mapping (Dumoulin and Wandell 2008). The receptive fields can be mapped by recording the activity of each voxel while a human participant is shown some visually salient movie. A mathematical model – such as a bivariate Gaussian – of the receptive field is assumed, and the data from each voxel are used to fit the parameters of that model. The specific images that are used will depend on which part of visual cortex is the focus of the experiment. A counterphasing black and white checkerboard might be close to optimal for primary visual cortex, but the checkerboard would only weakly stimulate neurons in higher level visual regions. To stimulate most of visual cortex, other researchers rely on more varied displays.\nReferences Dumoulin, Serge O, and Brian A Wandell. 2008. “Population Receptive Field Estimates in Human Visual Cortex.” Neuroimage 39 (2): 647–60.     e.g., if you want to understand how a neuron is tuned to color, it helps to know where to put the color↩︎\n  Voxels are the elements that hold data in magnetic resonance imaging. A voxel in a 3D image is analogous to a pixel in a 2D image; a voxel is a pixel with a volume.↩︎\n   ","date":1569542400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653490997,"objectID":"f822006066f1943ad98bba3440bca337","permalink":"https://psadil.github.io/psadil/post/population-receptive-field-mapping/","publishdate":"2019-09-27T00:00:00Z","relpermalink":"/psadil/post/population-receptive-field-mapping/","section":"post","summary":"Perceiving the world requires representing the world in neural tissue. A neuron is tuned to perceivable information when different values of that information cause the neuron to fire at a different rate. For example, most visual neurons are tuned to spatial location. The spatial tuning could be measured by placing a recording electrode in a neuron in a macaque’s visual cortex while the macaque fixated on the center of a computer monitor and a picture moved across that monitor.","tags":[],"title":"an overview of population receptive field mapping","type":"post"},{"authors":null,"categories":["dissertation"],"content":"  Objects in the visual environment move suddenly and erratically, and visual perception must be sensitive to the changes that are important. But each saccade and head tilt change the image imprinted on the retina, and to perceive every tremor ignores the stability of the visual environment; a desk will still look like a desk in a few seconds. The visual system must therefore balance the ability to detect subtle changes in the environment against the efficiency afforded by accurate predictions.\nThat the recent past influences current perception can be demonstrated easily. If you stare at Figure 1, you might observe that the Gabor has a bend immediately after changing orientations. The bend lasts for a moment, then straightens. But the bend is an illusion. While tracking Figure 1, the visual system allows for a momentary bias. Usefully, the bias is sensitive to experimental manipulation. Figure 2 shows the same Gabor with the same orientations, but the Gabor also moves. The movement largely eliminates the bending. The sensitivity of such biases to different experimental manipulations enables researchers to study how the visual system balances new information against the recent past.\n Figure 1: A Gabor alternates between two orientations.   Figure 2: A Gabor alternates between two orientations, appearing in a different location with each orientation.  A closely effect is called serial dependence. Serial dependence occurs when participants report the orientations of sequentially presented, tilted Gabors (Fischer and Whitney 2014). A visual mask to reduces the strong aftereffects present in Figures 1 and 2 [Figure 3]1. Even without the aftereffect, the perceptual-decision about one Gabor affects the perceptual-decision about the next; participants report orientations that consistently err toward the orientation of the most recently seen Gabor. Reports on the magnitude of the effect vary, but the error has an average maximum of less than a few degrees. However, serial dependence is affected by different manipulations than that the demonstration of Figures 1 and 2. For example, it appears insensitive to the location of the Gabors. This bias may therefore provide a unique way to study how current perception is not only biased by but toward the recent past.\n Figure 3: Differently oriented Gabors presented with interspersed noise masks..  However, it remains unclear whether serial dependence is a bias of perceptual or post-perceptual processes. That is, does serial dependence alter participants’ perception of the Gabors, or does it alter how they report the orientation? The sequential timing of each trial – in which participants respond in a designated period after seeing the Gabor – does not imply that participants decide on an orientation only after they have finished perceiving the Gabor. For example, a participant can make decisions before the response period, and they can adopt a biased response strategy even before seeing the Gabor. Where and when to delineate between perception and decision, or whether they can be delineated, depends on assumptions about the relationship between perception and decisions. A tool like the circular diffusion model can help make those assumptions explicit (Smith 2016).\nReferences Fischer, Jason, and David Whitney. 2014. “Serial Dependence in Visual Perception.” Nature Neuroscience 17 (5): 738.  Smith, Philip L. 2016. “Diffusion Theory of Decision Making in Continuous Report.” Psychological Review 123 (4): 425–51. https://doi.org/10.1037/rev0000023.     The timing and spacing of this figure does not quite match a typical experiment. For example, participants take a few seconds to respond, so the amount of time between Gabors in this figure is too short.↩︎\n   ","date":1568851200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653490997,"objectID":"c5a974fbde7de15e937400a107695bf0","permalink":"https://psadil.github.io/psadil/post/serial-dependence/","publishdate":"2019-09-19T00:00:00Z","relpermalink":"/psadil/post/serial-dependence/","section":"post","summary":"Objects in the visual environment move suddenly and erratically, and visual perception must be sensitive to the changes that are important. But each saccade and head tilt change the image imprinted on the retina, and to perceive every tremor ignores the stability of the visual environment; a desk will still look like a desk in a few seconds. The visual system must therefore balance the ability to detect subtle changes in the environment against the efficiency afforded by accurate predictions.","tags":[],"title":"Serial Dependence","type":"post"},{"authors":null,"categories":[],"content":"  Many cognitive experiments involve asking participants to answer questions that require circular responses (Figure 1). What was the color of the shape you just saw? In which direction was the arrow pointing? How tilted was the bar? The answers required by these questions differ fundamentally from the more common, categorical responses required to questions. Was the color green or red? Was the arrow pointing left or right? Was the bar tilted more than 45 degrees from vertical, between 45-90, or more than 90 degrees? In the continuous case, the experimenter looses the ability to classify responses as either correct or incorrect, and an analysis must consider participants’ degree of inaccuracy, their relative error. Circularity adds the additional complication that a response can only be erroneous up to a point; if a person responds that a vertical bar is 3 degrees offset from vertical on trial one and 359 degrees on trial two, the analysis must acknowledge that the average is close to truth. Although many models exist that describe how a participant will respond when the choice is binary, models of these are much more limited.\n Figure 1: Cognitive experiments often require participants to provide a circularly continuous response. A participant might be asked to study shapes of different colors, hold the colors of every shape in memory, and then report on the one shape’s color.  Smith (2016) present a new model of how participants provide circularly continuous responses, called the circular diffusion model. It is a model of the decision-making process, analyzing both the numerical value participants provided and how long it took them to provide a response. The model extends the drift diffusion model of binary decisions (Ratcliff 1978). Like the drift diffusion model, the circular diffusion model casts perceptual decisions as a stochastic process of evidence accumulation to a threshold; evidence is accumulated over time, and when enough evidence has been reached the process terminates in a motor behavior. The model is not concerned with how evidence accumulates, just that it does. In a working memory experiment, evidence might accumulate through repeated probes of memory. In a perceptual-decision task, each saccade might provide a different amount of evidence. In both cases, evidence grows at an average rate, and when there is enough evidence for a decision that decision is made. The amount of time required to reach that threshold of evidence is the response time. The circular diffusion model, therefore, proposes that the responses of rapid decisions which require circularly continuous responses can be modeled as a particle drifting in two dimensions out towards a circular boundary.\n Figure 2: A diffusing particle models perceptual decisions. In this example, wherever the particle first crosses the circle corresponds to the response, and the amount of time required to reach the edge is their response time.  Using the circular diffusion model affords researchers the same advantages conferred by using the standard drift diffusion model: the decision-making process can be decomposed into parameters of the model, and those parameters have psychologically meaningful values. For example, a participant might respond quickly, but that could either occur because they accumulate evidence rapidly or because they set a low threshold for evidence. There are three key parameters in the model: 1) the average direction the particle drifts (towards what decision are participants mostly accumulating evidence?), 2) the average rate at which the particle drifts (how quickly do participants accumulate evidence?), and 3) the radius of the circular boundary (how conservative are participants?). Estimating these parameters for participants across different conditions of an experiment enables the researcher to “measure” each of these psychological constructs given participants’ behavior.\nReferences Ratcliff, Roger. 1978. “A Theory of Memory Retrieval.” Psychological Review 85 (2): 59. https://doi.org/10.1037/0033-295X.85.2.59.  Smith, Philip L. 2016. “Diffusion Theory of Decision Making in Continuous Report.” Psychological Review 123 (4): 425–51. https://doi.org/10.1037/rev0000023.    ","date":1568246400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653490997,"objectID":"dd1e3fcd0adccfe6c280a06848e9d375","permalink":"https://psadil.github.io/psadil/post/circular-diffusion-model-of-response-times/","publishdate":"2019-09-12T00:00:00Z","relpermalink":"/psadil/post/circular-diffusion-model-of-response-times/","section":"post","summary":"Many cognitive experiments involve asking participants to answer questions that require circular responses (Figure 1). What was the color of the shape you just saw? In which direction was the arrow pointing? How tilted was the bar? The answers required by these questions differ fundamentally from the more common, categorical responses required to questions. Was the color green or red? Was the arrow pointing left or right? Was the bar tilted more than 45 degrees from vertical, between 45-90, or more than 90 degrees?","tags":["dissertation"],"title":"Circular Diffusion Model of Response Times","type":"post"},{"authors":null,"categories":["experiments"],"content":"  I’ve started trying out MATLAB’s OOP after mounting suspicion that the way I’d been coding experiments basically involved making something that looked and behaved like an object–but did so in a convoluted and inefficient way. See this post on eyetracking with PTB as proof.\nThis post is brief, and is about as well thought out as a github gist/gitlab snippet.\nThe two classes I’ll work with here is a Window class and a Tracker class. The window class has 3 methods. The first constructor method exists just to create the object. The constructed object will have a few default properties of the class. The second method is open, which (can you guess?) calls the PTB functions to open an onscreen window. The open method is fancier than it needs to be for this post (note the PsychImaging configuration, and the optional debugLevel flag). The final window method is the desctructor. The destructor method is one of the advantages of leaning on MATLAB’s OOP syntax. That method will get called whenever the Window object’s lifecycle has ended (which might happen from explicit deletion of the object, closing MATLAB, the object is no longer referenced in the call stack, etc).\nThe second class is the Tracker class, which interfaces with Eyelink. The Window class is only present here because Eyelink needs an open window to run calibration. There are five Tracker methods, but they are either analogous to the Window objects methods (constructor, destructor) or were largely presented in the previous post.\nWindow Object  classdef Window \u0026lt; handle % Window handles opening and closing of screen properties (Constant) screenNumber = 0 % background color of screen background = GrayIndex(Window.screenNumber) end properties pointer winRect end methods function obj = Window() end function open(obj, skipsynctests, debuglevel) PsychImaging(\u0026#39;PrepareConfiguration\u0026#39;); PsychImaging(\u0026#39;AddTask\u0026#39;, \u0026#39;General\u0026#39;, \u0026#39;FloatingPoint16Bit\u0026#39;); Screen(\u0026#39;Preference\u0026#39;, \u0026#39;SkipSyncTests\u0026#39;, skipsynctests); switch debuglevel % no debug. run as usual, without listening to keyboard input % and also hiding the cursor case 0 ListenChar(-1); HideCursor; [obj.pointer, obj.winRect] = ... PsychImaging(\u0026#39;OpenWindow\u0026#39;, obj.screenNumber, obj.background); % light debug: still open fullscreen window, but keep keyboard input case 1 [obj.pointer, obj.winRect] = ... PsychImaging(\u0026#39;OpenWindow\u0026#39;, obj.screenNumber, obj.background); % full debug: only open transparent window case 10 PsychDebugWindowConfiguration(0, .5) [obj.pointer, obj.winRect] = ... PsychImaging(\u0026#39;OpenWindow\u0026#39;, obj.screenNumber, obj.background); end % Turn on blendfunction for antialiasing of drawing dots Screen(\u0026#39;BlendFunction\u0026#39;, obj.pointer, \u0026#39;GL_SRC_ALPHA\u0026#39;, \u0026#39;GL_ONE_MINUS_SRC_ALPHA\u0026#39;); topPriorityLevel = MaxPriority(obj.pointer); Priority(topPriorityLevel); end % will auto-close open windows and return keyboard control when % this object is deleted function delete(obj) %#ok\u0026lt;INUSD\u0026gt; ListenChar(0); Priority(0); sca; end end end   Tracker Object The tracker object will mostly do what it did in the previous post. Same functionality, but the syntax is much cleaner than the heavy use of switch/case conditionals.\nclassdef Tracker \u0026lt; handle properties % flag to be called in scripts which enable turning on or off the tracker % in an experiment (e.g., when debug mode is on) using_tracker logical = false % name of the write. must follow eyelink conventions. alphanumeric only, no % more than 8 characters filename char = \u0026#39;\u0026#39; % eyelink object structure. stores many relevant parameters el end methods function obj = Tracker(using_tracker, filename, window) obj.using_tracker = using_tracker; obj.filename = filename; % run calibration for tracker (see method below) calibrate(obj, window); end function calibrate(obj, window) if obj.using_tracker % Provide Eyelink with details about the graphics environment % and perform some initializations. The information is returned % in a structure that also contains useful defaults % and control codes (e.g. tracker state bit and Eyelink key values). obj.el = EyelinkInitDefaults(window.pointer); if ~EyelinkInit(0, 1) error(\u0026#39;\\n Eyelink Init aborted \\n\u0026#39;); end %Reduce FOV for calibration and validation. Helpful when the % the stimulus is only in the center of the screen, or at places % like the fMRI scanner at UMass where the eyes have a lot in front % of them Eyelink(\u0026#39;Command\u0026#39;,\u0026#39;calibration_area_proportion = 0.5 0.5\u0026#39;); Eyelink(\u0026#39;Command\u0026#39;,\u0026#39;validation_area_proportion = 0.5 0.5\u0026#39;); % open file to record data to status = Eyelink(\u0026#39;Openfile\u0026#39;, obj.filename); if status ~= 0 error(\u0026#39;\\n Eyelink Init aborted \\n\u0026#39;); end % Setting the proper recording resolution, proper calibration type, % as well as the data file content; Eyelink(\u0026#39;Command\u0026#39;,\u0026#39;screen_pixel_coords = %ld %ld %ld %ld\u0026#39;, 0, 0, window.winRect(3)-1, window.winRect(4)-1); Eyelink(\u0026#39;message\u0026#39;, \u0026#39;DISPLAY_COORDS %ld %ld %ld %ld\u0026#39;, 0, 0, window.winRect(3)-1, window.winRect(4)-1); % set calibration type to 5 point. Eyelink(\u0026#39;Command\u0026#39;, \u0026#39;calibration_type = HV5\u0026#39;); % set EDF file contents using the file_sample_data and % file-event_filter commands % set link data thtough link_sample_data and link_event_filter Eyelink(\u0026#39;Command\u0026#39;, \u0026#39;file_event_filter = RIGHT,FIXATION,SACCADE,BLINK,MESSAGE,BUTTON,INPUT\u0026#39;); Eyelink(\u0026#39;Command\u0026#39;, \u0026#39;link_event_filter = RIGHT,FIXATION,SACCADE,BLINK,MESSAGE,BUTTON,INPUT\u0026#39;); % check the software version % add \u0026quot;HTARGET\u0026quot; to record possible target data for EyeLink Remote Eyelink(\u0026#39;command\u0026#39;, \u0026#39;file_sample_data = RIGHT,GAZE,HREF,GAZERES,AREA,HTARGET,STATUS,INPUT\u0026#39;); Eyelink(\u0026#39;command\u0026#39;, \u0026#39;link_sample_data = RIGHT,GAZE,HREF,GAZERES,AREA,HTARGET,STATUS,INPUT\u0026#39;); % make sure we\u0026#39;re still connected. if Eyelink(\u0026#39;IsConnected\u0026#39;)~=1 error(\u0026#39;\\n Eyelink Init aborted \\n\u0026#39;); end % set sample rate in camera setup screen Eyelink(\u0026#39;Command\u0026#39;, \u0026#39;sample_rate = %d\u0026#39;, 1000); % opens up main calibration scheme EyelinkDoTrackerSetup(obj.el); end end function status = eyelink(obj, varargin) % calls main Eyelink routines only when % this tracker object property using_tracker==true. status = []; if obj.using_tracker if nargin==2 % construct calls to eyelink that don\u0026#39;t output any % status if strcmp(varargin{1}, \u0026#39;StopRecording\u0026#39;) || ... strcmp(varargin{1}, \u0026#39;Shutdown\u0026#39;) ||... strcmp(varargin{1}, \u0026#39;SetOfflineMode\u0026#39;) % magic happens here, where the variable argument input % is expanded an repassed through to Eyelink() Eyelink(varargin{:}); else status = Eyelink(varargin{:}); end % all calls to Eyelink that have more than two inputs (e.g., the % name of a function with some parameters to that function) return % some status else status = Eyelink(varargin{:}); end end end % starts up the eyelink machine. call this once the start of each % experiment. could modify function to also draw something special % to the screen (e.g., a background image). this might be the kind % of function to modify if you wanted to draw trial-by-trial material % to the eyetracking computer function startup(obj) % Must be offline to draw to EyeLink screen obj.eyelink(\u0026#39;SetOfflineMode\u0026#39;); % clear tracker display and draw background img to host pc obj.eyelink(\u0026#39;Command\u0026#39;, \u0026#39;clear_screen 0\u0026#39;); % draw simple fixation cross as later reference obj.eyelink(\u0026#39;command\u0026#39;, \u0026#39;draw_cross %d %d\u0026#39;, 1920/2, 1080/2); % give image transfer time to finish WaitSecs(0.1); end % destructor function will get called whenever tracker object is deleted (e.g., % this function is automatically called when MATLAB closes, meaning you can\u0026#39;t % forget to close the file connection with the tracker computer). function delete(obj) % waitsecs occur because the filetransfer often takes a moment, and moving % on too quickly will result in an error % End of Experiment; close the file first % close graphics window, close data file and shut down tracker obj.eyelink(\u0026#39;StopRecording\u0026#39;); WaitSecs(0.1); % Slack to let stop definitely happen obj.eyelink(\u0026#39;SetOfflineMode\u0026#39;); obj.eyelink(\u0026#39;CloseFile\u0026#39;); WaitSecs(0.1); obj.eyelink(\u0026#39;ReceiveFile\u0026#39;, obj.filename, fullfile(pwd,\u0026#39;events\u0026#39;), 1); WaitSecs(0.2); obj.eyelink(\u0026#39;Shutdown\u0026#39;); end end end   Run the calibration (and use the tracker) Putting this together, the following script starts calibration, and outlines how this tracker could be used in an experiment.\n %% input % ------------------ skipsynctests = 2; debuglevel = 0; using_tracker = true; %% setup % ------------------ % boilerplate setup PsychDefaultSetup(2); % initialize window window = Window(); % open that window open(window, skipsynctests, debuglevel) % Initialize tracker object tracker = Tracker(using_tracker, \u0026#39;OOPDEMO.edf\u0026#39;, window); % run calibration tracker.startup(); % Let Eyelink know that the experiment starts now tracker.eyelink(\u0026#39;message\u0026#39;, \u0026#39;SYNCTIME\u0026#39;); %% Experiment/trial code % ------------------ % note that we should not need to wait to start recording, % given that the stimulus will always be drawn a bit later % (determined by how often phase changes occur) tracker.eyelink(\u0026#39;StartRecording\u0026#39;); % trial/experiment happens here ... tracker.eyelink(\u0026#39;StopRecording\u0026#39;); % Wait moment to ensure that tracker is definitely finished with the last few samples WaitSecs(0.001); %% Cleanup % ------------------ % closes connection to Eyelink system, saves file delete(tracker); % closes window, restores keyboard input delete(window);  What’s nice about this syntax (as before) is that only very minimal changes are required you don’t want to call the Eyelink functions (e.g., if you’re testing on a computer that doesn’t have the Eyelink system connected, or you’re debugging other parts of the experiment). By changing just the input, the Eyelink functions won’t be called.\n using_tracker = false; % all the rest as above % ...  Summary There’s not much to summarize because I haven’t explained much! Again, this post is largely just an attempt to revise what I now think is a poor implementation, presented in an earlier post.\n ","date":1558742400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653490997,"objectID":"ec91bd2dc1c2102db4ddc65ca992d76c","permalink":"https://psadil.github.io/psadil/post/eyetracking-in-psychtoolbox-oop/","publishdate":"2019-05-25T00:00:00Z","relpermalink":"/psadil/post/eyetracking-in-psychtoolbox-oop/","section":"post","summary":"I’ve started trying out MATLAB’s OOP after mounting suspicion that the way I’d been coding experiments basically involved making something that looked and behaved like an object–but did so in a convoluted and inefficient way. See this post on eyetracking with PTB as proof.\nThis post is brief, and is about as well thought out as a github gist/gitlab snippet.\nThe two classes I’ll work with here is a Window class and a Tracker class.","tags":["eyelink","matlab","psychtoolbox"],"title":"eyetracking with eyelink in psychtoolbox, now with oop","type":"post"},{"authors":["Patrick Sadil","Rosemary A Cowell","David E Huber"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653490997,"objectID":"489852e89263c425d0fd5fe70c2cddb6","permalink":"https://psadil.github.io/psadil/publication/sadil-2019-hierarchical/","publishdate":"2020-01-13T13:30:43.816727Z","relpermalink":"/psadil/publication/sadil-2019-hierarchical/","section":"publication","summary":"State trace analyses assess the latent dimensionality of a cognitive process by asking whether the means of two dependent variables conform to a monotonic function across a set of conditions. Using an assumption of independence between the measures, recently proposed statistical tests address bivariate measurement error, allowing both frequentist and Bayesian analyses of monotonicity (e.g., Davis-Stober, Morey, Gretton, \u0026 Heathcote, 2016; Kalish, Dunn, Burdakov, \u0026 Sysoev, 2016). However, statistical inference can be biased by unacknowledged dependencies between measures, particularly when the data are insufficient to overwhelm an incorrect prior assumption of independence. To address this limitation, we developed a hierarchical Bayesian model that explicitly models the separate roles of subject, item, and trial-level dependencies between two measures. Assessment of monotonicity is then performed by fitting separate models that do or do not allow a non-monotonic relation between the condition effects (i.e., same versus different rank orders). The Widely Applicable Information Criterion (WAIC) and Pseudo Bayesian Model Averaging – both cross validation measures of model fit – are used for model comparison, providing an inferential conclusion regarding the dimensionality of the latent psychological space. We validated this new state trace analysis technique using model recovery simulation studies, which assumed different ground truths regarding monotonicity and the direction/magnitude of the subject- and trial-level dependence. We also provide an example application of this new technique to a visual object learning study that compared performance on a visual retrieval task (forced choice part recognition) versus a verbal retrieval task (cued recall).","tags":null,"title":"A hierarchical Bayesian state trace analysis for assessing monotonicity while factoring out subject, item, and trial level dependencies","type":"publication"},{"authors":["Rosemary A Cowell","Morgan D Barense","Patrick Sadil"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653490997,"objectID":"aece8c8a0a3ad179d053868ed2bc2983","permalink":"https://psadil.github.io/psadil/publication/cowell-2019-roadmap/","publishdate":"2020-01-13T13:30:43.816285Z","relpermalink":"/psadil/publication/cowell-2019-roadmap/","section":"publication","summary":"Thanks to patients Phineas Gage and Henry Molaison, we have long known that behavioral control depends on the frontal lobes, whereas declarative memory depends on the medial temporal lobes (MTL). For decades, cognitive functions—behavioral control, declarative memory—have served as labels for characterizing the division of labor in cortex. This approach has made enormous contributions to understanding how the brain enables the mind, providing a systems-level explanation of brain function that constrains lower-level investigations of neural mechanism. Today, the approach has evolved such that functional labels are often applied to brain networks rather than focal brain regions. Furthermore, the labels have diversified to include both broadly-defined cognitive functions (declarative memory, visual perception) and more circumscribed mental processes (recollection, familiarity, priming). We ask whether a process—a high-level mental phenomenon corresponding to an introspectively-identifiable cognitive event—is the most productive label for dissecting memory. For example, recollection conflates a neurocomputational operation (pattern completion-based retrieval) with a class of representational content (associative, high-dimensional memories). Because a full theory of memory must identify operations and representations separately, and specify how they interact, we argue that processes like recollection constitute inadequate labels for characterizing neural mechanisms. Instead, we advocate considering the component operations and representations of processes like recollection in isolation. For the organization of memory, the evidence suggests that pattern completion is recapitulated widely across the ventral visual stream and MTL, but the division of labor between sites within this pathway can be explained by representational content.","tags":null,"title":"A roadmap for understanding memory: Decomposing cognitive processes into operations and representations","type":"publication"},{"authors":["Patrick Sadil","Kevin W Potter","David E Huber","Rosemary A Cowell"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653490997,"objectID":"32ccf7d5830acff4e57d7cb313766272","permalink":"https://psadil.github.io/psadil/publication/sadil-2019-connecting/","publishdate":"2020-01-13T13:30:43.817017Z","relpermalink":"/psadil/publication/sadil-2019-connecting/","section":"publication","summary":"Knowing the identity of an object can powerfully alter perception. Visual demonstrations of this—such as Gregory’s (1970) hidden Dalmatian—affirm the existence of both top-down and bottom-up processing. We consider a third processing pathway: lateral connections between the parts of an object. Lateral associations are assumed by theories of object processing and hierarchical theories of memory, but little evidence attests to them. If they exist, their effects should be observable even in the absence of object identity knowledge. We employed Continuous Flash Suppression (CFS) while participants studied object images, such that visual details were learned without explicit object identification. At test, lateral associations were probed using a part-to-part matching task. We also tested whether part-whole links were facilitated by prior study using a part-naming task, and included another study condition (Word), in which participants saw only an object’s written name. The key question was whether CFS study (which provided visual information without identity) would better support part-to-part matching (via lateral associations) whereas Word study (which provided identity without the correct visual form) would better support part-naming (via top-down processing). The predicted dissociation was found and confirmed by state-trace analyses. Thus, lateral part-to-part associations were learned and retrieved independently of object identity representations. This establishes novel links between perception and memory, demonstrating that (a) lateral associations at lower levels of the object identification hierarchy exist and contribute to object processing and (b) these associations are learned via rapid, episodic-like mechanisms previously observed for the high-level, arbitrary relations comprising episodic memories. ","tags":null,"title":"Connecting the dots without top-down knowledge: Evidence for rapidly-learned low-level associations that are independent of object identity.","type":"publication"},{"authors":null,"categories":["comps"],"content":"  This post attempts to provide a brief intro to importance sampling, presenting a relatively simple situation in which importance sampling enables a speedup in Monte Carlo by reducing the variance of the estimate.\nA lot of the material will be an effort to remember material that was taught to be in a Stochastic Processes and Applications class. This post should be though of as my notes on the course notes, with supplementation by https://www.statlect.com/asymptotic-theory/importance-sampling. That is, it can safely be assumed that any content comes from those notes, excluding errors (which will be due to my translation)1.\nWhat is importance sampling? Importance sampling comes up in the context of Monte Carlo, (MC) applications as a technique for reducing the variability in the MC estimator. As a reminder, MC can be used to estimate the expected value, \\(\\mu\\), of a random variable, \\(X\\).\n\\[ \\mu = \\mathbb{E}[h(x)] = \\int h(x)p_X(x)\\,dx \\]\nThe idea of Monte Carlo is that this expectation can be estimated by drawing \\(S\\) samples from the distribution \\(p_X\\), where \\(X \\sim p_X\\)\n\\[ \\hat{\\mu} =\\frac{1}{S}\\sum_{s=1}^S h(x_s) \\]\nwhere the subscript on \\(x\\) implies the \\(s^th\\) draw of \\(X\\), and the hat over \\(\\mu\\) indicates that the result is an estimate. Note that, since we’re starting from that equation, it is assumed that there is some way to produce random samples from the distribution \\(p_X\\), and that the function, \\(h\\) is calculable for any \\(X\\). Also, \\(h\\) might be something as simple as \\(h(x) = x\\) if the expectation should correspond to the mean of \\(x\\)].\nThis is a powerful idea, though a general downside is that some \\(\\mu\\) require many samples to obtain a reasonable estimate. The MC Standard Error (MCSE) provides an estimate of the approximation error, which is \\(\\frac{1}{n} Var(h(X))\\). This provides a metric by which to gauge different algorithms. In particular, if all else is equal, the estimator with lower variance will can obtain a given level of precision with a lower \\(S\\).\nThe basic idea with Importance Sampling (IS) is to draw samples from some alternative distribution, \\(p_Y\\), which has the same support as \\(p_X\\), then reweight those samples in accordance with the difference between \\(p_X\\) and \\(p_Y\\).\n\\[ \\begin{aligned} \\mathbb{E}[h(x)] \u0026amp; = \\int h(x)p_X(x) \\,dx \u0026amp; \\textrm{definition of expectation} \\\\ \u0026amp; = \\int h(x)\\frac{p_X(x)}{p_Y(x)}p_Y(x) \\,dx \u0026amp; \\textrm{multiplication by 1, assuming same support} \\\\ \u0026amp; \\int h(y)\\frac{p_X(y)}{p_Y(y)}p_Y(y) \\,dy \u0026amp; \\textrm{assuming same support} \\\\ \u0026amp; = \\mathbb{E} \\left[h(y)\\frac{p_X(y)}{p_Y(y)} \\right] \u0026amp; \\textrm{our new importance sampling estimator} \\end{aligned} \\]\nRecognize that there will often not be a single unique \\(p_Y\\). The goal is to find a \\(p_Y\\) that results in lower MCSE. The MCSE for the importance sampling estimator is \\(\\frac{1}{n}Var\\left[h(y)\\frac{p_X(y)}{p_Y(y)} \\right]\\). That will be used to gain an intuition for how to choose a useful \\(p_Y\\).\n Why does importance sampling work? One way to think about importance sampling is that, if we could sample from \\(h(y)\\frac{p_X(y)}{p_Y(y)}\\) such that the result were constant, the variance would be 0, and so we’d only need a single sample. To see why, note that, for a constant \\(c\\)\n\\[ \\begin{aligned} h(y)\\frac{p_X(y)}{p_Y(y)} \u0026amp; = c \\\\ \\implies p_Y(y)c \u0026amp; = h(y)p_X(y) \\\\ \\implies p_Y(y) \u0026amp; \\propto h(y)p_X(y) \\\\ \\end{aligned} \\]\nThat is, \\(h(y)\\frac{p_X(y)}{p_Y(y)}\\) will be constant whenever \\(p_Y(y)\\) is proportional to \\(h(y)p_X(x)\\).\n\\[ \\begin{aligned} p_Y(y) \u0026amp; = \\frac{h(y)p_X(y)}{\\int h(y)p_X(y)\\,dy} \\\\ \\implies p_Y(y) \u0026amp; = \\frac{h(y)p_X(y)}{\\mathbb{E}[h(X)]} \\\\ \u0026amp; = \\frac{h(y)p_X(y)}{\\mu} \u0026amp; \\textrm {definition of }\\mu \\end{aligned} \\]\nPlugging this distribution into the IS estimator\n\\[ \\begin{aligned} \\frac{1}{S} \\sum_{s=1}^S \\frac{h(Y_s)p_X(Y_s)}{p_Y(Y_s)} \u0026amp; = \\frac{1}{S} \\sum_{s=1}^S \\frac{h(Y_s)p_X(Y_s)}{\\frac{h(Y_s)p_X(Y_s)}{\\mathbb{E}[h(X_s)]}} \\\\ \u0026amp; = \\frac{1}{S} S\\mu \\\\ \u0026amp; = \\mu \\end{aligned} \\]\nSo, regardless of \\(S\\), the resulting estimator is always \\(\\mu\\).\nThat’s almost useful, but this means that to get an optimal \\(p_Y\\) we need to know \\(\\mathbb{E}[h(X)]\\), which is by definition the \\(\\mu\\) that we’re trying to estimate. Still, knowing what is optimal provides the intuition for why importance sampling works.\nThere are two ideas going on here. First, the optimal \\(p_Y\\) is one which places higher density on regions where \\(h(X)\\) is high, as compared to \\(p_X\\). Those “important” values are the ones that will determine the result of \\(h(x)\\), so those are the ones that need to be altered the most (going from \\(p_X\\) to \\(p_Y\\)). Then, the second idea is that when averaging, we must account for the oversampling of these points – downweighting them by however much they were oversampled, which is the ratio \\(\\frac{p_X(y)}{p_Y(y)}\\).\n Using IS to reduce variance Here’s an example of this working out. The value we’re trying to estimate will be, for \\(X \\sim N(0,1)\\)\n\\[ \\mu = \\int \\phi(x-4)p_X(x)\\,dx \\]\nwhere \\(\\phi\\) is the standard normal density function. This \\(h\\) is such that only values near 4 provide much contribution to the average.\nset.seed(1234) hx \u0026lt;- function(x) { return(dnorm(x - 4)) } x \u0026lt;- seq(-5, 10, length.out = 1000) plot(x, hx(x), type = \u0026quot;l\u0026quot;) points(x, dnorm(x), col = \u0026quot;blue\u0026quot;, type = \u0026quot;l\u0026quot;)  Figure 1: h(x) (black) has mostly 0 around where p_X (blue) has high density. The mismatch of these distributions will contribute to unnecessarily high variance of the estimator.  However, \\(X\\) will only rarely contain values that are near 4. Hence, the variance of the estimation will be relatively high. We can get an explicit formula for the variance.\n\\[ Var(h(x)) = \\mathbb{E}[h(x)^2] - \\mathbb{E}[h(x)]^2 \\]\nA formula that involves calculating the expected value of this function\n\\[ \\begin{aligned} \\mathbb{E}[h(x)] \u0026amp; = \\int_{-\\infty}^{\\infty} \\left(\\frac{\\exp(- \\frac{x^2}{2})}{\\sqrt{2\\pi}} \\right)\\left(\\frac{\\exp(- \\frac{x^2}{2})}{\\sqrt{2\\pi}} \\right) dx \\\\ \u0026amp; = \\int_{-\\infty}^{\\infty} \\frac{\\exp (- x^2 + 4x - 8 )}{2\\pi} dx \\\\ \u0026amp; = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} \\exp (- x^2 + 4x - 8 ) dx\\\\ \u0026amp; = \\frac{1}{2\\pi} \\sqrt{\\pi}\\exp \\left(\\frac{4^2}{4}-8 \\right) \u0026amp; \\textrm{en.wikipedia.org/wiki/Gaussian_function} \\\\ \u0026amp; = \\frac{1}{2 \\exp(4) \\sqrt{\\pi}} \\end{aligned} \\]\nWhich we’ll save for now to use later\nmu \u0026lt;- 1/(2 * exp(4) * sqrt(pi)) mu ## [1] 0.005166746 Returning to the variance calculation\n\\[ \\begin{aligned} Var(h(x)) \u0026amp; = \\left[ \\int_{-\\infty}^{\\infty} \\left(\\frac{\\exp(- \\frac{(x-2)^2}{2})}{\\sqrt{2\\pi}} \\right)^2 \\left(\\frac{\\exp(- \\frac{x^2}{2})}{\\sqrt{2\\pi}} \\right) \\,dx \\right] - \\mu^2 \\\\ \u0026amp; = \\frac{1}{2\\sqrt{2}\\pi^{3/2}} \\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{3}{2}x^2+8x-16 \\right) \\,dx - \\mu^2 \\\\ \u0026amp; = \\frac{1}{2\\sqrt{2}\\pi^{3/2}} \\sqrt{\\frac{\\pi}{3/2}}\\exp \\left(\\frac{8^2}{6} -16 \\right) \\\\ \u0026amp; = \\frac{1}{2 \\pi \\sqrt{3} \\exp(16/3)} - \\mu^2 \\end{aligned} \\]\n1/(2 * pi * sqrt(3) * exp(16/3)) - mu^2 ## [1] 0.0004169361 Standard MC estimate is accurate, but with relatively high variance Using an MC estimate,\nx \u0026lt;- rnorm(1e+06) y \u0026lt;- hx(x) var(y) ## [1] 0.0004270083 Note also that the estimate (our target), is also accurate\nmean(y) - mu ## [1] 4.582938e-05  Using a distribution that simply matches h(x) is also not-so-great Now, let’s instead construct and IS estimator. One intuitive (but by no means optimal) attempt would be to use \\(Y \\sim N(4,1)\\), a distribution that matches with \\(h(x)\\) perfectly. Indeed, that will provide an accurate answer\ny \u0026lt;- rnorm(1e+06, mean = 4) h \u0026lt;- hx(y) IS \u0026lt;- h * dnorm(y, 0, 1)/dnorm(y, 4, 1) mean(IS) ## [1] 0.005194268 But, it turns out that the variance is about the same as before.\nvar(IS) ## [1] 0.0004204215  The proposal distribution needs to be tuned to both p_X and h(x) This is a somewhat subtle point of the derivation provided above. We don’t just want a distribution that will be highest here \\(h(x)\\) is high. Instead, what we actually need is a distribution that will be highest when \\(h(x)p_X(x)\\) is high. That will be exactly where the two distributions intersect, at 2.\nx \u0026lt;- seq(-5, 10, length.out = 1000) plot(x, hx(x), type = \u0026quot;l\u0026quot;) points(x, dnorm(x), col = \u0026quot;blue\u0026quot;, type = \u0026quot;l\u0026quot;) abline(v = 2)  Figure 2: Same as above, but with line demonstrating intersection at x=2  y \u0026lt;- rnorm(1e+06, mean = 2) h \u0026lt;- hx(y) IS \u0026lt;- h * dnorm(y, 0)/dnorm(y, 2) mean(IS) ## [1] 0.005165774 var(IS) ## [1] 4.131799e-06 The answer is still accurate, but the variance has been reduced by many factors. This means that a desired level of MCSE could be achieved with many fewer samples.\nOne final demonstration, remember that \\(h(x)p_X(x)\\) describes a distribution. Hence it would be a mistake to try a \\(p_Y\\) that placed all of the density around that point of intersection. For example, let’s try \\(Y \\sim N(2,0.1)\\). Although that is centered on the region that contributes the largest values to the expectation, the largest values alone do not define the expectation; too much of the tails of \\(h(x)\\) are not included. Using this results is the worst variance.\ny \u0026lt;- rnorm(1e+06, mean = 2, 0.1) h \u0026lt;- hx(y) IS \u0026lt;- h * dnorm(y, 0)/dnorm(y, 2, 0.1) mean(IS) ## [1] 0.002601838 var(IS) ## [1] 0.006540712   Other References  integral of Gaussian Function course notes statlect    this page is mostly a study page for upcoming comprehensive exams↩︎\n   ","date":1541808e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653490997,"objectID":"ffe423688d255c6db762ff940a720600","permalink":"https://psadil.github.io/psadil/post/importance-sampling/","publishdate":"2018-11-10T00:00:00Z","relpermalink":"/psadil/post/importance-sampling/","section":"post","summary":"This post attempts to provide a brief intro to importance sampling, presenting a relatively simple situation in which importance sampling enables a speedup in Monte Carlo by reducing the variance of the estimate.\nA lot of the material will be an effort to remember material that was taught to be in a Stochastic Processes and Applications class. This post should be though of as my notes on the course notes, with supplementation by https://www.","tags":["MC"],"title":"Basic Importance Sampling for Variance Reduction","type":"post"},{"authors":null,"categories":["experiments"],"content":"  It may be the case that Amazon Mechanical Turk WorkerIDs are not anonymous. Lease et al., 2013 describe at length how personally identifying information may be exposed when a researcher shares WorkerIDs. It is unclear to me the extent to which Amazon constructs their WorkerIDs at present, given that one of their striking demonstrations did not apply to my WorkerID. That is, they describe simply googling the WorkerID and receiving a picture of the participant, along with their full name. My WorkerID turn up nothing. Though, I have only been a worker on MTurk for a short while, so maybe I’ve been lucky and my ID has just not yet been shared widely.\nRegardless, providing extra anonymity to participants isn’t too much trouble. This post serves as documentation for a brief script that takes a sqlite database produced by running an experiment in jspsych + psiturk and replaces all instances of the WorkerID with a more secure code.\nThe script relies on five R libraries\nmagrittr, for ease of writing dplyr, through (dbplyr)[cran.r-project.org/web/packages/dbplyr/vignettes/dbplyr.html], serves as the way to interface with the sqlite database openssl constructs a more secure identifier for each participant that can still be used to cross reference them across studies stringr does the work of replacing instances of the WorkerID with the more secure code docopt, wraps up the Rscript such that it can be called from the command line (in an environment in which Rscript is the name of a function. i.e., may be Rscript.exe in Windows powershell)  #!/usr/bin/env Rscript # Anonymize participants database NOTE: always overrides the file --outfile library(docopt) doc \u0026lt;- \u0026quot;Usage: anonymize_db.R [-i DBNAME] KEY OUTFILE anonymize_db.R -h Options: -i --infile DBNAME sqlite database filename from which to read [default: participants_raw.db] -t --table TABLE name of table within database to anonymize [default: participants] -h --help show this help text Arguments: KEY key to salt WorkerIDs for extra security OUTFILE sqlite database filename to write\u0026quot; opt \u0026lt;- docopt(doc) library(magrittr) library(dplyr) library(stringr) library(openssl) db \u0026lt;- dplyr::src_sqlite(opt$infile) %\u0026gt;% dplyr::tbl(opt$table) %\u0026gt;% dplyr::collect() %\u0026gt;% dplyr::mutate(uniqueid = stringr::str_replace(uniqueid, workerid, openssl::sha256(workerid, key = opt$KEY)), datastring = dplyr::case_when(is.na(datastring) ~ datastring, TRUE ~ stringr::str_replace_all(datastring, workerid, openssl::sha256(workerid, key = opt$KEY))), workerid = openssl::sha256(workerid, key = opt$KEY)) message(paste0(\u0026quot;read raw database: \u0026quot;, opt$infile)) con \u0026lt;- DBI::dbConnect(RSQLite::SQLite(), opt$OUTFILE) dplyr::copy_to(con, db, opt$table, temporary = FALSE, indexes = list(\u0026quot;uniqueid\u0026quot;), overwrite = TRUE) DBI::dbDisconnect(con) message(paste0(\u0026quot;wrote anonymized database: \u0026quot;, opt$OUTFILE)) message(paste0(\u0026quot;Store your KEY securely if you want the same WorkerIDs to create the same HMACs!\u0026quot;)) As stated in the initial string of this script, a typical call might be\nanonymize_db.R longandsecurelystoredsalt participants.db\nwhich will read in the sqlite database participants_raw.db (default for –infile), convert all instances of WorkerID into a hash-digest with the sha256 algorithm, and store the result in a new sqlite database called participants.db.\nThe general workflow would be to include in your .gitignore the raw database output by psiturk. That way, the raw database is never uploaded into any repository. Then, when you are ready to host the experiment, you pull your repository as usual. As an extra step, you will now need to separately move around your raw database such that when you run the next experiment psiturk will know which workers have already participated. After collecting data, retrieve the database and run this anonymization script on it. The newly created database can then be bundled with your repository.\nThis is a bit of extra work (i.e., you must manually send the database, retrieve the database, then anonymize it). However, the whole point is to avoid making it easy to download something with potentially identifying information.\nGotchas This function will overwrite any database of the same name as OUTFILE. Though, that’s often not an issue. If you’ve anonymized a database (call the result participants.db), added new participants to the same raw database, and then anonymize the raw database again, those participants that were anonymized in the first round will be re-anonymized and included in the new result.\n If you want this function to convert a given WorkerID into a consistent code, you’ll need to call it with the same value for KEY.\n It would be more secure to use a salt of random length for each participant separately.\n   ","date":15336e5,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653490997,"objectID":"c041368f7106e7af89f967b5a65bb03b","permalink":"https://psadil.github.io/psadil/post/anonymizing-mtruk-worker-ids/","publishdate":"2018-08-07T00:00:00Z","relpermalink":"/psadil/post/anonymizing-mtruk-worker-ids/","section":"post","summary":"It may be the case that Amazon Mechanical Turk WorkerIDs are not anonymous. Lease et al., 2013 describe at length how personally identifying information may be exposed when a researcher shares WorkerIDs. It is unclear to me the extent to which Amazon constructs their WorkerIDs at present, given that one of their striking demonstrations did not apply to my WorkerID. That is, they describe simply googling the WorkerID and receiving a picture of the participant, along with their full name.","tags":["mturk","R","psiturk"],"title":"Anonymizing MTurk WorkerIDs","type":"post"},{"authors":null,"categories":["experiments"],"content":"  UPDATE: I now think that the examples I’ve presented here obscure the interface with Eyelink. Much cleaner to use MATLAB’s object oriented programming. This is covered in another post.\nThis post is designed as minimal documentation for using the Eyelink software at the UMass Amherst hMRC. The goals are very modest\nProvide sample Psychtoolbox (PTB) and MATLAB code for integrating eyelink Explain a few parameters that you might want to change in your experiment  The main audience includes members of the cMAP and CEMNL labs at UMass, but other users of the hMRC may also benefit. This post includes various lines of code throughout this post, but the full files can be downloaded from the links at the bottom. Many of those links are private and will only work if you are a member of one of those labs.\nNOTE: This post is not designed to be a full introduction to the Eyelink toolbox within PTB. I’m not qualified to give a detailed tutorial. These are just a few bits of code that I have found useful. But, my needs have so far been really simple (i.e., make a record of where the eyes were during each run so that runs can be discarded if fixations during that run deviate more than x degrees from the center of the screen). The main resource in this post is probably the collection of links in the next section.\nBackground links + installing extra software You’ll need to download the Eyelink API provided by SR Research. To do that, register an account here. Note that they moderate the accounts fairly heavily, so it may take 24 hrs+ for the registration to go though. Once you’re registered, you can download the developers kit API ( Windows, Linux ). You’ll need that kit to be able to call Eyelink functions from within matlab (otherwise you get an error about missing mex files whenever you search for help pages). Registering also gives access to a support forum.\nBefore moving to the next session, it may make sense to look through their manuals. If you have access to our box folder, here’s a link to the relevant Eyelink II manual and the Data Viewer. The manuals are, well, manuals, but reading through them takes less time than their length might suggest. If you are not a member of our lab, you may be able to ask a member of the hMRC to share the manuals.\nWithout a licensing key, the version of the data viewer that can be downloaded is more or less useless (but, here it is). Instead, for working with the data in R, see edfR and itrackR. Note that these are only working on Mac and Linux. So, you may need to be working on the server to install / use those libraries. Alternatively, you can also read the edf files directly into matlab using EDFMEX. However, I won’t be able to help much with using these packages, given that I only discovered them while writing this post.\nKwan-Jin Jung wrote a technical note about the eyetracking system, see here, and here’s the advertisement for our tracker.\n Initializing Eyelink This section walks through a function that initializes the eyelink system. The first step to interfacing with the Eyelink is to call the PTB command EyelinkInitDefaults. This defines a struct with a number of default parameters, el about how the eyetracker will operate. I generally don’t want all of those defaults, so the function below modifies them as needed. After the parameters in el have been modified, this function calls EyelinkUpdateDefaults(el) to indicate to inform the eyelink system that the parameters should change.\nThe main other point of this function is to start the eyetracker calibration. That should be done at the start of each run.\n function [el, exit_flag] = setupEyeTracker( tracker, window, constants ) % SET UP TRACKER CONFIGURATION. Main goal is to modify defaults set in EyelinkInitDefaults. %{ REQUIRED INPUT: tracker: string, either \u0026#39;none\u0026#39; or \u0026#39;T60\u0026#39; window: struct containing at least the fields window.background: background color (whatever was set during call to e.g., PsychImaging(\u0026#39;OpenWindow\u0026#39;, window.screenNumber, window.background)) window.white: numeric defining the color white for the open window (e.g., window.white = WhiteIndex(window.screenNumber);) window.pointer: scalar pointing to main screen (e.g., [window.pointer, window.winRect] = PsychImaging(\u0026#39;OpenWindow\u0026#39;, ... window.screenNumber,window.background);) window.winRect; PsychRect defining size of main window (e.g., [window.pointer, window.winRect] = PsychImaging(\u0026#39;OpenWindow\u0026#39;, ... window.screenNumber,window.background);) constants: struct containing at least constants.eyelink_data_fname: string defining eyetracking data to be saved. Cannot be longer than 8 characters (before file extention). File extension must be \u0026#39;.edf\u0026#39;. (e.g., constants.eyelink_data_fname = [\u0026#39;scan\u0026#39;, num2str(input.runnum, \u0026#39;%02d\u0026#39;), \u0026#39;.edf\u0026#39;];) OUTPUT: if tracker == \u0026#39;T60\u0026#39; el: struct defining parameters that have been set up about the eyetracker (see EyelinkInitDefaults) if tracker == \u0026#39;none\u0026#39; el == [] exit_flag: string that can be used to check whether this function exited successfully SIDE EFFECTS: When tracker == \u0026#39;T60\u0026#39;, calibration is started %} %% exit_flag = \u0026#39;OK\u0026#39;; switch tracker case \u0026#39;T60\u0026#39; % Provide Eyelink with details about the graphics environment % and perform some initializations. The information is returned % in a structure that also contains useful defaults % and control codes (e.g. tracker state bit and Eyelink key values). el = EyelinkInitDefaults(window.pointer); % overrride default gray background of eyelink, otherwise runs end % up gray! also, probably best to calibrate with same colors of % background / stimuli as participant will encounter el.backgroundcolour = window.background; el.foregroundcolour = window.white; el.msgfontcolour = window.white; el.imgtitlecolour = window.white; el.calibrationtargetcolour=[window.white window.white window.white]; EyelinkUpdateDefaults(el); if ~EyelinkInit(0, 1) fprintf(\u0026#39;\\n Eyelink Init aborted \\n\u0026#39;); exit_flag = \u0026#39;ESC\u0026#39;; return; end %Reduce FOV Eyelink(\u0026#39;command\u0026#39;,\u0026#39;calibration_area_proportion = 0.5 0.5\u0026#39;); Eyelink(\u0026#39;command\u0026#39;,\u0026#39;validation_area_proportion = 0.48 0.48\u0026#39;); % open file to record data to i = Eyelink(\u0026#39;Openfile\u0026#39;, constants.eyelink_data_fname); if i ~= 0 fprintf(\u0026#39;\\n Cannot create EDF file \\n\u0026#39;); exit_flag = \u0026#39;ESC\u0026#39;; return; end Eyelink(\u0026#39;command\u0026#39;, \u0026#39;add_file_preamble_text \u0026#39;\u0026#39;Recorded by NAME OF EXPERIMENT\u0026#39;\u0026#39;\u0026#39;); % Setting the proper recording resolution, proper calibration type, % as well as the data file content; Eyelink(\u0026#39;command\u0026#39;,\u0026#39;screen_pixel_coords = %ld %ld %ld %ld\u0026#39;, 0, 0, window.winRect(3)-1, window.winRect(4)-1); Eyelink(\u0026#39;message\u0026#39;, \u0026#39;DISPLAY_COORDS %ld %ld %ld %ld\u0026#39;, 0, 0, window.winRect(3)-1, window.winRect(4)-1); % set calibration type. Eyelink(\u0026#39;command\u0026#39;, \u0026#39;calibration_type = HV5\u0026#39;); % set EDF file contents using the file_sample_data and % file-event_filter commands % set link data thtough link_sample_data and link_event_filter Eyelink(\u0026#39;command\u0026#39;, \u0026#39;file_event_filter = LEFT,RIGHT,FIXATION,SACCADE,BLINK,MESSAGE,BUTTON,INPUT\u0026#39;); Eyelink(\u0026#39;command\u0026#39;, \u0026#39;link_event_filter = LEFT,RIGHT,FIXATION,SACCADE,BLINK,MESSAGE,BUTTON,INPUT\u0026#39;); % check the software version % add \u0026quot;HTARGET\u0026quot; to record possible target data for EyeLink Remote Eyelink(\u0026#39;command\u0026#39;, \u0026#39;file_sample_data = LEFT,RIGHT,GAZE,HREF,GAZERES,AREA,HTARGET,STATUS,INPUT\u0026#39;); Eyelink(\u0026#39;command\u0026#39;, \u0026#39;link_sample_data = LEFT,RIGHT,GAZE,HREF,GAZERES,AREA,HTARGET,STATUS,INPUT\u0026#39;); % make sure we\u0026#39;re still connected. if Eyelink(\u0026#39;IsConnected\u0026#39;)~=1 \u0026amp;\u0026amp; input.dummymode == 0 exit_flag = \u0026#39;ESC\u0026#39;; return; end % possible changes from EyelinkPictureCustomCalibration % set sample rate in camera setup screen Eyelink(\u0026#39;command\u0026#39;, \u0026#39;sample_rate = %d\u0026#39;, 1000); % Will call the calibration routine EyelinkDoTrackerSetup(el); case \u0026#39;none\u0026#39; el = []; end end  Here are a few parts of that function that you will probably want to adapt for your experiment.\nThe various color arguments   Eyelink changes the background color of whatever screen is open. So, these colors (e.g., el.backgroundcolour) should match whatever background your stimuli will be displayed on.  Eyelink('command','calibration_area_proportion = 0.5 0.5'); and Eyelink('command','validation_area_proportion = 0.48 0.48');   The setup at the scanner has a hard time tracking eyes that are fixating near the edges of the screen. The issue is bad enough that it can be almost impossible to calibrate the tracker when the calibration dots appear on the edges. I only really use the eyetracker to have a record confirming that participants were more-or-less fixating during a run, so good calibration at the edges isn’t important to me. For this reason, I reduce the size of the calibration.  Related to 2: Eyelink('command', 'calibration_type = HV5');   This sets the calibration routine to only use 5 dots, rather than 9. Again, my needs are pretty simple and calibration can be challenging, so 5 seems good enough.  Wrapping the function in a switch argument (e.g., tracker ==)   See the next section for some of the logic in writing code with a switch statement or two that all depends on how an initial variable is set1.   The Eyelink functions In setupEyeTracker, you may have noticed many calls that took the following format Eyelink('dosomethingspecial');. Commands like these are PTB’s way of communicating with the Eyelink software.\nThere are a few such functions that you’ll need to include to record any usable data. First, the function we defined above, setupEyeTracker, called the function EyelinkDoTrackerSetup(el). This is a function internal to PTB. It runs the calibration routine. So, you’ll want a call to [el, exitflag] = setupEyeTracker( input.tracker, window, constants ); somewhere early in your code. I rerun the calibration at the start of each experimental run.\nNext, the following commands make sure that you’ve turned on the eyetracker\n% Must be offline to draw to EyeLink screen Eyelink(\u0026#39;Command\u0026#39;, \u0026#39;set_idle_mode\u0026#39;); % clear tracker display Eyelink(\u0026#39;Command\u0026#39;, \u0026#39;clear_screen 0\u0026#39;); Eyelink(\u0026#39;StartRecording\u0026#39;); % always wait a moment for recording to have definitely started WaitSecs(0.1); Eyelink will save it’s files in a specialized format2. For that file, it’s useful to mark when the experiment has actually started. So, include a command like\nEyelink(\u0026#39;message\u0026#39;, \u0026#39;SYNCTIME\u0026#39;); to mark the start. Since this will probably be run in the scanner, a sensible time to place that would be shortly after receiving the scanner trigger, but before the next flip.\nWhen you’re done with the experiment run Eyelink('Command', 'set_idle_mode'); before saving data. Here’s an example of a short routine to save the data. I’ve defined a variable constants.eyelink_data_fname to be a string that ends in ‘.edf’. Note that the filename can be no longer than 8 characters and cannot contain any special characters (only digits and letters).\n% the Eyelink(\u0026#39;ReceiveFile\u0026#39;) function does not wait for the file % transfer to complete so you must have the entire try loop % surrounding the function to ensure complete transfer of the EDF. try fprintf(\u0026#39;Receiving data file \u0026#39;\u0026#39;%s\u0026#39;\u0026#39;\\n\u0026#39;, constants.eyelink_data_fname ); status = eyetrackerFcn(\u0026#39;ReceiveFile\u0026#39;); if status \u0026gt; 0 fprintf(\u0026#39;ReceiveFile status %d\\n\u0026#39;, status); end if 2==exist(edfFile, \u0026#39;file\u0026#39;) fprintf(\u0026#39;Data file \u0026#39;\u0026#39;%s\u0026#39;\u0026#39; can be found in \u0026#39;\u0026#39;%s\u0026#39;\u0026#39;\\n\u0026#39;, constants.eyelink_data_fname, pwd ); end catch fprintf(\u0026#39;Problem receiving data file \u0026#39;\u0026#39;%s\u0026#39;\u0026#39;\\n\u0026#39;, constants.eyelink_data_fname ); end   Sample script Unfortunately, attempting to call these function from a computer that does not have Eyelink’s software installed will produce an error. This makes developing and testing an experimental script challenging, because if we litter our code with calls to Eyelink(...), then when we’re not at the scanner computer we need to comment out all of those lines. I have no faith that I’ll remember to uncomment all of these lines when I’m at the scanner each time, so when I’m writing code that calls these functions I place them in a wrapper. Credit goes to Will Hopper for showing me this strategy when designing functions that receive input.\nThe main idea is two wrap all calls to Eyelink(...) with a function that starts like this\n function eyelinkFcn = makeEyelinkFcn(handlerName) valid_types = {\u0026#39;none\u0026#39;,\u0026#39;T60\u0026#39;}; assert(ismember(handlerName, valid_types),... [\u0026#39;\u0026quot;handlerType\u0026quot; argument must be one of the following: \u0026#39; strjoin(valid_types,\u0026#39;, \u0026#39;)]) switch handlerName case \u0026#39;T60\u0026#39; eyelinkFcn = @T60; case \u0026#39;none\u0026#39; eyelinkFcn = @do_nothing; end % more code to follow end  The outer function, makeEyelinkFcn receives as input the variable handlerName, which can be either none or T60. Depending on that variable, the output to eyelinkFcn is then a call to an anonymous function which implements the actual calls to Eyelink. When handlerName == 'T60', makeEyelinkFcn returns a function that is going to try to call various Eyelink(...) routines (shown below). But, when handlerName == 'none' makeEyelinkFcn will return a function that does nothing.\nThis enables the writing of code that will call the eyelink functions when desired (e.g., when at the scanner), but calls to those functions can also be avoided when desired (by calling makeEyeLinkFcn('none') instead of makeEyeLinkFcn('T60')).\n % ... eyetrackerFcn = makeEyelinkFcn(input.tracker); eyetrackerFcn(\u0026#39;message\u0026#39;, \u0026#39;SYNCTIME\u0026#39;); % ...  So long as input.tracker is taking different values, there’s no need to comment or uncomment when I’m working on a computer that has or doesn’t have an eyelink hooked up3.\nThe remainder of this script defines the local function T60, which allows all of the necessary wrapping to the different Eyelink(...) commands.\n function eyelinkFcn = makeEyelinkFcn(handlerName) valid_types = {\u0026#39;none\u0026#39;,\u0026#39;T60\u0026#39;}; assert(ismember(handlerName, valid_types),... [\u0026#39;\u0026quot;handlerType\u0026quot; argument must be one of the following: \u0026#39; strjoin(valid_types,\u0026#39;, \u0026#39;)]) switch handlerName case \u0026#39;T60\u0026#39; eyelinkFcn = @T60; case \u0026#39;none\u0026#39; eyelinkFcn = @do_nothing; end function status = T60(varargin) status = []; switch varargin{1} case \u0026#39;EyelinkDoDriftCorrection\u0026#39; % Do a drift correction at the beginning of each trial % Performing drift correction (checking) is optional for % EyeLink 1000 eye trackers. EyelinkDoDriftCorrection(varargin{2},[],[],0); case \u0026#39;Command\u0026#39; Eyelink(\u0026#39;Command\u0026#39;, varargin{2}) case \u0026#39;ImageTransfer\u0026#39; %transfer image to host transferimginfo = imfinfo(varargin{2}); [width, height] = Screen(\u0026#39;WindowSize\u0026#39;, 0); % image file should be 24bit or 32bit b5itmap % parameters of ImageTransfer: % imagePath, xPosition, yPosition, width, height, trackerXPosition, trackerYPosition, xferoptions transferStatus = Eyelink(\u0026#39;ImageTransfer\u0026#39;,transferimginfo.Filename,... 0, 0, transferimginfo.Width, transferimginfo.Height, ... width/2-transferimginfo.Width/2 ,height/2-transferimginfo.Height/2, 1); if transferStatus ~= 0 fprintf(\u0026#39;*****Image transfer Failed*****-------\\n\u0026#39;); end case \u0026#39;StartRecording\u0026#39; Eyelink(\u0026#39;StartRecording\u0026#39;); case \u0026#39;Message\u0026#39; if nargin == 2 Eyelink(\u0026#39;Message\u0026#39;, varargin{2}); elseif nargin == 3 Eyelink(\u0026#39;Message\u0026#39;, varargin{2}, varargin{3}); elseif nargin == 4 Eyelink(\u0026#39;Message\u0026#39;, varargin{2}, varargin{3}, varargin{4}); end case \u0026#39;StopRecording\u0026#39; Eyelink(\u0026#39;StopRecording\u0026#39;); case \u0026#39;CloseFile\u0026#39; Eyelink(\u0026#39;CloseFile\u0026#39;); case \u0026#39;ReceiveFile\u0026#39; Eyelink(\u0026#39;ReceiveFile\u0026#39;); case \u0026#39;EyeAvailable\u0026#39; status = Eyelink(\u0026#39;EyeAvailable\u0026#39;); end end function do_nothing(varargin) % do nothing with arguments end end   Extra Resourcess For examples of these methods in action, check out an experiment on Voxel Tuning Functions. In particular, see setupEyeTracker, makeEyelinkFcn. That repository also has examples of using the value returned by makeEyelinkFcn in runContrast. Note that the repository may change from time to time and might not match the code in this post exactly. To download the exact files defined above, see setupEyeTracker, makeEyelinkFcn\nHere’s the original publication that introduced the Eyelink interface to PTB.\nAlso, for inspiration about the cool experiments that can be run with Eyelink’s software, see the PTB Demos. See a list of Eyelink functions here. You’ll need to look at this page if you want access to the help files for these commands on a computer without Eyelink installed.\nFinally, thanks to Ramiro for sharing a PTB script that got me started with Eyelink.\n  though, I’ve already broken some of the logic I outline in that section by having more than one function with a switch statement.↩︎\n The options relating to saving data are for another post. It seems like you can do quite a lot with the Eyelink Data Viewer when various event tags have been set up properly (see manual, on box ), but my needs are so simple that I haven’t bothered digging too deeply.↩︎\n Of course, a similar effect could be achieved by littering the experimental code with a bunch of if then else statements. However, this method has the advantage of massively reducing the number of switch statements in the code. Fewer switch statements can be easier to follow and modify, because most of the effect of the input.tracker variable can be localized to a single function (the definition of makeEyelinkFcn)↩︎\n   ","date":1528156800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653490997,"objectID":"c3057701063bf93f1e6506ccf0c06f46","permalink":"https://psadil.github.io/psadil/post/eyetracking-init/","publishdate":"2018-06-05T00:00:00Z","relpermalink":"/psadil/post/eyetracking-init/","section":"post","summary":"UPDATE: I now think that the examples I’ve presented here obscure the interface with Eyelink. Much cleaner to use MATLAB’s object oriented programming. This is covered in another post.\nThis post is designed as minimal documentation for using the Eyelink software at the UMass Amherst hMRC. The goals are very modest\nProvide sample Psychtoolbox (PTB) and MATLAB code for integrating eyelink Explain a few parameters that you might want to change in your experiment  The main audience includes members of the cMAP and CEMNL labs at UMass, but other users of the hMRC may also benefit.","tags":["eyelink","matlab","psychtoolbox"],"title":"eyetracking with eyelink in psychtoolbox","type":"post"},{"authors":["Patrick Sadil","David Huber","John Serences","Rosemary Cowell"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653490997,"objectID":"45c378e5d72baade6ecb9033ae419bd9","permalink":"https://psadil.github.io/psadil/publication/sadil-2018-hierarchical/","publishdate":"2020-01-13T13:30:43.817295Z","relpermalink":"/psadil/publication/sadil-2018-hierarchical/","section":"publication","summary":"It is tempting to infer the behavior of individual neurons from the behavior of individual voxels in an fMRI experiment. For instance, voxel tuning functions (VTFs) measure the magnitude of the BOLD response to a range of stimulus features (e.g., orientation), producing results that resemble individual neural tuning functions (NTFs) from single-cell recordings – like a simple cell in V1, a voxel will prefer a particular orientation. However, a voxel likely reflects a mixture of different kinds of neurons with different preferred orientations. Taking a GLM approach to this problem, forward encoding models (e.g., Brouwer and Heeger, 2009, 2011) specify the strength of different neural sub-populations (e.g., neurons preferring different orientations) for each voxel. However, these models cannot identify changes in the shape of the neural tuning function because they assume a fixed NTF shape. For instance, these models could not identify whether the NTF sharpens with perceptual learning. To address this limitation, we developed a hierarchical Bayesian model for inferring not only the relative proportions of neural sub-populations contributing to a voxel, but also the shape of the NTF and changes in NTF shape. To test the validity of this approach, we collected fMRI data while subjects viewed oriented gratings at low and high contrast. We considered three alternative forms of NTF modulation by stimulus contrast (additive shift, multiplicative gain, bandwidth sharpening). To the naked eye, the VTFs revealed an additive shift from low to high contrast. However, the hierarchical Bayesian model indicated that this shift was caused by multiplicative gain in the underlying NTFs, in line with single cell recordings. Beyond orientation, this approach could determine the form of neuromodulation in many fMRI experiments that test multiple points along a well-established dimension of variation (e.g., speed of motion, angle of motion, isoluminant hue).","tags":null,"title":"A hierarchical Bayesian model for inferring neural tuning functions from voxel tuning functions","type":"publication"},{"authors":["Patrick Sadil","Rosemary A Cowell"],"categories":null,"content":"","date":1493251200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653490997,"objectID":"c58bfb3caef606a9ace15cde50655827","permalink":"https://psadil.github.io/psadil/publication/sadil-2017-computational/","publishdate":"2017-04-24T13:30:43.817561Z","relpermalink":"/psadil/publication/sadil-2017-computational/","section":"publication","summary":"Damage to the medial temporal lobe (MTL) has long been known to impair declarative memory, and recent evidence suggests that it also impairs visual perception. A theory termed the representational-hierarchical account explains such impairments by assuming that MTL stores conjunctive representations of items and events, and that individuals with MTL damage must rely upon representations of simple visual features in posterior visual cortex, which are inadequate to support memory and perception under certain circumstances. One recent study of visual discrimination behavior revealed a surprising antiperceptual learning effect in MTL-damaged individuals: With exposure to a set of visual stimuli, discrimination performance worsened rather than improved [Barense, M. D., Groen, I. I. A., Lee, A. C. H., Yeung, L. K., Brady, S. M., Gregori, M., et al. Intact memory for irrelevant information impairs perception in amnesia. Neuron, 75, 157–167, 2012]. We extend the representational-hierarchical account to explain this paradox by assuming that difficult visual discriminations are performed by comparing the relative “representational tunedness”—or familiarity—of the to-be-discriminated items. Exposure to a set of highly similar stimuli entails repeated presentation of simple visual features, eventually rendering all feature representations maximally and, thus, equally familiar; hence, they are inutile for solving the task. Discrimination performance in patients with MTL lesions is therefore impaired by stimulus exposure. Because the unique conjunctions represented in MTL do not occur repeatedly, healthy individuals are shielded from this perceptual interference. We simulate this mechanism with a neural network previously used to explain recognition memory, thereby providing a model that accounts for both mnemonic and perceptual deficits caused by MTL damage with a unified architecture and mechanism.","tags":null,"title":"A computational model of perceptual and mnemonic deficits in medial temporal lobe amnesia","type":"publication"},{"authors":["David A Ross","Patrick Sadil","D Merika Wilson","Rosemary A Cowell"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653490997,"objectID":"95fb1b4ac503c18d16874e2a711e8fc2","permalink":"https://psadil.github.io/psadil/publication/ross-2017-hippocampal/","publishdate":"2020-01-13T13:30:43.817561Z","relpermalink":"/psadil/publication/ross-2017-hippocampal/","section":"publication","summary":"The hippocampus is considered pivotal to recall, allowing retrieval of information not available in the immediate environment. In contrast, neocortex is thought to signal familiarity, contributing to recall only when called upon by the hippocampus. However, this view is not compatible with representational accounts of memory, which reject the mapping of cognitive processes onto brain regions. According to representational accounts, the hippocampus is not engaged by recall per se, rather it is engaged whenever hippocampal representations are required. To test whether hippocampus is engaged by recall when hippocampal representations are not required, we used functional imaging and a non-associative recall task, with images (objects, scenes) studied in isolation, and image patches as cues. As predicted by a representational account, hippocampal activation was modulated by the content of the recalled memory, increasing during recall of scenes—which are known to be processed by hippocampus—but not during recall of objects. Object recall instead engaged neocortical regions known to be involved in object-processing. Further supporting the representational account, effective connectivity analyses revealed that changes in functional activation during recall were driven by increased information flow from neocortical sites, rather than by the spreading of recall-related activation from hippocampus back to neocortex.","tags":null,"title":"Hippocampal engagement during recall depends on memory content","type":"publication"}]