[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "Hinting Cache Decorator Types\n\n\n\ngist\n\n\n\n\n\n\n\n\n\nOct 23, 2022\n\n\nPatrick Sadil\n\n\n\n\n\n\n\n\n\n\n\n\nHow to store a NifTi as a TFRecord\n\n\n\ngist\n\n\n\n\n\n\n\n\n\nApr 23, 2022\n\n\nPatrick Sadil\n\n\n\n\n\n\n\n\n\n\n\n\nSpurious Serial Dependencies\n\n\n\nresearch\n\n\n\n\n\n\n\n\n\nMay 7, 2021\n\n\nPatrick Sadil\n\n\n\n\n\n\n\n\n\n\n\n\nNew England GAN\n\n\n\n\n\n\n\n\n\n\n\nJan 17, 2021\n\n\nPatrick Sadil\n\n\n\n\n\n\n\n\n\n\n\n\ncounterbalanced continuous designs with eulerian walks\n\n\n\nexperiments\n\n\n\n\n\n\n\n\n\nMar 14, 2020\n\n\nPatrick Sadil\n\n\n\n\n\n\n\n\n\n\n\n\nthoughts on eye movement\n\n\n\n\n\n\n\n\n\n\n\nNov 21, 2019\n\n\nPatrick Sadil\n\n\n\n\n\n\n\n\n\n\n\n\nHalf of a parameter\n\n\n\ndissertation\n\n\n\n\n\n\n\n\n\nNov 15, 2019\n\n\nPatrick Sadil\n\n\n\n\n\n\n\n\n\n\n\n\nStaircases for Thresholds, Part 2\n\n\n\ndissertation\n\n\n\n\n\n\n\n\n\nNov 8, 2019\n\n\nPatrick Sadil\n\n\n\n\n\n\n\n\n\n\n\n\nStaircases for Thresholds\n\n\n\ndissertation\n\n\n\n\n\n\n\n\n\nOct 30, 2019\n\n\nPatrick Sadil\n\n\n\n\n\n\n\n\n\n\n\n\nForward encoding model\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2019\n\n\nPatrick Sadil\n\n\n\n\n\n\n\n\n\n\n\n\nModulations to tuning functions can bias evidence accumulation\n\n\n\ndissertation\n\n\n\n\n\n\n\n\n\nOct 18, 2019\n\n\nPatrick Sadil\n\n\n\n\n\n\n\n\n\n\n\n\nserial dependence reflects a preference for low variability\n\n\n\ndissertation\n\n\n\n\n\n\n\n\n\nOct 11, 2019\n\n\nPatrick Sadil\n\n\n\n\n\n\n\n\n\n\n\n\nderivative of gaussian for serial dependence\n\n\n\ndissertation\n\n\n\n\n\n\n\n\n\nOct 3, 2019\n\n\nPatrick Sadil\n\n\n\n\n\n\n\n\n\n\n\n\nan overview of population receptive field mapping\n\n\n\ndissertation\n\n\n\n\n\n\n\n\n\nSep 27, 2019\n\n\nPatrick Sadil\n\n\n\n\n\n\n\n\n\n\n\n\nSerial Dependence\n\n\n\ndissertation\n\n\n\n\n\n\n\n\n\nSep 19, 2019\n\n\nPatrick Sadil\n\n\n\n\n\n\n\n\n\n\n\n\nCircular Diffusion Model of Response Times\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2019\n\n\nPatrick Sadil\n\n\n\n\n\n\n\n\n\n\n\n\neyetracking with eyelink in psychtoolbox, now with oop\n\n\n\nexperiments\n\n\n\n\n\n\n\n\n\nMay 25, 2019\n\n\nPatrick Sadil\n\n\n\n\n\n\n\n\n\n\n\n\nBasic Importance Sampling for Variance Reduction\n\n\n\ncomps\n\n\n\n\n\n\n\n\n\nNov 10, 2018\n\n\nPatrick Sadil\n\n\n\n\n\n\n\n\n\n\n\n\nAnonymizing MTurk WorkerIDs\n\n\n\nexperiments\n\n\n\n\n\n\n\n\n\nAug 7, 2018\n\n\nPatrick Sadil\n\n\n\n\n\n\n\n\n\n\n\n\neyetracking with eyelink in psychtoolbox\n\n\n\nexperiments\n\n\n\n\n\n\n\n\n\nJun 5, 2018\n\n\nPatrick Sadil\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2019-11-15-half-of-a-parameter/index.html",
    "href": "posts/2019-11-15-half-of-a-parameter/index.html",
    "title": "Half of a parameter",
    "section": "",
    "text": "Science produces models that provide parsimonious descriptions of the world. In cognitive psychology, models regularly compete to explain a few phenomena. But models can survive experiment after experiment, both because of the difficulty of capturing participant’s nuanced behavior, and because models often make highly overlapping predictions. In these cases, a model succeeds through its relative parsimony.\nIn cognitive psychology, measures of information criteria, specifically Akaike’s and the Bayesian information criteria (Schwarz 1978; Akaike 1973), determine a winning model. These criteria measure complexity by tallying the number of parameters in a model. Long maths and specific assumptions justify the claim that a model with more parameters is more complex than a model with fewer parameters, and so does our intuition that a model is complex if it has many moving parts. Unfortunately, the assumptions fail in common situations, such as when the models are fit in a Bayesian rather than frequentist setting. An alphabet soup of other information criteria exists (in addition to Akaike’s the Bayesian criteria, there is the DIC, WAIC, KIC, NIC, TIC, etc), and these other criteria assign complexity more complexly. These criteria are sensitive not only to the number of parameters in a model but also to the varied roles that a parameter can have. They assign the complexity of a model based on the model’s number of ‘effective parameters.’\nFor intuition on why tallying the number of parameters is an insufficient measure complexity, consider two models of response time. Both models assume that response times are distributed according to a normal distribution. In this simple example, the variability of the distributions are known, and so the models have only a single free parameter, which is the average response time. In one model, that average can be any number, a value from negative to positive infinity. This is the kind of model implicitly assumed when we conduct a t-test on the averages of response times. Of course the model is a simplification of response times, but this model also has the glaring flaw that it allows the average response time to be negative; a participant cannot respond to stimulation before the stimulus appears. The second model addresses this flaw by adding the constraint that the average response time cannot be negative. Although the second model is more constrained, the models have the same number of parameters. The second model can only account for half of the patterns of data as the first; the second model is twice as parsimonious as the first (Gelman, Hwang, and Vehtari 2014). To adjudicate between these model requires a measure that is sensitive to complexity but does not simply tally the number of parameters in each model.\n\n\n\n\n\n\n\nReferences\n\nAkaike, Hirotogu. 1973. “Information Theory and an Extension of the Maximum Likelihood Principle.” In Proceedings of the Second International Symposium on Information Theory, 267–81.\n\n\nGelman, Andrew, Jessica Hwang, and Aki Vehtari. 2014. “Understanding Predictive Information Criteria for Bayesian Models.” Statistics and Computing 24 (6): 997–1016. https://doi.org/10.1007/s11222-013-9416-2.\n\n\nSchwarz, Gideon. 1978. “Estimating the Dimension of a Model.” The Annals of Statistics 6 (2): 461–64. https://doi.org/10.1214/aos/1176344136."
  },
  {
    "objectID": "posts/2019-10-03-derivative-of-gaussian-for-serial-dependence/index.html",
    "href": "posts/2019-10-03-derivative-of-gaussian-for-serial-dependence/index.html",
    "title": "derivative of gaussian for serial dependence",
    "section": "",
    "text": "Cognitive experiments can require participants to complete hundreds of trials, but completing so many trials invariably alters participants’ behavior. Their behavior late in the experiment can depend on their behavior early in the experiment. Although such dependence can be an experimental confound, the dependence itself can provide clues about cognition. One simple kind of dependence occurs through learning; hundreds of trials provides participants ample practice. A more subtle dependence can emerge between sequential trials, an effect called serial dependence. Theoretical interpretations of serial dependence vary, and some of that variability may relate to how the dependence is measured. In this post, I review a statistical method commonly used to analyze serial dependence and discuss one way that method can fail.\nI will focus on the analysis of an orientation judgment task, in which participants simply see an oriented bar on each trial, remember the bar’s orientation for a short period, and then report the orientation. Participants’ responses on one trial can depend on the orientation they saw in the previous trial. The dependence follows a Gaussian’s derivative function. Figure 1 a shows a Gaussian function with its derivative, and Figure 1 b shows the derivative modeling a range of different serial dependence patterns. The derivative captures three key features of the data. First, different changes in orientation between trials result in serial dependencies of different magnitude. The responsiveness of dependence is captured by the width of the derivative. Second, serial dependence can have a different magnitude. The magnitude is captured by the amplitude of the derivative. Finally, responses on the current trial can either be attracted towards or repulsed away from the orientation of the previous trial. The direction of the effect is captured with the sign of the amplitude. The direction of the effect–and the experimental manipulations that change that direction–are often critical to different theoretical interpretations of serial dependence.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Models. a) A Gaussian function and its derivative. b) The derivative captures how errors on the current trial can depend on how the relationship between the orientation seen in the current and previous trials. Positive values on the horizontal axis signify a clockwise difference and negative values a counterclockwise difference. Likewise, positive errors signify responses on the current trial which were clockwise to the true orientation, and negative errors are counterclockwise. When errors are in the same direction as the difference in orientations, the error is said to be attractive. Otherwise, the error is repulsive. Whether errors are attractive or repulsive is given by the sign of the derivative’s amplitude.\n\n\n\nAlthough the Gaussian’s derivative adequately models the serial dependence between trials with similar orientations (less than 45 degree differences), the derivative fits poorly the dependencies following large changes. When sequential trials have a large orientation difference, the sign of the dependence often changes; small orientation differences can elicit an attractive dependence even while large differences are repulsive. These sign flips are called the peripheral bumps, and they are not captured by the Gaussian’s derivative. If the bumps are large enough, they can interpretations about the sign to of dependencies following small changes can be inverted (Figure 2). Unfortunately, noticing the peripheral bumps can be hard with sparse data. But even with sparse data, the width of the best-fitting derivative can help identify bumps. If the best-fitting derivative is abnormally wide (with peaks larger than approximately 35 degrees), then the derivative is tracking dependencies wider than it should. In that circumstance, it may be best to focus analyses on only the trials with smaller orientation differences.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Misfits of the Gaussian’s derivative. The dots give hypothetical data. The data were generated with a function whose average is traced by the dashed line. The data were fit with a derivative of Guassian function, and the best-fitting derivative is shown with a solid line. The derivative does not match the data-generating function."
  },
  {
    "objectID": "posts/2019-10-24-forward-encoding-model/index.html",
    "href": "posts/2019-10-24-forward-encoding-model/index.html",
    "title": "Forward encoding model",
    "section": "",
    "text": "Functional magnetic resonance imaging records brain activity with spatially distinct voxels, but this segmentation will be misaligned with a brain’s meaningful boundaries. The segmentation results in some voxels recording activity from different types of tissue – types that are both neural an non-neural – but even voxels that exclusively sample gray matter can span functionally distinct cortex. For example, a 3T scanner allows voxels in the range of 1.5-3 mm\\(^3\\), but orientation columns have an average width of 0.8 mm (Yacoub, Harel, and Uğurbil 2008). Studying orientation columns with such low resolution requires statistical tools.\nOne statistical tool models voxel activity as a linear combination of the activity of a small number of neural channels (Brouwer and Heeger 2009; Kay et al. 2008). These models are called forward models, describing how the channel activity transforms into voxel activity. In early sensory cortex, the channels are analogous to cortical columns. In later cortex, the channels are more abstract dimensions of a representational space. Developing a forward model requires assuming not only how many channels contribute of a voxel’s activity, but also the tuning properties of those channels. With these assumptions, regression allows inferring the contribution of each channel to each voxel’s activity. Let \\(N\\) be the number of observations for each voxel, \\(M\\) be the number of voxels, and \\(K\\) be the number of channels within a voxel. The forward model specifies that the data (\\(B\\), \\(M \\times N\\)) result from a weighted combination of the assumed channels responses (\\(C\\), \\(K \\times N\\)), where the weights (\\(W\\), \\(M \\times K\\)) are unknown.\n\\[\nB = WC\n\\]\nTaking the pseudoinverse of the channel matrix and multiplying the result by the data gives an estimate of the weight matrix:\n\\[\n\\widehat{W} = BC^T(CC^T)^{-1}\n\\]\nAssumptions about \\(C\\) are assumptions about how the channels encode stimuli. Different encoding schemes can be instantiated with different \\(C\\), and any method for comparing linear models could be used to compare the schemes.\nThe forward encoding model enables comparison of static encoding schemes, but neural encoding schemes are dynamic. Attentional fluctuations, perceptual learning, and stimulation history all modulate neural tuning functions (McAdams and Maunsell 1999; Reynolds, Pasternak, and Desimone 2000; Siegel, Buschman, and Miller 2015; Yang and Maunsell 2004). To explore modulations with functional magnetic resonance imaging, some researchers have inverted the encoding model (Garcia, Srinivasan, and Serences 2013; Rahmati, Saber, and Curtis 2018; Saproo and Serences 2014; Scolari, Byers, and Serences 2012; Sprague and Serences 2013; Vo, Sprague, and Serences 2017). The inversion is a variation of cross validation. The method estimates the weight matrix with only some of the data (e.g., all data excluding a single run). The held out data, \\(B_H\\), contains observations from all experimental condition across which the tuning functions might vary. The encoding model is inverted by multiplying the pseudoinverse of the weight matrix with the held out data to estimate a new channel response matrix.\n\\[\n\\widehat{C} = \\widehat{W}^T(\\widehat{W}\\widehat{W}^T)^{-1}B_H\n\\]\nThe new channel response matrix estimates how the channels respond in each experimental condition.\nAlthough validation studies demonstrated that the inverted encoding model enables inferences that recapitulate some modulations observed with electrophysiology (Sprague et al. 2018; Sprague, Saproo, and Serences 2015), the inversion also misleads inferences about certain fundamental modulations (Gardner and Liu 2019; Liu, Cable, and Gardner 2018). In particular, increasing the contrast of an orientation increases the gain of neurons tuned to orientation without altering their tuning bandwidth (Alitto and Usrey 2004; Sclar and Freeman 1982; Skottun et al. 1987), but the inverted encoding model (incorrectly) suggests that higher contrast decreases bandwidth (Liu, Cable, and Gardner 2018). Inferences are misled because the estimated channel responses are constrained by the initial assumptions about \\(C\\) (Gardner and Liu 2019). Using the encoding model to study modulations requires a way to estimate the contribution of each channel without assuming a fixed channel response function.\n\n\n\n\n\n\n\nReferences\n\nAlitto, Henry J, and W Martin Usrey. 2004. “Influence of Contrast on Orientation and Temporal Frequency Tuning in Ferret Primary Visual Cortex.” Journal of Neurophysiology 91 (6): 2797–2808.\n\n\nBrouwer, Gijs Joost, and David J Heeger. 2009. “Decoding and Reconstructing Color from Responses in Human Visual Cortex.” Journal of Neuroscience 29 (44): 13992–4003.\n\n\nGarcia, Javier O, Ramesh Srinivasan, and John T Serences. 2013. “Near-Real-Time Feature-Selective Modulations in Human Cortex.” Current Biology 23 (6): 515–22.\n\n\nGardner, Justin L, and Taosheng Liu. 2019. “Inverted Encoding Models Reconstruct an Arbitrary Model Response, Not the Stimulus.” eNeuro 6 (2).\n\n\nKay, Kendrick N, Thomas Naselaris, Ryan J Prenger, and Jack L Gallant. 2008. “Identifying Natural Images from Human Brain Activity.” Nature 452 (7185): 352.\n\n\nLiu, Taosheng, Dylan Cable, and Justin L Gardner. 2018. “Inverted Encoding Models of Human Population Response Conflate Noise and Neural Tuning Width.” Journal of Neuroscience 38 (2): 398–408.\n\n\nMcAdams, Carrie J, and John HR Maunsell. 1999. “Effects of Attention on Orientation-Tuning Functions of Single Neurons in Macaque Cortical Area V4.” Journal of Neuroscience 19 (1): 431–41.\n\n\nRahmati, Masih, Golbarg T Saber, and Clayton E Curtis. 2018. “Population Dynamics of Early Visual Cortex During Working Memory.” Journal of Cognitive Neuroscience 30 (2): 219–33.\n\n\nReynolds, John H, Tatiana Pasternak, and Robert Desimone. 2000. “Attention Increases Sensitivity of V4 Neurons.” Neuron 26 (3): 703–14.\n\n\nSaproo, Sameer, and John T Serences. 2014. “Attention Improves Transfer of Motion Information Between V1 and MT.” Journal of Neuroscience 34 (10): 3586–96.\n\n\nSclar, G, and RD Freeman. 1982. “Orientation Selectivity in the Cat’s Striate Cortex Is Invariant with Stimulus Contrast.” Experimental Brain Research 46 (3): 457–61.\n\n\nScolari, Miranda, Anna Byers, and John T Serences. 2012. “Optimal Deployment of Attentional Gain During Fine Discriminations.” Journal of Neuroscience 32 (22): 7723–33.\n\n\nSiegel, Markus, Timothy J Buschman, and Earl K Miller. 2015. “Cortical Information Flow During Flexible Sensorimotor Decisions.” Science 348 (6241): 1352–55.\n\n\nSkottun, Bernt C, Arthur Bradley, Gary Sclar, Izumi Ohzawa, and Ralph D Freeman. 1987. “The Effects of Contrast on Visual Orientation and Spatial Frequency Discrimination: A Comparison of Single Cells and Behavior.” Journal of Neurophysiology 57 (3): 773–86.\n\n\nSprague, Thomas C, Kirsten CS Adam, Joshua J Foster, Masih Rahmati, David W Sutterer, and Vy A Vo. 2018. “Inverted Encoding Models Assay Population-Level Stimulus Representations, Not Single-Unit Neural Tuning.” eNeuro 5 (3).\n\n\nSprague, Thomas C, Sameer Saproo, and John T Serences. 2015. “Visual Attention Mitigates Information Loss in Small-and Large-Scale Neural Codes.” Trends in Cognitive Sciences 19 (4): 215–26.\n\n\nSprague, Thomas C, and John T Serences. 2013. “Attention Modulates Spatial Priority Maps in the Human Occipital, Parietal and Frontal Cortices.” Nature Neuroscience 16 (12): 1879.\n\n\nVo, Vy A, Thomas C Sprague, and John T Serences. 2017. “Spatial Tuning Shifts Increase the Discriminability and Fidelity of Population Codes in Visual Cortex.” Journal of Neuroscience 37 (12): 3386–3401.\n\n\nYacoub, Essa, Noam Harel, and Kâmil Uğurbil. 2008. “High-Field fMRI Unveils Orientation Columns in Humans.” Proceedings of the National Academy of Sciences 105 (30): 10607–12.\n\n\nYang, Tianming, and John HR Maunsell. 2004. “The Effect of Perceptual Learning on Neuronal Responses in Monkey Visual Area V4.” Journal of Neuroscience 24 (7): 1617–26."
  },
  {
    "objectID": "posts/2019-09-27-population-receptive-field-mapping/index.html",
    "href": "posts/2019-09-27-population-receptive-field-mapping/index.html",
    "title": "an overview of population receptive field mapping",
    "section": "",
    "text": "Perceiving the world requires representing the world in neural tissue. A neuron is tuned to perceivable information when different values of that information cause the neuron to fire at a different rate. For example, most visual neurons are tuned to spatial location. The spatial tuning could be measured by placing a recording electrode in a neuron in a macaque’s visual cortex while the macaque fixated on the center of a computer monitor and a picture moved across that monitor. The electrode would report higher activity only when the picture was in certain parts of the macaque’s visual field. The function relating the position of the picture to the neuron’s activity is the tuning function. Such functions often resembles a bivariate Gaussian (Figure 1). To study these tuning functions is to study how these neurons represent the world.\nSensory neurons are tuned to many other features such as orientation, color, pitch, direction of motion. Most neurons tuned to one visual feature are also tuned to spatial location, so understanding a neuron’s spatial can facilitate understanding its other sensitivities1. The tuning functions of neurons even have a special name, their receptive field. However, it is often unfeasible to record from individual neurons in humans, and instead only non-invasive neuroimaging methods are available. But these non-invasive methods have low spatial resolution. Even the relatively well spatially resolved technique of functional magnetic resonance imaging reflects the aggregated activity of 10e5 - 10e6 neurons.\nFortunately, the retinotopic arrangement of visual neurons facilitates relating the spatial tuning of a voxel2 to the receptive field of individual neurons. A retinotopic arrangement means that neighboring neurons are tuned to neighboring locations in the visual field; for example, neurons tuned to things in the fovea cluster together and neurons tuned to peripheral locations surround that cluster. This retinotopic arrangement implies that all neurons sampled by a voxel represent nearby regions in the visual environment. Referring to the neurons in a voxel as a population, the receptive field of a voxel is called a population receptive field. Studying population receptive fields alone cannot reveal how individual neurons contribute to a coherent perceptual experience, but studying them can reveal how the populations respond as a group.\nTo chart out all of the mountainous population receptive fields in visual cortex is called population receptive field mapping (Dumoulin and Wandell 2008). The receptive fields can be mapped by recording the activity of each voxel while a human participant is shown some visually salient movie. A mathematical model – such as a bivariate Gaussian – of the receptive field is assumed, and the data from each voxel are used to fit the parameters of that model. The specific images that are used will depend on which part of visual cortex is the focus of the experiment. A counterphasing black and white checkerboard might be close to optimal for primary visual cortex, but the checkerboard would only weakly stimulate neurons in higher level visual regions. To stimulate most of visual cortex, other researchers rely on more varied displays."
  },
  {
    "objectID": "posts/2019-09-27-population-receptive-field-mapping/index.html#footnotes",
    "href": "posts/2019-09-27-population-receptive-field-mapping/index.html#footnotes",
    "title": "an overview of population receptive field mapping",
    "section": "Footnotes",
    "text": "Footnotes\n\n\ne.g., if you want to understand how a neuron is tuned to color, it helps to know where to put the color↩︎\n Voxels are the elements that hold data in magnetic resonance imaging. A voxel in a 3D image is analogous to a pixel in a 2D image; a voxel is a pixel with a volume.↩︎"
  },
  {
    "objectID": "posts/2021-05-07-spurious-serial-dependencies/index.html",
    "href": "posts/2021-05-07-spurious-serial-dependencies/index.html",
    "title": "Spurious Serial Dependencies",
    "section": "",
    "text": "renv::use(lockfile = \"renv.lock\", verbose = FALSE)\nknitr::opts_chunk$set(echo = TRUE, dev = \"ragg_png\")\n\nset.seed(1234)\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(ragg)\nI’m working on a project involving serial dependence. The project involves disentangling a dependence on the previous orientation from a dependence on the previous response. Unfortunately, there is a common way for a dependence on the previous response to be spurious, due to the oblique effect1. The first reference I’ve seen for this is a master’s thesis by Fritsche (2016). I didn’t follow that explanation, and so I’m using this post to explain how the oblique effect causes a spurious dependence on the previous response.\nFirst, let’s show that the confound is real. Data will be generated with an oblique effect, and there will be no dependence between trials – neither on the previous orientation nor the previous response. There will be no response variability, meaning that errors will only be caused by the oblique effect. Since the data are simulated without dependencies, any dependence that emerges will necessarily be spurious.\n# helper functions for converting between angles and degrees\nrad &lt;- function(degree) degree * pi / 180\ndeg &lt;- function(radian) radian * 180 / pi\n\n# magnitude of oblique effect\noblique &lt;- rad(-22.5)\n\nd0 &lt;- tibble(\n  orientation = runif(5000, 0, pi)) |&gt;\n  mutate(\n    trial = 1:n(),\n    oblique = oblique*sin(orientation*4),\n    response = rnorm(n(), orientation, 0) + oblique)\nTo help generate the data, define a helper functions that calculates the signed, shortest angle between two angles (measured in radians).\n#' @param deg1 numeric degree\n#' @param deg2 numeric degree\n#'\n#' @return \n#' signed difference between inputs, wrapped to +-pi\n#' output is shortest distance to the first input\n#'\n#' @examples\n#' # pi/2 is 45 degrees clockwise from pi, so the output is pi/2\n#' ang_diff(pi/2, pi)\n#' # pi is 45 degrees counterclockwise from pi/2, so the output is -pi/2\n#' ang_diff(pi, pi/2)\n#' # notice the discontinuity when the shortest angle switches direction\n#' ang_diff(pi/2 - .01, 0)\n#' ang_diff(pi/2 + .01, 0)\nang_diff &lt;- function(deg1, deg2){\n  stopifnot(length(deg1) == length(deg2))\n  diff &lt;- ( deg1 - deg2 + pi/2 ) %% pi - pi/2\n  out &lt;- dplyr::if_else(diff &lt; -pi/2, diff + pi, diff)\n  return(out)\n}\nThen use the function ang_diff to calculate errors, and to calculate the relative orientation difference between the current trial and either the previous orientation or the previous response.\nd &lt;- d0 |&gt;   \n  mutate(\n    prev_response = lag(response),\n    prev_orientation = lag(orientation),\n    error = ang_diff(orientation, response),\n    orientation_diff = ang_diff(orientation, prev_orientation),\n    response_diff = ang_diff(orientation, prev_response)) |&gt;\n  filter(trial &gt; 1) |&gt;\n  mutate(across(where(is.double), deg))\nPlot errors as a function of the current orientation to confirm that there is an oblique effect.\nIs there a dependence on either the previous response or previous orientation?\nWhat’s going on? There are two key factors: first, the oblique effect operates on the previous trial to make some previous responses more likely than others, and second the oblique effect operates on the current trial to make certain previous responses more likely to have errors in a consistent direction. To be precise, I’ll use the following terminology."
  },
  {
    "objectID": "posts/2021-05-07-spurious-serial-dependencies/index.html#terminology",
    "href": "posts/2021-05-07-spurious-serial-dependencies/index.html#terminology",
    "title": "Spurious Serial Dependencies",
    "section": "Terminology",
    "text": "Terminology\nTrials will be indexed by natural numbers. The “current trial” will be referred to as trial \\(n\\), and the “previous trial” as trial \\(n-1\\). The orientation and responses on each trial will be thought of as sequences2. The variable \\(O_n\\) means the orientation on trial \\(n\\) (i.e., the current trial), whereas the variable \\(O_{n-1}\\) means the orientation on trial \\(n-1\\) (i.e., the previous trial). Similarly, the variable \\(R_n\\) means the response on trial \\(n\\), whereas the variable \\(R_{n-1}\\) means the response on trial \\(n-1\\).\nAll angles (e.g., \\(O_n\\) and \\(R_n\\)) use the convention that \\(0^\\circ\\) is horizontal, \\(45^\\circ\\) is one quarter rotation counterclockwise from horizontal (e.g., at 1:30 on a clock), \\(90^\\circ\\) is vertical, etc. However, differences between angles are reported such that a positive value implies a clockwise shift (i.e., moving forward on the clock) and a negative value implies a counterclockwise shift. For example, an error of \\(10^\\circ\\) means that \\(R_n\\) is \\(10^\\circ\\) clockwise from \\(O_n\\). This means that we can determine an “attraction” effect based on whether the sign of the error on trial \\(n\\) matches the sign of the difference between \\(O_n\\) and either \\(O_{n-1}\\) or \\(R_{n-1}\\). Conversely, a “repulsive” effect is when the error and differences have mismatched signs."
  },
  {
    "objectID": "posts/2021-05-07-spurious-serial-dependencies/index.html#explanation",
    "href": "posts/2021-05-07-spurious-serial-dependencies/index.html#explanation",
    "title": "Spurious Serial Dependencies",
    "section": "Explanation",
    "text": "Explanation\nFirst, consider a specific sequence of trials that could produce a spurious effect. To help with the explanation, the trials are colored based on the current trial.\n\n\n\n\nd |&gt;\n  pivot_longer(\n    cols=c(orientation_diff, response_diff), \n    names_to = \"covariate\",\n    names_pattern = \"(orientation|response)\",\n    values_to = \"x\") |&gt;\n  ggplot(aes(x=x, y=error)) +\n  geom_point(aes(color=oblique)) +\n  scale_color_gradient2(low = scales::muted(\"blue\"), high = scales::muted(\"red\")) +\n  facet_wrap(~covariate) +\n  geom_smooth(\n    method = \"gam\",\n    formula = y ~ s(x, bs = \"cc\", k=9)) +\n  scale_y_continuous(\n    breaks = c(-20, -10, 0, 10, 20),\n    labels = c(\"CCW\", -10, 0, 10, \"CW\")) +\n  scale_x_continuous(\n    name = \"relative orientation/response on previous trial\",\n    labels = c(-90, 0, 90),\n    breaks = c(-90, 0, 90))\n\n\n\n\n\n\n\n\n\n\nFigure 3: Same figure as above, but colored based on the magnitude and direction of the oblique effect.\n\n\n\nWhen \\(O_{n-1}\\) is \\(0^\\circ\\), the oblique effect will have not caused an error. So, for \\(R_{n-1}\\) to be \\(22.5^\\circ\\) clockwise to \\(O_n\\), then \\(O_n\\) could be, itself \\(22.5^\\circ\\). But when \\(O_n\\) is \\(22.5^\\circ\\), the oblique effect will cause an error; \\(R_n\\) will be a clockwise error, in the same direction as \\(R_{n-1}\\). Since \\(R_n\\) exhibits an error in the direction of \\(R_{n-1}\\), it will look like \\(R_{n-1}\\) caused an attraction.\nMore importantly, when the oblique effect acts on trial \\(n-1\\), it will cause responses to collect along the cardinal axes. That is, regardless of the orientation on trial \\(n-1\\), \\(R_{n-1}\\) will be close to either \\(0^\\circ\\) or \\(90^\\circ\\). This means that, whenever \\(O_n\\) is close to \\(22.5^\\circ\\), the oblique effect’s influence on the previous response, \\(R_{n-1}\\), makes it more likely that \\(R_{n-1}\\) will be approximately \\(22.5^\\circ\\) clockwise from \\(O_n\\), and then the oblique effect on trial \\(n\\) will further push the response toward \\(R_{n-1}\\).\nWe can see this play out empirically by looking at \\(O_n\\) as a function of \\(R_{n-1}\\); when \\(R_{n-1}\\) is close to \\(22.5^\\circ\\), there is an over-representation of orientations for which the oblique effect will bias responses toward the previous response.\n\n\n\n\nd |&gt; \n  filter(between(response_diff, 21, 24)) |&gt;\n  ggplot(aes(x=orientation)) +\n  geom_histogram(bins=30) +\n  scale_x_continuous(\n    name = \"orientation on current trial\",\n    labels = c(0, 90, 180),\n    breaks = c(0, 90, 180)) +\n  geom_vline(xintercept = c(22.5, 112.5), color=\"blue\") \n\n\n\n\n\n\n\n\n\n\nFigure 4: When the previous orientation is 22.5 clockwise, the current orientation tends to be either 22.5 or 112.5, which is when the oblique effect causes maximal error.\n\n\n\nWe can think about this from the other direction, too; when \\(R_{n-1}\\) is \\(22.5^\\circ\\) clockwise from \\(O_n\\), it’s relatively difficult for \\(O_n\\) to be around \\(67.5^\\circ\\). For example, when \\(O_n=67.5\\), the previous response could be \\(22.5^\\circ\\) if \\(O_n=45^\\circ\\), but nearly no other orientation would work; when \\(O_n\\) is near but not exactly \\(45^\\circ\\), the oblique effect on trial \\(n-1\\) will push \\(R_{n-1}\\) away from \\(45^\\circ\\), away from a response that could be \\(22.5^\\circ\\) clockwise to \\(O_n\\). This is important because, if it is rare for trial \\(n\\) to have both \\(O_n=67.5\\) and \\(R_{n-1}\\) be \\(22.5^\\circ\\) clockwise from \\(O_n\\), then the oblique effect will be imbalanced.\nTogether, this means that when the oblique effect on trial \\(n\\) causes a maximal clockwise error, the oblique effect on trial \\(n-1\\) makes it more likely that the previous response is also clockwise and less likely that it’s counterclockwise. The result is a spurious dependence on the previous response.\nWe can see this play out more generally by looking at the current orientation as a function of the previous orientations and responses.\n\n\n\n\nd |&gt;\n  pivot_longer(\n    cols = c(orientation_diff, response_diff), \n    names_to = \"covariate\",\n    names_pattern = \"(response|orientation)\") |&gt;\n  ggplot(aes(x=orientation, y=value)) +\n  facet_wrap(~covariate) +\n  geom_point() +\n  coord_fixed()  +\n  scale_y_continuous(\n    name = \"relative orientation/response on previous trial\",\n    labels = c(-90, 0, 90),\n    breaks = c(-90, 0, 90)) +\n  scale_x_continuous(\n    name = \"orientation on current trial\",\n    labels = c(0, 90, 180),\n    breaks = c(0, 90, 180))\n\n\n\n\n\n\n\n\n\n\nFigure 5: The current orientation is unrelated to the previous orientation, but there is a dependency on the previous response.\n\n\n\nAs expected, there is no relationship between \\(O_n\\) and \\(O_{n-1}\\), but there is a strong relationship between \\(O_n\\) and \\(R_{n-1}\\). When \\(O_n \\in (0,45)\\), then it’s likely that \\(R_{n-1} \\in (0,22.5)\\) (clockwise), or \\(R_{n-1} \\in (-90, -67.5)\\) (counterclockwise). The figure below shows the same data, but now the data are colored according to how the oblique effect will cause errors on trial \\(n\\).\n\n\n\n\nd |&gt;\n  na.omit() |&gt;\n  select(-response, -error) |&gt;\n  pivot_longer(\n    cols = c(orientation_diff, response_diff), \n    names_to = \"covariate\",\n    names_pattern = \"(response|orientation)\") |&gt;\n  ggplot(aes(x=orientation, y=value)) +\n  facet_wrap(~covariate) +\n  geom_point(aes(color=oblique)) +\n  scale_color_gradient2(low = scales::muted(\"blue\"), high = scales::muted(\"red\")) +\n  coord_fixed()  +\n  scale_y_continuous(\n    name = \"relative orientation/response on previous trial\",\n    labels = c(-90, 0, 90),\n    breaks = c(-90, 0, 90)) +\n  scale_x_continuous(\n    name = \"orientation on current trial\",\n    labels = c(0, 90, 180),\n    breaks = c(0, 90, 180))\n\n\n\n\n\n\n\n\n\n\nFigure 6: This is the same plot as above, but now the data have been colored according to how the oblique effect will cause errors. The point is that the errors caused by the oblique effect on trial \\(n\\) are balanced when looking at the orientation on the previous trial (consider a horizontal slice), but no such balancing happens when looking at the response on the previous trial.\n\n\n\nFortunately, this spurious bias isn’t too hard to adjust for3. But the point is that it would be a mistake to look at just a dependence on the previous orientation if there is an oblique effect; analyses must adjust for the oblique effect.\nI’m not sure if there is a similar issue with other domains (e.g., when participants discriminate tones, pain, faces, etc). Perhaps edge effects could cause a similar issue (e.g., if people are more likely to respond to the ends or middle of the scale)?"
  },
  {
    "objectID": "posts/2021-05-07-spurious-serial-dependencies/index.html#footnotes",
    "href": "posts/2021-05-07-spurious-serial-dependencies/index.html#footnotes",
    "title": "Spurious Serial Dependencies",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis effect occurs when participants are asked to report orientations. Participants are differently accurate across the range of orientations; they are maximally accurate when reporting \\(0^\\circ\\), \\(45^\\circ\\), \\(90^\\circ\\), and \\(135^\\circ\\), but minimally accurate at intermediate orientations (\\(22.5^\\circ\\), \\(67.5^\\circ\\), etc). The errors can either be clockwise or counterclockwise, depending on the experiment. For an overview, see Wei and Stocker (2015).↩︎\nThis won’t be used, but sequence of orientations could be written \\((O_n)_{n\\in\\mathbb{N}}\\), and the sequence of responses \\((R_n)_{n\\in\\mathbb{N}}\\). Selecting a particular trial involves dropping the parentheses; \\((O_n)_{n\\in\\mathbb{N}}\\) emphasizes the whole sequence, whereas \\(O_n\\) means take a particular (but arbitrary) element of the sequence. I am not a mathematician, and this post is a quick and dirty explanation mostly meant for later me, so don’t expect formality.↩︎\nIn a regression model of the errors, it would suffice to include a sinusoidal term.↩︎"
  },
  {
    "objectID": "posts/2018-06-05-eyetracking-with-eyelink-in-psychtoolbox/index.html",
    "href": "posts/2018-06-05-eyetracking-with-eyelink-in-psychtoolbox/index.html",
    "title": "eyetracking with eyelink in psychtoolbox",
    "section": "",
    "text": "UPDATE: I now think that the examples I’ve presented here obscure the interface with Eyelink. Much cleaner to use MATLAB’s object oriented programming. This is covered in another post.\nThis post is designed as minimal documentation for using the Eyelink software at the UMass Amherst hMRC. The goals are very modest\nThe main audience includes members of the cMAP and CEMNL labs at UMass, but other users of the hMRC may also benefit. This post includes various lines of code throughout this post, but the full files can be downloaded from the links at the bottom. Many of those links are private and will only work if you are a member of one of those labs.\nNOTE: This post is not designed to be a full introduction to the Eyelink toolbox within PTB. I’m not qualified to give a detailed tutorial. These are just a few bits of code that I have found useful. But, my needs have so far been really simple (i.e., make a record of where the eyes were during each run so that runs can be discarded if fixations during that run deviate more than x degrees from the center of the screen). The main resource in this post is probably the collection of links in the next section."
  },
  {
    "objectID": "posts/2018-06-05-eyetracking-with-eyelink-in-psychtoolbox/index.html#footnotes",
    "href": "posts/2018-06-05-eyetracking-with-eyelink-in-psychtoolbox/index.html#footnotes",
    "title": "eyetracking with eyelink in psychtoolbox",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nthough, I’ve already broken some of the logic I outline in that section by having more than one function with a switch statement.↩︎\nThe options relating to saving data are for another post. It seems like you can do quite a lot with the Eyelink Data Viewer when various event tags have been set up properly (see manual, on box ), but my needs are so simple that I haven’t bothered digging too deeply.↩︎\nOf course, a similar effect could be achieved by littering the experimental code with a bunch of if then else statements. However, this method has the advantage of massively reducing the number of switch statements in the code. Fewer switch statements can be easier to follow and modify, because most of the effect of the input.tracker variable can be localized to a single function (the definition of makeEyelinkFcn)↩︎"
  },
  {
    "objectID": "posts/2022-10-19-typehint-cache-decorator/index.html",
    "href": "posts/2022-10-19-typehint-cache-decorator/index.html",
    "title": "Hinting Cache Decorator Types",
    "section": "",
    "text": "Recently, I encountered a python function that I didn’t want to modify, but whose output I wanted cached. Providing typehints for this seemed a bit complicated, so I’m posting this gist as a reminder. The main trick involves extensions from PEP 612.\nIn words, the decorator will take some function f that takes parameters P and produces a str. I want to be able to call f and additionally provide a filename that will point to a file in which the string will be written. The cache will be super basic, checking just for the existence of the file and skipping f if that file exists.\nIn code, that looks like the following\n\nfrom pathlib import Path\nimport typing\nP = typing.ParamSpec(\"P\")\n\ndef cache_str(f: typing.Callable[P, str]) -&gt; typing.Callable[typing.Concatenate[Path, P], str]:\n    def wrapper(_filename: Path, *args: P.args, **kwargs: P.kwargs) -&gt; str:\n        if _filename.exists():\n            print(f\"Found cached result {_filename}. Skipping!\")\n            i = _filename.read_text()\n        else:\n            # run!\n            i = f(*args, **kwargs)\n            _filename.touch()\n            _filename.write_text(i)\n        return i\n    return wrapper\n\nAnd then the decorator could be used like\n\n@cache_str\ndef get_str(msg: str) -&gt; str:\n    print(\n        \"This function takes so long to run--I hope that I don't need to run it twice\"\n    )\n    return msg\n\n\nimport tempfile\n\nmsg = \"cached message\"\n\nwith tempfile.TemporaryDirectory() as tmpdir:\n    tmp_path = Path(tmpdir) / \"cache.txt\"\n    get_str(tmp_path, msg=msg)\n    print(\"trying to read message from cache...\")\n    print(f\"{tmp_path.read_text()=}\")\n    get_str(tmp_path, msg=msg)\n\nThis function takes so long to run--I hope that I don't need to run it twice\ntrying to read message from cache...\ntmp_path.read_text()='cached message'\nFound cached result /var/folders/v_/kcpb096s1m3_37ctfd2sp2xm0000gn/T/tmpzhczr6ch/cache.txt. Skipping!\n\n\nThis is neat, but although my editor recognizes that the decorated function can accept a path as the first argument, that first argument could not be named. This appears to have been an explicit choice in PEP 612, made to avoid potential clashes with keyword arguments from the decorated function."
  },
  {
    "objectID": "posts/2019-10-11-serial-dependence-reflects-a-preference-for-low-variability/index.html",
    "href": "posts/2019-10-11-serial-dependence-reflects-a-preference-for-low-variability/index.html",
    "title": "serial dependence reflects a preference for low variability",
    "section": "",
    "text": "One framework for understanding perception casts it as inference: just as a statistician uncovers noisy data to uncover patterns, an organism perceives when it converts sensations into guesses about its environment. The framework not concrete enough to be called a theory of perception, since it is not clear what data could falsify it1. But the framework can remind perceptual researchers about the many strategies available for modeling the world. Do perceiving organisms employ similar strategies?\nOne property that distinguishes many statistical strategies is a tradeoff between bias and variability Consider this tradeoff with an example. A statistician must estimate the average height of college-level soccer players. The true average could be uncovered by measuring the height of every player at every college–the statistician would not need inference. But the statistician is constrained by limited resources. They can only measure the players from a single college, though they may measure the heights of any student at the college. The statistician must now decide between an unbiased but variable or biased but precise strategy. Measuring only the soccer players gives an unbiased estimate, but with so few players the team’s average may be far from the true average. The statistician cannot be confident that the single team resembles all teams. Alternatively, the statistician may supplement their estimate with the heights of players from another, related sport. Since ultimate frisbee players may have similar heights to soccer players, incorporating their heights into the estimate may counteract any anomalously sized soccer players. However, incorporating even a single player from another sport biases the estimate, in the sense that the average height of all soccer players will not equal the average height of all soccer players and the one ultimate player2. One strategy – measure only soccer players – would give the true answer if there were enough resources, but the other strategy – measure everyone that is similar to a soccer player – may approximate the truth well with limited resources.\nThe bias-variability tradeoff gives a functional interpretation to the perceptual effect called serial dependence (Fischer and Whitney 2014; Cicchini, Mikellidou, and Burr 2018). Serial dependence occurs when participants judge perceptual stimuli across many trials, and their judgments on one trial depend on their immediately preceding judgment. Like the constrained statistician, participants may not process each stimulus completely: participants only see stimuli for brief durations, their judgments are made after the stimuli are masked, and their attention fluctuates throughout the hundreds of trials. The bias is often attractive, meaning that participants’ judgments reflect a blending of the stimuli on the current and previous trials. Serial dependence may reflect a strategy – not necessarily intentional – that reduces variability across judgments by combining information. Although the strategy biases the judgments, it may help each individual estimate approach truth."
  },
  {
    "objectID": "posts/2019-10-11-serial-dependence-reflects-a-preference-for-low-variability/index.html#footnotes",
    "href": "posts/2019-10-11-serial-dependence-reflects-a-preference-for-low-variability/index.html#footnotes",
    "title": "serial dependence reflects a preference for low variability",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA perceptual system does need to be limited to the statistical tools that have already been developed, so even a demonstration that organisms don’t employ any known statistical tool would not rule out the framework.↩︎\nAssume that the ultimate player is not as tall as the average soccer player↩︎"
  },
  {
    "objectID": "posts/2018-08-07-anonymizing-mturk-worker-ids/index.html",
    "href": "posts/2018-08-07-anonymizing-mturk-worker-ids/index.html",
    "title": "Anonymizing MTurk WorkerIDs",
    "section": "",
    "text": "It may be the case that Amazon Mechanical Turk WorkerIDs are not anonymous. Lease et al., 2013 describe at length how personally identifying information may be exposed when a researcher shares WorkerIDs. It is unclear to me the extent to which Amazon constructs their WorkerIDs at present, given that one of their striking demonstrations did not apply to my WorkerID. That is, they describe simply googling the WorkerID and receiving a picture of the participant, along with their full name. My WorkerID turn up nothing. Though, I have only been a worker on MTurk for a short while, so maybe I’ve been lucky and my ID has just not yet been shared widely.\nRegardless, providing extra anonymity to participants isn’t too much trouble. This post serves as documentation for a brief script that takes a sqlite database produced by running an experiment in jspsych + psiturk and replaces all instances of the WorkerID with a more secure code.\nThe script relies on five R libraries\n\nmagrittr, for ease of writing\ndplyr, through (dbplyr)[cran.r-project.org/web/packages/dbplyr/vignettes/dbplyr.html], serves as the way to interface with the sqlite database\nopenssl constructs a more secure identifier for each participant that can still be used to cross reference them across studies\nstringr does the work of replacing instances of the WorkerID with the more secure code\ndocopt, wraps up the Rscript such that it can be called from the command line (in an environment in which Rscript is the name of a function. i.e., may be Rscript.exe in Windows powershell)\n\n\n#!/usr/bin/env Rscript\n#\n# Anonymize participants database\n# NOTE: always overrides the file --outfile\n#\n\nlibrary(docopt)\n\ndoc &lt;- \"Usage: \n  anonymize_db.R [-i DBNAME] KEY OUTFILE\n  anonymize_db.R -h\n\nOptions:\n  -i --infile DBNAME     sqlite database filename from which to read [default: participants_raw.db]\n  -t --table TABLE       name of table within database to anonymize [default: participants]\n  -h --help              show this help text\n\nArguments:\n  KEY                   key to salt WorkerIDs for extra security\n  OUTFILE               sqlite database filename to write\"\n\n\nopt &lt;- docopt(doc)\n\nlibrary(magrittr)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(openssl)\n\ndb &lt;- dplyr::src_sqlite(opt$infile) %&gt;%\n  dplyr::tbl(opt$table) %&gt;%\n  dplyr::collect() %&gt;%\n  dplyr::mutate(\n    uniqueid = stringr::str_replace(\n      uniqueid, \n      workerid, \n      openssl::sha256(workerid, key = opt$KEY)),\n    datastring = dplyr::case_when(\n      is.na(datastring) ~ datastring,\n      TRUE ~ stringr::str_replace_all(\n        datastring, \n        workerid, \n        openssl::sha256(workerid, key = opt$KEY))),\n    workerid = openssl::sha256(workerid, key = opt$KEY)\n  )\nmessage(paste0(\"read raw database: \", opt$infile))\n\ncon &lt;- DBI::dbConnect(RSQLite::SQLite(), opt$OUTFILE)\ndplyr::copy_to(\n  con, db, opt$table,   \n  temporary = FALSE, \n  indexes = list(\n    \"uniqueid\" \n  ),\n  overwrite = TRUE)\n\nDBI::dbDisconnect(con)\nmessage(paste0(\"wrote anonymized database: \", opt$OUTFILE))\nmessage(paste0(\"Store your KEY securely if you want the same WorkerIDs to create the same HMACs!\"))\n\nAs stated in the initial string of this script, a typical call might be\nanonymize_db.R longandsecurelystoredsalt participants.db\nwhich will read in the sqlite database participants_raw.db (default for –infile), convert all instances of WorkerID into a hash-digest with the sha256 algorithm, and store the result in a new sqlite database called participants.db.\nThe general workflow would be to include in your .gitignore the raw database output by psiturk. That way, the raw database is never uploaded into any repository. Then, when you are ready to host the experiment, you pull your repository as usual. As an extra step, you will now need to separately move around your raw database such that when you run the next experiment psiturk will know which workers have already participated. After collecting data, retrieve the database and run this anonymization script on it. The newly created database can then be bundled with your repository.\nThis is a bit of extra work (i.e., you must manually send the database, retrieve the database, then anonymize it). However, the whole point is to avoid making it easy to download something with potentially identifying information.\n\nGotchas\n\nThis function will overwrite any database of the same name as OUTFILE. Though, that’s often not an issue. If you’ve anonymized a database (call the result participants.db), added new participants to the same raw database, and then anonymize the raw database again, those participants that were anonymized in the first round will be re-anonymized and included in the new result.\nIf you want this function to convert a given WorkerID into a consistent code, you’ll need to call it with the same value for KEY.\nIt would be more secure to use a salt of random length for each participant separately."
  },
  {
    "objectID": "posts/2019-09-19-serial-dependence/index.html",
    "href": "posts/2019-09-19-serial-dependence/index.html",
    "title": "Serial Dependence",
    "section": "",
    "text": "Objects in the visual environment move suddenly and erratically, and visual perception must be sensitive to the changes that are important. But each saccade and head tilt change the image imprinted on the retina, and to perceive every tremor ignores the stability of the visual environment; a desk will still look like a desk in a few seconds. The visual system must therefore balance the ability to detect subtle changes in the environment against the efficiency afforded by accurate predictions.\nThat the recent past influences current perception can be demonstrated easily. If you stare at Figure 1, you might observe that the Gabor has a bend immediately after changing orientations. The bend lasts for a moment, then straightens. But the bend is an illusion. While tracking Figure 1, the visual system allows for a momentary bias. Usefully, the bias is sensitive to experimental manipulation. Figure 2 shows the same Gabor with the same orientations, but the Gabor also moves. The movement largely eliminates the bending. The sensitivity of such biases to different experimental manipulations enables researchers to study how the visual system balances new information against the recent past.\nA closely effect is called serial dependence. Serial dependence occurs when participants report the orientations of sequentially presented, tilted Gabors (Fischer and Whitney 2014). A visual mask to reduces the strong aftereffects present in Figure 1 and Figure 2, Figure 31. Even without the aftereffect, the perceptual-decision about one Gabor affects the perceptual-decision about the next; participants report orientations that consistently err toward the orientation of the most recently seen Gabor. Reports on the magnitude of the effect vary, but the error has an average maximum of less than a few degrees. However, serial dependence is affected by different manipulations than that the demonstration of Figures Figure 1 and Figure 2. For example, it appears insensitive to the location of the Gabors. This bias may therefore provide a unique way to study how current perception is not only biased by but toward the recent past.\nHowever, it remains unclear whether serial dependence is a bias of perceptual or post-perceptual processes. That is, does serial dependence alter participants’ perception of the Gabors, or does it alter how they report the orientation? The sequential timing of each trial – in which participants respond in a designated period after seeing the Gabor – does not imply that participants decide on an orientation only after they have finished perceiving the Gabor. For example, a participant can make decisions before the response period, and they can adopt a biased response strategy even before seeing the Gabor. Where and when to delineate between perception and decision, or whether they can be delineated, depends on assumptions about the relationship between perception and decisions. A tool like the circular diffusion model can help make those assumptions explicit (Smith 2016)."
  },
  {
    "objectID": "posts/2019-09-19-serial-dependence/index.html#footnotes",
    "href": "posts/2019-09-19-serial-dependence/index.html#footnotes",
    "title": "Serial Dependence",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe timing and spacing of this figure does not quite match a typical experiment. For example, participants take a few seconds to respond, so the amount of time between Gabors in this figure is too short.↩︎"
  },
  {
    "objectID": "posts/2021-01-02-gan-mass/index.html",
    "href": "posts/2021-01-02-gan-mass/index.html",
    "title": "New England GAN",
    "section": "",
    "text": "A friend recently moved away from Western Mass, so I wanted to send them a gift to help remind them of the area. I also happened to want to learn a bit more about Generative Adversarial Networks (GANs)."
  },
  {
    "objectID": "posts/2021-01-02-gan-mass/index.html#footnotes",
    "href": "posts/2021-01-02-gan-mass/index.html#footnotes",
    "title": "New England GAN",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf I wanted perfect pictures, I could have just used a camera.↩︎\nThe van Gough example is slightly misleading; in practice, van Gough didn’t paint enough pictures to train a GAN. Training a GAN from scratch doesn’t require labeled data, but it still requires many images. There are tricks that could help a GAN, but simply training his images would likely be insufficient.↩︎\nHaving not owned a car during graduate school, I found it funny that these networks learned about New England through its highways↩︎\nBut also, they provided a helpful docker image, functions to prep the data, and decent documentation. This is a good reminder about the benefits of polishing a repository.↩︎\nAfter one day, the training error was still decreasing. But I was using a preemptible virtual machine, and so after 24 hours it was automatically shutdown.↩︎\nI removed the text from the curated examples, but it can be seen in the preview image at the top.↩︎"
  },
  {
    "objectID": "posts/2019-10-18-neural-modulation-for-serial-dependence/index.html",
    "href": "posts/2019-10-18-neural-modulation-for-serial-dependence/index.html",
    "title": "Modulations to tuning functions can bias evidence accumulation",
    "section": "",
    "text": "Perceptual decisions can be deconstructed with evidence accumulation models. These models formalize expectations about how participants behave, when that behavior involves repeatedly sampling information towards until surpassing a necessary threshold of information. At a cognitive level, the different models instantiate the components differently, but at a neural level the models rely on common mechanisms. To accumulate evidence the models assume two distinct populations of neurons. One population responds to available information. This population can be thought of as a sensory population, such that each neuron in the population represents one of the available options. The second population listens to the first, transforming the sensory activity into evidence for each decision and accumulating the evidence through time. This second population can be called an integrating population. While the location of the sensory population depends on the information that needs to be represented, the location of the integrating population depends on the required behavior. If participants must make decisions about orientations, the sensory population might be striatal neurons tuned to different orientations. If participants make decisions with saccades, the integrating population might be in the frontal eye fields. Understanding how these two populations reveals different ways that decisions can be biased.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Sensory channels evenly represent orientations. The curves from the sensory population represent the average activity of each neuron to a given orientation. Although the curves show average activity, at any given moment the actual activity of each neuron may be higher or lower. The integrating curve reflects the average evidence that the integrating population will record. The flatness of the integrating curve reflects an unbiased representation. Only eight neurons from the sensory population are shown to avoid overcrowding.\n\n\n\nFor the integrating population to accumulate evidence, it must transform the activity of the sensory population into a meaningful signal. Figure 1 depicts that transformation when participants must report orientations. Each neuron in the sensory population responds most strongly to a specific orientation, but all of them are active whenever an orientation is present. The function describing how a sensory neuron respond to different orientations is called the neuron’s tuning function. The integrating population will respond according to some other function of that sensory activity. One simple integrating function associates each neuron with its preferred orientation; the integrating population then tallies evidence based on whichever neuron is most active. This function requires the sensory population to represent each orientation with at least one neuron. If there are enough sensory neurons, the integrating population will be able to accumulate evidence for each orientation without bias.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Modulations to activity of the sensory population will provide the integrating population a biased representation of the orientation. Unlike in Figure 1, the sensory population in both panels provides the integrating population with an uneven representation of orientation.\n\n\n\nThe tuning characteristics of sensory neurons are variable, and this variability causes biases to emerge in the evidence accumulation process (Figure 2). One common alteration is an increased gain, whereby the tuning function is multiplied by some value. When the gains of tuning functions are altered heterogeneously, a neuron may have a higher activity even when the orientation it is responsible for is not present. The neurons with the highest gain will bias the evidence gathered by the integrating population. Alternatively, tuning functions might shift, along with the orientation each neuron signals. The shift causes the sensory population to over-represent of some orientations and leave others underrepresented. These alterations can provide advantages in certain circumstances. For example, a heterogeneously increased gain will be useful when some orientations are known to be more likely than others, and a shift will be useful when different orientations require differently precise responses. But to accumulate evidence without bias, the sensory population must restore more uniform tuning."
  },
  {
    "objectID": "posts/2019-11-21-thoughts-on-eye-movement/index.html",
    "href": "posts/2019-11-21-thoughts-on-eye-movement/index.html",
    "title": "thoughts on eye movement",
    "section": "",
    "text": "Visual perception research has produced many illusions. Stare at a waterfall for a minute, then look away and the whole world appears in motion, traveling upwards (Addams 1834). Given proper lighting, black paper can appear white, but placing a white piece of paper nearby colors the first dark gray (Gelb 1929; as cited by Cataliotti and Gilchrist 1995). Inspecting two sets of black lines – horizontal lines that obscure a solid red field like a picket fence, along with vertical lines that obscure a solid green field – causes the black lines alone to induce a perception of color, an illusory shading that can last for days (McCollough 1965). Such illusions reveal the intricacies of visual perception, kludges and all1. The pizzazz of the illusions affords visual perception a kind of scientific rigor; the effects are obviously real and reproducible, so to satisfyingly explain how visual perception works so well also requires explaining how atypical visual environments can so often dupe vision.\nHowever, research on visual perception inevitably strays from fascinating and easily demonstrable illusions. Of course, even without the glamour of classic visual illusions an effect can still be a reliable and valid object of research. But as the effect becomes more subtle, observing the effect requires increasingly complex analyses. Unfortunately, the most complex analyses, when misapplied, can also transmute noise into something that appears noteworthy2. So when a subtle effect relies entirely on a complex analysis, the effect becomes suspect. To highlight the obviousness of an effect, it can help to visualize and revisualize the data.\nA few weeks ago, I posted about an effect that warrants revisualizing, the apparent stability of visual perception. I discussed this stability as one of the reasons that perception exhibits serial dependence (Fischer and Whitney 2014), which is the tendency of visual perception to be slightly erroneous, resembling not an accurate reproduction of the input it receives but a mixture of current input and input from the past. This dependence may occur during perception to refine the inherently erratic input provided by the retina. I attempt to demonstrate the need for refinement with Figures 1-3. Figure 1 shows a stimulus from an ongoing experiment. In the experiment, the participant was required to simply hold their gaze still. The figure is a heatmap, showing that this participant successfully fixated on a small region of the stimulus. However, the heatmap obscures how fixating on a “small” region implies ample movement. Figure 2 recapitulates, in real time, how the gaze wandered during fixation. But then Figure 2 obscures what that wandering means for the visual system; whenever the eye moves, the retina receives (and so must then output) a different image. With Figure 3, I attempt to visualize what these eye movements mean for the retinal image. In Figure 3, the movements of the gaze are transferred to the stimulus, revealing how the retina receives a twitching stimulus. The effect to explain here is why fixating at the dot in Figure 1 – given that the eyes move as shown in Figure 2 – does not elicit the jumpy movie depicted in Figure 3, but instead elicits the stable image of Figure 1."
  },
  {
    "objectID": "posts/2019-11-21-thoughts-on-eye-movement/index.html#footnotes",
    "href": "posts/2019-11-21-thoughts-on-eye-movement/index.html#footnotes",
    "title": "thoughts on eye movement",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTo personally experience the illusions mentioned here – and many others – explore Michael Bach’s website.↩︎\nFull disclosure: I write this as someone who has written a paper on a novel development of an already obscure analysis, a development that I needed to support the claims in another paper. I am a kettle.↩︎"
  },
  {
    "objectID": "posts/2018-11-10-importance-sampling/index.html",
    "href": "posts/2018-11-10-importance-sampling/index.html",
    "title": "Basic Importance Sampling for Variance Reduction",
    "section": "",
    "text": "This post attempts to provide a brief intro to importance sampling, presenting a relatively simple situation in which importance sampling enables a speedup in Monte Carlo by reducing the variance of the estimate.\nA lot of the material will be an effort to remember material that was taught to be in a Stochastic Processes and Applications class. This post should be though of as my notes on the course notes, with supplementation by https://www.statlect.com/asymptotic-theory/importance-sampling. That is, it can safely be assumed that any content comes from those notes, excluding errors (which will be due to my translation)1."
  },
  {
    "objectID": "posts/2018-11-10-importance-sampling/index.html#footnotes",
    "href": "posts/2018-11-10-importance-sampling/index.html#footnotes",
    "title": "Basic Importance Sampling for Variance Reduction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nthis page is mostly a study page for upcoming comprehensive exams↩︎"
  },
  {
    "objectID": "posts/2019-10-30-staircases-for-thresholds/index.html",
    "href": "posts/2019-10-30-staircases-for-thresholds/index.html",
    "title": "Staircases for Thresholds",
    "section": "",
    "text": "Performing any experiment on cognition requires deciding which stimuli to use. Some experiments require participants to make many errors, requiring the stimuli to be challenging. In other experiments, participants must respond accurately, requiring stimuli that are easy but not so easy that participants lose attention. Moreover, participants behave idiosyncratically, so to avoid wasting either the researchers’ or participants’ time the stimuli ought to be tailored to each participant. To decide which stimuli to use, researchers can rely on a psychometric function (Figure 1). These functions describe a relationship between the intensity of a stimulus and how a participant responds to that stimulus, when responses can be classified as either a positive or negative. Precisely what is meant by ‘intensity’ and ‘positive or negative’ depends on the experimental task, but they roughly correspond to the amount of stimulation on each trial and how difficult it is to notice that stimulation. In a task in which participants must detect a pure tone that is occasionally presented over white noise, the intensity could be the volume of the tone and participants’ responses are positive when they detect the tone. With a psychometric function, deciding on a stimulus translates to picking the proportion of trials that should receive positive responses – picking the desired difficulty – and then using the intensity that elicits that behavior. This replaces the task of picking stimuli with inferring participants’ psychometric functions.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: A psychometric curve with threshold intensity. Psychometric curves relate the intensity of stimulation to the perception of stimulation, or the proportion positive responses. There are many parameterizations of these functions, but they are typically sigmoidal. The intensity at which, on average, half of responses are positive is often of interest. This intensity is called the threshold.\n\n\n\nTo infer psychometric functions, standard procedures exist, though these procedures have varied efficiency. In particular, when only a single stimulus intensity is required, it would be inefficient to estimate the entire function. Consider a researcher attempting to elicit half positive responses, behavior elicited by the so called threshold stimulus intensity. A simple procedure to infer the psychometric function involves presenting a wide range of stimulus intensities, fitting the function to the data, and using the estimated function to infer the threshold. Each datum increases the precision of the estimate, but some data will be more useful than others. Intensities close to the tails of the function will pin down the function at those tails, but functions with different thresholds can behave similarly in their tails (Figure 2). The threshold is most tightly constrained by responses to stimuli at the threshold (Levitt 1971). Therefore, an ideal procedure to estimate the threshold intensity would involve repeatedly presenting the threshold intensity. The ideal procedure is unfeasible, since if the threshold were known there would be no need for inference. But although the exact threshold intensity cannot be presented on every trial, certain procedures enable most trials to approximate the ideal.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Psychometric functions with different thresholds. Although behavior at the tails of these functions are similar, they have different thresholds.\n\n\n\nOne type of procedure that locates the threshold intensity, both simply and efficiently, is called a staircase. A staircase procedure changes the stimulus intensity on every trial based on how participants respond. A staircase that locates the threshold increases stimulus intensity after a participant makes a negative response and decreases the intensity after a participant responds positively. Even when the first stimulus has an intensity far from the threshold (Figure 3), the staircase brings the intensity to the threshold, a convergence that is ensured by the psychometric function. For example, when intensity is lower than the threshold, a participant tends to make negative responses. After a negative response, the contrast is increased. With an increased contrast, the participant will be more likely to make a positive response. If the intensity is still lower then then threshold, the participant will likely provide another negative response, causing the intensity increase further. After enough trials with intensity too low, the intensity will be pushed towards the threshold. If the intensity strays from the threshold, the same dynamics push the threshold back to threshold. The staircase forces the intensity to remain close to the threshold.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Example sequence of trials in which stimulus intensity is controlled by a staircase procedure. After each negative response, the intensity increases, and after each positive response the intensity decreases. Although the initial intensity was much lower than the threshold, the staircase brings the intensity close to threshold and then keeps it there.\n\n\n\n\n\n\n\n\n\n\nReferences\n\nLevitt, H. 1971. “Transformed up-down Methods in Psychoacoustics.” The Journal of the Acoustical Society of America 49 (2B): 467–77."
  },
  {
    "objectID": "posts/2019-11-08-staircases-for-thresholds-part2/index.html",
    "href": "posts/2019-11-08-staircases-for-thresholds-part2/index.html",
    "title": "Staircases for Thresholds, Part 2",
    "section": "",
    "text": "In last week’s post, I discussed how some experiments in cognitive psychology require researchers to pick a differently intense stimulus for each participant. In particular, I discussed a procedure for picking an intensity that elicits positive responses on approximately half of trials, the staircase procedure. In the staircase procedure, the researcher increases the intensity after every positive response and decreases the intensity after every negative response. If the participant completes another set of trials in which the intensity is fixed to the average of the intensities that were used during the staircase, the participant will provide positive responses approximately half of the time. But a researcher may want participants to give positive responses with a different proportion. A different proportion of responses can be achieved by transforming the staircase (Levitt 1971).\nThe original staircase produced an intensity that elicited half positive responses by balancing the proportion of positive and negative responses. Intuitively, to elicit a higher proportion of positive responses, the transformed staircase must tilt this balance by converging on a more intense stimulus. Any staircase affects responses by changing the stimulus intensity as a function of how participants behave. The staircase that changes the intensity after every response is called a one-up, one-down staircase; one negative response causes the intensity to go up, one positive response causes the intensity to go down. A transformed staircase can make a positive response more likely by making intensity decreases less likely. The names of transformed staircases are analogous to the one-up, one-down label: a one-up, two-down staircase increases the stimulus after any negative response and decreases the intensity after two positive responses; a two-up, three-down staircase increases the intensity after two negative responses and only decreases the intensity after three positive responses; and so on. Altering when the staircase increments the intensity alters the intensity at which the staircase converges.\nWe can use algebra to calculate the proportion of positive responses elicited by the intensity converged on by a staircase. As an example, consider a staircase that increases the intensity after a single positive response but decreases the intensity after two negative responses, a one-up, two-down staircase. Since the one-up, two-down regime results in fewer decreases than the one-up, one-down staircase, we should expect that the proportion of positive responses will be higher than half. For the calculation, note that there are three possible sequences of responses that result in an intensity change. Two of these sequences cause an increase, either a single negative or a positive followed by a negative. For the algebra later, let \\(p(x|i)\\) be the probability of obtaining a positive response at stimulus intensity, \\(i\\). Our goal is to solve for this probability. Participants can only provide either positive or negative responses, so the probability of obtaining a negative response to that stimulus is \\(1-p(x|i)\\). The probability of increasing the stimulus intensity away from intensity \\(i\\), \\(p(\\text{up|i})\\) is the sum of the probabilities for the two sequences:\n\\[\n\\begin{equation}\np(\\text{up|i}) = (1-p(x|i)) + p(x|i)(1-p(x|i)) \\\\\n\\end{equation}\n\\tag{1}\\]\nThere is only a single way in which the staircase decreases intensity: the participant must provide two positive responses in a row. The probability of the intensity decreasing away from \\(i\\), \\(p(\\text{down|i})\\) is equal to the probability of two positive responses to that stimulus, or\n\\[\n\\begin{equation}\np(\\text{down|i}) = p(x|i)^2 \\\\\n\\end{equation}\n\\tag{2}\\]\nCombining Equation 1 and Equation 2 will give a relationship that determines the proportion of positive responses participants will tend to provide under this staircase. To see how to combine these equations, remember that the one-up, one-down staircase converged on an intensity for which the proportion of positive and negative responses were equal. This equality occurred because at that intensity, the probability of an up and down step were equally likely. So, to determine at which intensity the one-up, two-down staircase converges, we must determine the probability of a positive response that will make an up and down step likely in the one-up, two-down staircase. That is, we set the right hand sides of Equation 1 and Equation 2 to be equal, and solve for \\(p(x|i)\\)1.\n\\[\n\\begin{equation}\n\\begin{aligned}\np(x|i)(1-p(x|i)) + (1-p(x|i)) & = p(x|i)^2 \\\\\n\\implies p(x|i) - p(x|i)^2 + 1-p(x|i) & = p(x|i)^2 \\\\\n\\implies -2 p(x|i)^2 & = -1 \\\\\n\\implies p(x|i)& = \\frac{1}{\\sqrt{2}} \\\\\n& \\approx 0.707\n\\end{aligned}\n\\end{equation}\n\\tag{3}\\]\nEquation 3 shows that a one-up, two-down staircase will converge on a stimulus intensity that elicits approximately 70% positive responses (Figure 1). As one way to see that this solution makes sense, relate this solution back to the probabilities of making either an up or down step. By Equation 2, this solution implies that at this intensity, that an up step occurs with a 50% probability. As desired, any sequence that elicits a transition has an equal chance of being one that elicits either an up or down step.\nThe original staircase procedure capitalized on the idea that the stimulus intensity which elicits half positive responses can be estimated by starting from an arbitrary intensity, changing the intensity on every trial based on whether a participant responded positively or negatively, and then retroactively looking at which intensities were shown. The transformed staircase enables estimation of an intensity that elicits different behavior. Similar algebra to that outlined in this post can be used to determine the proportion of positive responses elicited by other staircases. Unfortunately, most proportions will not have an easy staircase regime. Moreover, complex staircases will only change the stimulus intensity infrequently, requiring more trials to estimate the converged upon intensity stably. However, the proportions reachable by simple staircase are often good enough; rare is the experiment that requires, not 70.7% but 73% positive responses. And the staircase procedure did not require any knowledge of the exact shape of the psychometric function, just that there was a psychometric function. The simplicity of the transformed staircase makes it an attractive way to pick an intensity."
  },
  {
    "objectID": "posts/2019-11-08-staircases-for-thresholds-part2/index.html#footnotes",
    "href": "posts/2019-11-08-staircases-for-thresholds-part2/index.html#footnotes",
    "title": "Staircases for Thresholds, Part 2",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe equation will have two solutions, but one of those solutions will be negative. A negative value is not an actual solution, because we are dealing with probabilities and so there is an additional constraint that \\(0 \\leq p(x|i) \\leq 1\\)↩︎"
  },
  {
    "objectID": "posts/2019-05-25-eyetracking-with-eyelink-in-psychtoolbox-now-with-oop/index.html",
    "href": "posts/2019-05-25-eyetracking-with-eyelink-in-psychtoolbox-now-with-oop/index.html",
    "title": "eyetracking with eyelink in psychtoolbox, now with oop",
    "section": "",
    "text": "I’ve started trying out MATLAB’s OOP after mounting suspicion that the way I’d been coding experiments basically involved making something that looked and behaved like an object–but did so in a convoluted and inefficient way. See this post on eyetracking with PTB as proof.\nThis post is brief, and is about as well thought out as a github gist/gitlab snippet.\nThe two classes I’ll work with here is a Window class and a Tracker class. The window class has 3 methods. The first constructor method exists just to create the object. The constructed object will have a few default properties of the class. The second method is open, which (can you guess?) calls the PTB functions to open an onscreen window. The open method is fancier than it needs to be for this post (note the PsychImaging configuration, and the optional debugLevel flag). The final window method is the desctructor. The destructor method is one of the advantages of leaning on MATLAB’s OOP syntax. That method will get called whenever the Window object’s lifecycle has ended (which might happen from explicit deletion of the object, closing MATLAB, the object is no longer referenced in the call stack, etc).\nThe second class is the Tracker class, which interfaces with Eyelink. The Window class is only present here because Eyelink needs an open window to run calibration. There are five Tracker methods, but they are either analogous to the Window objects methods (constructor, destructor) or were largely presented in the previous post."
  },
  {
    "objectID": "posts/2019-05-25-eyetracking-with-eyelink-in-psychtoolbox-now-with-oop/index.html#window-object",
    "href": "posts/2019-05-25-eyetracking-with-eyelink-in-psychtoolbox-now-with-oop/index.html#window-object",
    "title": "eyetracking with eyelink in psychtoolbox, now with oop",
    "section": "Window Object",
    "text": "Window Object\n\n\nclassdef Window &lt; handle\n    % Window handles opening and closing of screen\n    \n    properties (Constant)\n        screenNumber = 0\n        \n        % background color of screen\n        background = GrayIndex(Window.screenNumber)\n    end\n    \n    properties\n        pointer\n        winRect\n    end\n    \n    methods\n        function obj = Window()\n        end\n        \n        function open(obj, skipsynctests, debuglevel)\n            \n            PsychImaging('PrepareConfiguration');\n            PsychImaging('AddTask', 'General', 'FloatingPoint16Bit');\n            \n            Screen('Preference', 'SkipSyncTests', skipsynctests);\n            switch debuglevel\n                % no debug. run as usual, without listening to keyboard input\n                % and also hiding the cursor\n                case 0\n                    ListenChar(-1);\n                    HideCursor;\n                    [obj.pointer, obj.winRect] = ...\n                        PsychImaging('OpenWindow', obj.screenNumber, obj.background);\n                % light debug: still open fullscreen window, but keep keyboard input\n                case 1\n                    [obj.pointer, obj.winRect] = ...\n                        PsychImaging('OpenWindow', obj.screenNumber, obj.background);\n                % full debug: only open transparent window\n                case 10\n                    PsychDebugWindowConfiguration(0, .5)\n                    [obj.pointer, obj.winRect] = ...\n                        PsychImaging('OpenWindow', obj.screenNumber, obj.background);\n            end\n                        \n            % Turn on blendfunction for antialiasing of drawing dots\n            Screen('BlendFunction', obj.pointer, 'GL_SRC_ALPHA', 'GL_ONE_MINUS_SRC_ALPHA');\n            \n            topPriorityLevel = MaxPriority(obj.pointer);\n            Priority(topPriorityLevel);\n        end\n        \n        % will auto-close open windows and return keyboard control when\n        % this object is deleted\n        function delete(obj) %#ok&lt;INUSD&gt;\n            ListenChar(0);\n            Priority(0);\n            sca;\n        end\n    end\n    \nend"
  },
  {
    "objectID": "posts/2019-05-25-eyetracking-with-eyelink-in-psychtoolbox-now-with-oop/index.html#tracker-object",
    "href": "posts/2019-05-25-eyetracking-with-eyelink-in-psychtoolbox-now-with-oop/index.html#tracker-object",
    "title": "eyetracking with eyelink in psychtoolbox, now with oop",
    "section": "Tracker Object",
    "text": "Tracker Object\nThe tracker object will mostly do what it did in the previous post. Same functionality, but the syntax is much cleaner than the heavy use of switch/case conditionals.\n\nclassdef Tracker &lt; handle\n    \n    properties\n        % flag to be called in scripts which enable turning on or off the tracker\n        % in an experiment (e.g., when debug mode is on)\n        using_tracker logical = false\n        \n        % name of the write. must follow eyelink conventions. alphanumeric only, no \n        % more than 8 characters\n        filename char = ''\n        \n        % eyelink object structure. stores many relevant parameters\n        el\n    end\n    \n    methods\n        function obj = Tracker(using_tracker, filename, window)            \n            obj.using_tracker = using_tracker;\n            obj.filename = filename;\n            \n            % run calibration for tracker (see method below)\n            calibrate(obj, window);\n        end\n        \n        function calibrate(obj, window)\n            if obj.using_tracker\n                % Provide Eyelink with details about the graphics environment\n                % and perform some initializations. The information is returned\n                % in a structure that also contains useful defaults\n                % and control codes (e.g. tracker state bit and Eyelink key values).\n                obj.el = EyelinkInitDefaults(window.pointer);\n                \n                if ~EyelinkInit(0, 1)\n                    error('\\n Eyelink Init aborted \\n');\n                end\n                \n                %Reduce FOV for calibration and validation. Helpful when the\n                % the stimulus is only in the center of the screen, or at places\n                % like the fMRI scanner at UMass where the eyes have a lot in front\n                % of them\n                Eyelink('Command','calibration_area_proportion = 0.5 0.5');\n                Eyelink('Command','validation_area_proportion = 0.5 0.5');\n                \n                % open file to record data to\n                status = Eyelink('Openfile', obj.filename);\n                if status ~= 0\n                    error('\\n Eyelink Init aborted \\n');\n                end\n                \n                % Setting the proper recording resolution, proper calibration type,\n                % as well as the data file content;\n                Eyelink('Command','screen_pixel_coords = %ld %ld %ld %ld', 0, 0, window.winRect(3)-1, window.winRect(4)-1);\n                Eyelink('message', 'DISPLAY_COORDS %ld %ld %ld %ld', 0, 0, window.winRect(3)-1, window.winRect(4)-1);\n                % set calibration type to 5 point.\n                Eyelink('Command', 'calibration_type = HV5');\n                \n                % set EDF file contents using the file_sample_data and\n                % file-event_filter commands\n                % set link data thtough link_sample_data and link_event_filter\n                Eyelink('Command', 'file_event_filter = RIGHT,FIXATION,SACCADE,BLINK,MESSAGE,BUTTON,INPUT');\n                Eyelink('Command', 'link_event_filter = RIGHT,FIXATION,SACCADE,BLINK,MESSAGE,BUTTON,INPUT');\n                \n                % check the software version\n                % add \"HTARGET\" to record possible target data for EyeLink Remote\n                Eyelink('command', 'file_sample_data  = RIGHT,GAZE,HREF,GAZERES,AREA,HTARGET,STATUS,INPUT');\n                Eyelink('command', 'link_sample_data  = RIGHT,GAZE,HREF,GAZERES,AREA,HTARGET,STATUS,INPUT');\n                \n                % make sure we're still connected.\n                if Eyelink('IsConnected')~=1\n                    error('\\n Eyelink Init aborted \\n');\n                end\n                \n                % set sample rate in camera setup screen\n                Eyelink('Command', 'sample_rate = %d', 1000);\n                \n                % opens up main calibration scheme\n                EyelinkDoTrackerSetup(obj.el);\n            end\n        end\n        \n        function status = eyelink(obj, varargin)\n            % calls main Eyelink routines only when\n            % this tracker object property using_tracker==true.\n            status = [];\n            if obj.using_tracker\n                if nargin==2\n                    % construct calls to eyelink that don't output any\n                    % status\n                    if strcmp(varargin{1}, 'StopRecording') || ...\n                            strcmp(varargin{1}, 'Shutdown') ||...\n                            strcmp(varargin{1}, 'SetOfflineMode')\n                            \n                        % magic happens here, where the variable argument input\n                        % is expanded an repassed through to Eyelink()\n                        Eyelink(varargin{:});\n                    else\n                        status = Eyelink(varargin{:});\n                    end\n                % all calls to Eyelink that have more than two inputs (e.g., the\n                % name of a function with some parameters to that function) return\n                % some status\n                else\n                    status = Eyelink(varargin{:});\n                end\n            end\n        end\n        \n        % starts up the eyelink machine. call this once the start of each\n        % experiment. could modify function to also draw something special\n        % to the screen (e.g., a background image). this might be the kind\n        % of function to modify if you wanted to draw trial-by-trial material\n        % to the eyetracking computer\n        function startup(obj)\n            \n            % Must be offline to draw to EyeLink screen\n            obj.eyelink('SetOfflineMode');\n            \n            % clear tracker display and draw background img to host pc\n            obj.eyelink('Command', 'clear_screen 0');\n            \n            % draw simple fixation cross as later reference\n            obj.eyelink('command', 'draw_cross %d %d', 1920/2, 1080/2);\n            \n            % give image transfer time to finish\n            WaitSecs(0.1);\n        end\n        \n        % destructor function will get called whenever tracker object is deleted (e.g.,\n        % this function is automatically called when MATLAB closes, meaning you can't\n        % forget to close the file connection with the tracker computer).\n        function delete(obj)\n          % waitsecs occur because the filetransfer often takes a moment, and moving\n          % on too quickly will result in an error\n            \n            % End of Experiment; close the file first\n            % close graphics window, close data file and shut down tracker\n            obj.eyelink('StopRecording');\n            WaitSecs(0.1); % Slack to let stop definitely happen\n            obj.eyelink('SetOfflineMode'); \n            \n            obj.eyelink('CloseFile');\n            WaitSecs(0.1);\n            \n            obj.eyelink('ReceiveFile', obj.filename, fullfile(pwd,'events'), 1);\n            WaitSecs(0.2);\n            obj.eyelink('Shutdown');\n        end\n        \n    end\nend"
  },
  {
    "objectID": "posts/2019-05-25-eyetracking-with-eyelink-in-psychtoolbox-now-with-oop/index.html#run-the-calibration-and-use-the-tracker",
    "href": "posts/2019-05-25-eyetracking-with-eyelink-in-psychtoolbox-now-with-oop/index.html#run-the-calibration-and-use-the-tracker",
    "title": "eyetracking with eyelink in psychtoolbox, now with oop",
    "section": "Run the calibration (and use the tracker)",
    "text": "Run the calibration (and use the tracker)\nPutting this together, the following script starts calibration, and outlines how this tracker could be used in an experiment.\n\n\n%% input \n% ------------------\n\nskipsynctests = 2;\ndebuglevel = 0;\nusing_tracker = true;\n\n%% setup\n% ------------------\n  \n% boilerplate setup\nPsychDefaultSetup(2);\n\n% initialize window\nwindow = Window();\n\n% open that window\nopen(window, skipsynctests, debuglevel)\n\n% Initialize tracker object\ntracker = Tracker(using_tracker, 'OOPDEMO.edf', window);\n\n% run calibration\ntracker.startup();\n\n% Let Eyelink know that the experiment starts now\ntracker.eyelink('message', 'SYNCTIME');\n\n%% Experiment/trial code\n% ------------------\n\n% note that we should not need to wait to start recording,\n% given that the stimulus will always be drawn a bit later\n% (determined by how often phase changes occur)\ntracker.eyelink('StartRecording');\n\n% trial/experiment happens here ...\n\ntracker.eyelink('StopRecording');\n% Wait moment to ensure that tracker is definitely finished with the last few samples\nWaitSecs(0.001);\n\n%% Cleanup\n% ------------------\n\n% closes connection to Eyelink system, saves file\ndelete(tracker);\n\n% closes window, restores keyboard input\ndelete(window);\n\nWhat’s nice about this syntax (as before) is that only very minimal changes are required you don’t want to call the Eyelink functions (e.g., if you’re testing on a computer that doesn’t have the Eyelink system connected, or you’re debugging other parts of the experiment). By changing just the input, the Eyelink functions won’t be called.\n\n\nusing_tracker = false;\n\n% all the rest as above\n% ..."
  },
  {
    "objectID": "posts/2022-04-23-tf-dataset-from-3d-nifti/index.html",
    "href": "posts/2022-04-23-tf-dataset-from-3d-nifti/index.html",
    "title": "How to store a NifTi as a TFRecord",
    "section": "",
    "text": "A recent project required sending brain images to TensorFlow. Unfortunately, the data exceeded memory and so during training would need to be read from the disk. Poking around the TF documentation, it seems like a recommended way to do this is to store the images as a TFRecord. The steps for doing that are collected in this gist1.\nThere are five main steps\nimport math\nfrom numbers import Number\nimport numpy as np\nimport tensorflow as tf\nimport nibabel as nb\nfrom nilearn import plotting\n\nfrom typing import List\nimport numpy.typing as npt"
  },
  {
    "objectID": "posts/2022-04-23-tf-dataset-from-3d-nifti/index.html#preprocess",
    "href": "posts/2022-04-23-tf-dataset-from-3d-nifti/index.html#preprocess",
    "title": "How to store a NifTi as a TFRecord",
    "section": "Preprocess",
    "text": "Preprocess\nA full implementation might involve several preprocessing steps (e.g., registration, masking, cropping), but in this gist we’ll just rescale the images to have intensity values that range from 0-1.\n\ndef to_numpy(img: nb.Nifti1Image) -&gt; np.ndarray:\n    return np.asanyarray(img.dataobj)\n\ndef load_and_preprocess(img: str) -&gt; np.ndarray:\n    # convert to numpy array and preprocess\n    nii = to_numpy(nb.load(img))\n    # rescale to 0-1\n    preprocessed = (nii - nii.min())/(nii.max() - nii.min()).astype(np.float32)\n    return preprocessed\n\nSince this method of storing data was new for me, I wanted to ensure that I didn’t mess up the data. For that, I mainly relied on a basic plot of the images.\n\ndef plot_array(img: np.ndarray) -&gt; None:\n    nii = nb.Nifti1Image(img, affine=np.eye(4)*2)\n    plotting.plot_anat(nii)\n\nThis first step is standard neuroimaging, but just to check that the functions are working preprocess an example brain and see how it looks.\n\n# example images packaged with fsl, found at $FSLDIR/data/standard\nimg = ['MNI152lin_T1_2mm.nii.gz']\npreprocesed = load_and_preprocess(img[0])\nplot_array(preprocesed)\n\n\n\n\n\n\n\n\nGreat! That looks like a brain. I’ll use it as a reference to ensure that the roundtrip processing, serializing and unserializing, returns the arrays we need."
  },
  {
    "objectID": "posts/2022-04-23-tf-dataset-from-3d-nifti/index.html#serialize",
    "href": "posts/2022-04-23-tf-dataset-from-3d-nifti/index.html#serialize",
    "title": "How to store a NifTi as a TFRecord",
    "section": "Serialize",
    "text": "Serialize\nHere’s where TensorFlow starts. As those that came before have always done, we’ll rely on these incantations.\n\ndef _bytes_feature(value: bytes):\n    \"\"\"\n    Returns a bytes_list from a string / byte.\n\n    Example:\n          _bytes_feature(b'\\x00')\n          _bytes_feature(b'a')\n    \"\"\"\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\ndef _float_feature(value: Number):\n    \"\"\"\n    Returns a float_list from a float / double.\n    \n        Example:\n          _float_feature(2)\n          _float_feature(2.)\n    \"\"\"\n    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n\nThose functions seem to help ensure that the serialized data ends up with appropriate types. But the full steps involve serializing both the image and the label together.\n\ndef serialize_example(img: str, label: Number):\n  \n    # convert to numpy array and preprocess\n    preprocessed = load_and_preprocess(img)\n\n    # Notice how the numpy array is converted into a byte_string. That's serialization in memory!\n    feature = {\n        'label': _float_feature(label),\n        'image_raw': _bytes_feature(tf.io.serialize_tensor(preprocessed).numpy())\n    }\n\n    return tf.train.Example(features=tf.train.Features(feature=feature))\n\nTo get the image back out, we need two more functions: one to unpack/parse the example (decode_example) and another to unserialize/parse (parse_1_example) the unpacked example.\n\ndef decode_example(record_bytes) -&gt; dict:\n    example = tf.io.parse_example(\n        record_bytes,     \n        features = {\n          \"label\": tf.io.FixedLenFeature([], dtype=tf.float32),\n          'image_raw': tf.io.FixedLenFeature([], dtype=tf.string)\n          }\n    )\n    return example\n\ndef parse_1_example(example):\n    \"\"\" \n    Note that the network I was using worked with 3D images, and so the batches of data were of shape `(batch_size, x_dim, y_dim, z_dim, 1)`, rather than what is typical for 2d images: `(batch_size, x_dim, y_dim, n_channels)`. \n    \"\"\"\n    X = tf.io.parse_tensor(example['image_raw'], out_type=tf.float32)\n\n    # the images output by tf.io.parse_tensor will have shape (x_dim, y_dim, z_dim), which is to say that they're missing the channels dimension. expand_dims is used to indicate channel (i.e., be explicit about grayscale)\n    return tf.expand_dims(X, 3), example['label'] \n\nAt this point, we have functions for preprocessing the images, serializing them2, and packing each of them along with their labels into a tf.train.Example. This is all for converting the data into a format that can then be more easily written to and read from the disk."
  },
  {
    "objectID": "posts/2022-04-23-tf-dataset-from-3d-nifti/index.html#write-serialized-examples-as-tfrecords",
    "href": "posts/2022-04-23-tf-dataset-from-3d-nifti/index.html#write-serialized-examples-as-tfrecords",
    "title": "How to store a NifTi as a TFRecord",
    "section": "Write serialized examples as TFRecords",
    "text": "Write serialized examples as TFRecords\nSo far, everything has been in memory. Next comes a function that performs the above steps sequentially on several examples, and along the way writes the examples as a TFRecord.3\n\ndef write_records(niis: List[str], labels: npt.ArrayLike, n_per_record: int, outfile: str) -&gt; None:\n    \"\"\"\n    store list of niftis (and associated label) into tfrecords for use as dataset\n\n    Args:\n        niis: files that will be preprocessed and stored in record\n        labels: true label associated with each element in niis. these are the \"y\"\n        n_per_record: number of examples to store in each record. TF documentation advises that the files are +100Mb. Around 400 images cropped images at 2mm resolution seems to work.\n        outfile: base prefix to use when writing the records\n\n    Returns:\n        Nothing, but the records will be written to disk. \n\n    \"\"\"\n    n_niis = len(niis)\n    n_records = math.ceil(len(niis) / n_per_record)\n\n    for i, shard in enumerate(range(0, n_niis, n_per_record)):\n        print(f\"writing record {i} of {n_records-1}\")\n        with tf.io.TFRecordWriter(\n                f\"{outfile}_{i:0&gt;3}-of-{n_records-1:0&gt;3}.tfrecords\", \n                options= tf.io.TFRecordOptions(compression_type=\"GZIP\")\n        ) as writer:\n            for nii, label in zip(niis[shard:shard+n_per_record], labels[shard:shard+n_per_record]):\n                example = serialize_example(img=nii, label=label)\n                writer.write(example.SerializeToString())"
  },
  {
    "objectID": "posts/2022-04-23-tf-dataset-from-3d-nifti/index.html#create-dataset",
    "href": "posts/2022-04-23-tf-dataset-from-3d-nifti/index.html#create-dataset",
    "title": "How to store a NifTi as a TFRecord",
    "section": "Create Dataset",
    "text": "Create Dataset\nFor this gist, let’s store several copies of the MNI 2mm brain.\n\n# (e.g., put nifti of label MNI152_T1_1mm_brain.nii.gz in the working directory)\nn_imgs = 3\nmni_nii = ['MNI152lin_T1_2mm.nii.gz'] * n_imgs\n\n# store examples in each tfrecord. number of examples per record is configurable.\n# aim for as many examples as produces files of size &gt; 100M \nprefix = \"tmp\"\nwrite_records(mni_nii, np.arange(n_imgs), n_imgs, prefix)\n\nwriting record 0 of 0\n\n\nCalling the above will write a TFRecord file to disk. To read that record, define a pipeline that will create a tf.Data.dataset.\n\ndef get_batched_dataset(files, batch_size: int = 32) -&gt; tf.data.Dataset:\n    # Note: an actual pipeline would probably include at least the shuffle and prefetch methods\n    dataset = (\n        tf.data.Dataset.list_files(files) # note shuffling is on by default, changes order when there are multiple records\n        .flat_map(lambda x: tf.data.TFRecordDataset(x, compression_type=\"GZIP\"))\n        .map(decode_example)\n        .map(parse_1_example)\n        .batch(batch_size)\n    )\n    return dataset\n\nNow, use that function to read the records back.\n\n# a full dataset will have a list with many records\nlist_of_records=[f'{prefix}*.tfrecords']\nds = get_batched_dataset(list_of_records, batch_size=2)\n\nThat dataset, ds, was our goal in the gist. It can be passed to methods of the tf.keras.Model, including [tf.keras.Model.fit()(https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit)], serving up more data than can fit in memory.\nBut first, the serialization is a lot, so it is a good idea to verify that the images look okay when loaded. To do so, pluck a single example from the dataset.\n\n(Xs, Ys) = next(ds.as_numpy_iterator())\n\nThe dimensions of the labels are relatively easy. It’s a 1d array with as many elements as are in the batch.\n\n# (batch_size, )\nYs.shape\n\n(2,)\n\n\nThe order will depend on what shuffling is embedded in the dataset pipeline. In this case, there was no shuffling and so we should expect that the order is preserved.\n\nYs\n\narray([0., 1.], dtype=float32)\n\n\nThe data, Xs, also has a predictable shape.\n\n# (batch_size, x_dim, y_dim, z_dim, 1)\nXs.shape\n\n(2, 91, 109, 91, 1)\n\n\nTake that first element in the batch and plot.\n\nparsed_img = np.squeeze(Xs[0,])\nplot_array(parsed_img)\n\n\n\n\n\n\n\n\nThat looks great! Just in case, let’s check more explicitly\n\nnp.array_equal(preprocesed, parsed_img)\n\nTrue\n\n\nYay! done"
  },
  {
    "objectID": "posts/2022-04-23-tf-dataset-from-3d-nifti/index.html#footnotes",
    "href": "posts/2022-04-23-tf-dataset-from-3d-nifti/index.html#footnotes",
    "title": "How to store a NifTi as a TFRecord",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nInspiration for writing came from responding to this question on NeuroStars, and also from an urge to try a python-based post.↩︎\nThis serializing stuff is spooky black magic. I’m going to skip over those details and instead leave this reference, a journey through serializing in R https://blog.djnavarro.net/posts/2021-11-15_serialisation-with-rds/.↩︎\nIt seems to me that this storage on disk involves a second serialization step (where the first was done by tf.io.serialize_tensor in serialize_example). Even so, I assume the roundtrip isn’t so much of a big deal, considering that the data can be served to the model in parallel.↩︎"
  },
  {
    "objectID": "posts/2020-03-14-counterbalanced-continuous-designs-with-eulerian-walks/index.html",
    "href": "posts/2020-03-14-counterbalanced-continuous-designs-with-eulerian-walks/index.html",
    "title": "counterbalanced continuous designs with eulerian walks",
    "section": "",
    "text": "Many experiments require counterbalancing sequences of trials. For example, I’m currently running an experiment on serial dependence1. In my experiment, participants report the orientation of a grating2 stimulus on each trial. The serial dependence effect is how their responses on one trial depend on either the orientation of the previous trial or their response on that trial. To tease apart the effects of prior stimuli from prior responses, I’m manipulating the visual contrast of the gratings ( Michelson contrast ). There are three levels of contrast: high, low, and zero (at zero contrast, there is no grating stimulus). This experiment will only need a few of the eight possible pairs of contrasts, and I’d like a sequence of trials that does not have any filler trials. So I need a flexible way to generate sequences of contrast.\nIt turns out that this problem can be formulated as constructing an Eulerian, directed cycle. There are likely other ways 3, but I think this is a neat approach. I won’t talk much about why any of this works, primarily because I don’t feel qualified to do so. However, the post includes a script that implements the algorithm, and checks that it has worked. So, hopefully it’ll be useful to at least a future me. But before discussing an Eulerian circuit, let’s talk about formulating the stimulus conditions as a graph."
  },
  {
    "objectID": "posts/2020-03-14-counterbalanced-continuous-designs-with-eulerian-walks/index.html#footnotes",
    "href": "posts/2020-03-14-counterbalanced-continuous-designs-with-eulerian-walks/index.html#footnotes",
    "title": "counterbalanced continuous designs with eulerian walks",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWell, I was running. Out of precaution for COVID-19, it currently seems like a bad idea to try to collect more participants. And UMass is closed for the rest of the semester.↩︎\nadjacent black and white lines cropped to a circle, where the transitions between luminance follows a sinusoid↩︎\nIn this particular case, a simpler solution would be to assign each pair of contrasts a number. For example,\n\nhigh -&gt; high\nlow -&gt; high\nzero -&gt; high\n\nAn appropriate sequence could be generated by simply permuting the numbers. For example 2, 3, 1, 3, 2 In that case, the sequence of trials would be low high zero high high high zero high low high. This works because the second trial of each of the transitions are high. But what if you also wanted a few low-&gt;low and zero-&gt;zero transitions, but wanted neither low-&gt;zero nor zero-&gt;low? By simply permuting the number codes, a zero-&gt;zero transition could appear right after a low-&gt;low transition, but to do that would require a filler low-zero.↩︎\na cycle or circuit is a walk that starts and ends at the same node↩︎"
  },
  {
    "objectID": "posts/2019-09-12-circular-diffusion-model-of-response-times/index.html",
    "href": "posts/2019-09-12-circular-diffusion-model-of-response-times/index.html",
    "title": "Circular Diffusion Model of Response Times",
    "section": "",
    "text": "Many cognitive experiments involve asking participants to answer questions that require circular responses (Figure 1). What was the color of the shape you just saw? In which direction was the arrow pointing? How tilted was the bar? The answers required by these questions differ fundamentally from the more common, categorical responses required to questions. Was the color green or red? Was the arrow pointing left or right? Was the bar tilted more than 45 degrees from vertical, between 45-90, or more than 90 degrees? In the continuous case, the experimenter looses the ability to classify responses as either correct or incorrect, and an analysis must consider participants’ degree of inaccuracy, their relative error. Circularity adds the additional complication that a response can only be erroneous up to a point; if a person responds that a vertical bar is 3 degrees offset from vertical on trial one and 359 degrees on trial two, the analysis must acknowledge that the average is close to truth. Although many models exist that describe how a participant will respond when the choice is binary, models of these are much more limited.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Cognitive experiments often require participants to provide a circularly continuous response. A participant might be asked to study shapes of different colors, hold the colors of every shape in memory, and then report on the one shape’s color.\n\n\n\nSmith (2016) present a new model of how participants provide circularly continuous responses, called the circular diffusion model. It is a model of the decision-making process, analyzing both the numerical value participants provided and how long it took them to provide a response. The model extends the drift diffusion model of binary decisions (Ratcliff 1978). Like the drift diffusion model, the circular diffusion model casts perceptual decisions as a stochastic process of evidence accumulation to a threshold; evidence is accumulated over time, and when enough evidence has been reached the process terminates in a motor behavior. The model is not concerned with how evidence accumulates, just that it does. In a working memory experiment, evidence might accumulate through repeated probes of memory. In a perceptual-decision task, each saccade might provide a different amount of evidence. In both cases, evidence grows at an average rate, and when there is enough evidence for a decision that decision is made. The amount of time required to reach that threshold of evidence is the response time. The circular diffusion model, therefore, proposes that the responses of rapid decisions which require circularly continuous responses can be modeled as a particle drifting in two dimensions out towards a circular boundary.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: A diffusing particle models perceptual decisions. In this example, wherever the particle first crosses the circle corresponds to the response, and the amount of time required to reach the edge is their response time.\n\n\n\nUsing the circular diffusion model affords researchers the same advantages conferred by using the standard drift diffusion model: the decision-making process can be decomposed into parameters of the model, and those parameters have psychologically meaningful values. For example, a participant might respond quickly, but that could either occur because they accumulate evidence rapidly or because they set a low threshold for evidence. There are three key parameters in the model: 1) the average direction the particle drifts (towards what decision are participants mostly accumulating evidence?), 2) the average rate at which the particle drifts (how quickly do participants accumulate evidence?), and 3) the radius of the circular boundary (how conservative are participants?). Estimating these parameters for participants across different conditions of an experiment enables the researcher to “measure” each of these psychological constructs given participants’ behavior.\n\n\n\n\n\n\n\nReferences\n\nRatcliff, Roger. 1978. “A Theory of Memory Retrieval.” Psychological Review 85 (2): 59. https://doi.org/10.1037/0033-295X.85.2.59.\n\n\nSmith, Philip L. 2016. “Diffusion Theory of Decision Making in Continuous Report.” Psychological Review 123 (4): 425–51. https://doi.org/10.1037/rev0000023."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Patrick Sadil",
    "section": "",
    "text": "Research Associate Faculty, Department of Biostatistics | 2022-Present | Johns Hopkins Bloomberg School of Public Health\n\n\n\nPost-Doc, Biostatistics | 2021-2022 | Johns Hopkins Bloomberg School of Public Health\nPost-Doc, Cognitive Psychology | 2020-2021 | University of Massachusetts, Amherst\nPhD, Cognitive Psychology | 2020 | University of Massachusetts, Amherst\nMS, Cognitive Psychology | 2019 | University of Massachusetts, Amherst\nBA, Biology | 2014 | Reed College"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Patrick Sadil",
    "section": "",
    "text": "Research Associate Faculty, Department of Biostatistics | 2022-Present | Johns Hopkins Bloomberg School of Public Health"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Patrick Sadil",
    "section": "",
    "text": "Post-Doc, Biostatistics | 2021-2022 | Johns Hopkins Bloomberg School of Public Health\nPost-Doc, Cognitive Psychology | 2020-2021 | University of Massachusetts, Amherst\nPhD, Cognitive Psychology | 2020 | University of Massachusetts, Amherst\nMS, Cognitive Psychology | 2019 | University of Massachusetts, Amherst\nBA, Biology | 2014 | Reed College"
  }
]